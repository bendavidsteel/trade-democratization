{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "recurrent_model_future.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMTGuS1lGjZacHANJWxx2FD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bendavidsteel/trade-democratization/blob/master/recurrent_model_future.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Oxcg4dGh326",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b2080ee-dae0-4ec2-cde8-0b002b745582"
      },
      "source": [
        "!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install torch-scatter==2.0.4+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-sparse==0.6.5+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-cluster==1.5.5+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-spline-conv==1.2.0+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-geometric"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 18kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 39.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "  Found existing installation: torchvision 0.8.1+cu101\n",
            "    Uninstalling torchvision-0.8.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.8.1+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-scatter==2.0.4+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_scatter-2.0.4%2Bcu101-cp36-cp36m-linux_x86_64.whl (12.2MB)\n",
            "\u001b[K     |████████████████████████████████| 12.3MB 309kB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.4\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-sparse==0.6.5+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_sparse-0.6.5%2Bcu101-cp36-cp36m-linux_x86_64.whl (21.6MB)\n",
            "\u001b[K     |████████████████████████████████| 21.6MB 257kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-sparse==0.6.5+cu101) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-sparse==0.6.5+cu101) (1.18.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.5\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-cluster==1.5.5+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_cluster-1.5.5%2Bcu101-cp36-cp36m-linux_x86_64.whl (22.0MB)\n",
            "\u001b[K     |████████████████████████████████| 22.0MB 205kB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.5.5\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-spline-conv==1.2.0+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_spline_conv-1.2.0%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.3MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3MB 403kB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.0\n",
            "Collecting torch-geometric\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/67/6c0bce6b6e6bc806e25d996e46a686e5a11254d89257983265a988bb02ee/torch_geometric-1.6.1.tar.gz (178kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.5.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.48.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.1.4)\n",
            "Collecting rdflib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/6b/6454aa1db753c0f8bc265a5bd5c10b5721a4bb24160fb4faf758cf6be8a1/rdflib-5.0.0-py3-none-any.whl (231kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 30.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.10.0)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n",
            "Collecting ase\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/78/edadb45c7f26f8fbb99da81feadb561c26bb0393b6c5d1ac200ecdc12d61/ase-3.20.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 45.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.11.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (0.17.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (50.3.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2020.6.20)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.7)\n",
            "Collecting isodate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ase->torch-geometric) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->torch-geometric) (1.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (0.10.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-1.6.1-cp36-none-any.whl size=308552 sha256=99e60f75571fcf9bd82bbc5a2b596e2ade96e59e6d30724c94992bedae815132\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/25/ea/3d71d2088dccc63214fa59259dcc598ded4150a5f8b41d84ff\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: isodate, rdflib, ase, torch-geometric\n",
            "Successfully installed ase-3.20.1 isodate-0.6.0 rdflib-5.0.0 torch-geometric-1.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H85oVjFvqJf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10234ec6-d663-4e20-edc6-00531c94d469"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrJ0ZsX-qRDP"
      },
      "source": [
        "!cp \"/content/drive/My Drive/projects/trade_democratization/trade/datasets.py\" .\n",
        "!cp \"/content/drive/My Drive/projects/trade_democratization/trade/utils.py\" ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AflAeT2iRDk"
      },
      "source": [
        "import copy\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch_geometric as geo\n",
        "import tqdm\n",
        "\n",
        "import datasets\n",
        "import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOY0o2CNjUoQ"
      },
      "source": [
        "def get_norm_stats(root):\n",
        "    return torch.load(os.path.join(root, 'dataset', \"processed\", \"norm_stats.pt\"))\n",
        "\n",
        "def trade_demo_series_dataset(root):\n",
        "    node_dict = os.path.join(root, 'dataset', \"processed\", \"node_dict.pt\")\n",
        "    \n",
        "    dataset_file_path = os.path.join(root, 'dataset', \"processed\", 'traddem_series_encodedecode.pt')\n",
        "\n",
        "    if os.path.exists(dataset_file_path):\n",
        "        return torch.load(dataset_file_path)\n",
        "\n",
        "    with open(os.path.join(root, \"country_mapping.json\"), \"r\") as f:\n",
        "        country_mapping = json.loads(f.read())\n",
        "\n",
        "    dataset = datasets.TradeDemoYearByYearDataset(os.path.join(root, 'dataset'))\n",
        "\n",
        "    node_dicts = torch.load(node_dict)\n",
        "\n",
        "    num_countries = len(country_mapping)\n",
        "    num_initial_features = 1\n",
        "    num_node_features = 3 # include GDP and population data, and democracy data from last year\n",
        "    num_targets = 1 # 5 main indicators of democracy from the VDem dataset\n",
        "    num_edge_features = 7 # Trade flow, current colony relationship, ever a colony, distance, maritime distance, common language, and shared border\n",
        "\n",
        "    num_seq_combos = 500\n",
        "\n",
        "    all_sequences = []\n",
        "\n",
        "    for _ in tqdm.tqdm(range(num_seq_combos)):\n",
        "\n",
        "        encode_len = random.randint(10, 50) # generate length of input encoded seq\n",
        "        decode_len = random.randint(10, 50) # generate length of output decoded seq\n",
        "        start_idx = random.randint(0, len(dataset) - encode_len - decode_len) # generate start idx with available room\n",
        "\n",
        "        # get encoder inputs\n",
        "        sequence_data = []\n",
        "        for year_idx in range(start_idx, start_idx + encode_len):\n",
        "        \n",
        "            x = torch.zeros(num_countries, num_node_features, dtype=torch.float32)\n",
        "            edge_index = torch.zeros(dataset[year_idx].edge_index.shape, dtype=torch.long)\n",
        "\n",
        "            node_dict = node_dicts[year_idx][\"node_mapping\"]\n",
        "            for country_idx, node_idx in node_dict.items():\n",
        "                x[country_idx, :] = dataset[year_idx].x[node_idx, :]\n",
        "                edge_index[dataset[year_idx].edge_index == node_idx] = country_idx\n",
        "\n",
        "            sequence_data.append(geo.data.Data(x=x, \n",
        "                                                edge_index=edge_index, \n",
        "                                                edge_attr=dataset[year_idx].edge_attr))\n",
        "\n",
        "        # get decoder input\n",
        "        initial = torch.zeros(num_countries, num_initial_features)\n",
        "        node_dict = node_dicts[start_idx + encode_len][\"node_mapping\"]\n",
        "        for country_idx, node_idx in node_dict.items():\n",
        "            initial[country_idx, :] = dataset[start_idx + encode_len].x[node_idx, 2]\n",
        "\n",
        "        # get decoder targets\n",
        "        missing_mask = torch.zeros(decode_len, num_countries, num_targets, dtype=torch.float32)\n",
        "        target = torch.zeros(decode_len, num_countries, num_targets, dtype=torch.float32)\n",
        "\n",
        "        # TODO think about how we can create a missing mask for input data too\n",
        "        # TODO for now we will hope example of missing input data are in the minority and don't effect the output too much\n",
        "\n",
        "        for seq_idx, year_idx in enumerate(range(start_idx + encode_len, start_idx + encode_len + decode_len)):\n",
        "            node_dict = node_dicts[year_idx][\"node_mapping\"]\n",
        "            for country_idx, node_idx in node_dict.items():\n",
        "                missing_mask[seq_idx, country_idx, :] = 1\n",
        "                target[seq_idx, country_idx, :] = dataset[year_idx].y[node_idx, 0]\n",
        "\n",
        "        sequence = utils.Sequence(initial, sequence_data, missing_mask, target)\n",
        "        all_sequences.append(sequence)\n",
        "\n",
        "    torch.save(all_sequences, dataset_file_path)\n",
        "\n",
        "    return all_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cjBed_9Op2z"
      },
      "source": [
        "dataset = trade_demo_series_dataset(os.path.join('/', 'content', 'drive', 'My Drive', 'projects', 'trade_democratization'))\n",
        "\n",
        "# split into three sets\n",
        "# num_train = int(len(dataset) * 0.8)\n",
        "# num_val = int(len(dataset) * 0.1)\n",
        "# num_test = int(len(dataset) * 0.1)\n",
        "\n",
        "# overlapping sequences means there is some potential for biasing the val and test sets\n",
        "# but small size of dataset means having a reasonable sequence length and a strictly non biased val and test set is not possible\n",
        "# train_set = dataset[:num_train]\n",
        "# val_set = dataset[num_train:num_train + num_val]\n",
        "# test_set = dataset[-num_test:]\n",
        "\n",
        "NUM_COUNTRIES = 177\n",
        "\n",
        "# OR we can just split dataset by country\n",
        "# country links are directed so it should be okay to have vertices from same graph split between sets\n",
        "test_set_idx = [0, 30, 120] # small test set of interesting countries\n",
        "val_set_idx = [1, 11, 21, 31, 41, 51, 61, 71, 81, 91, 101, 111, 121, 131, 141, 151, 161, 171] # roughly 10% of rest of set for val\n",
        "train_set_idx = [idx for idx in list(range(NUM_COUNTRIES)) if ((idx not in test_set_idx) or (idx not in val_set_idx))]\n",
        "\n",
        "test_set_mask = torch.zeros((NUM_COUNTRIES))\n",
        "test_set_mask[test_set_idx] = 1\n",
        "\n",
        "val_set_mask = torch.zeros((NUM_COUNTRIES))\n",
        "val_set_mask[val_set_idx] = 1\n",
        "\n",
        "train_set_mask = torch.zeros((NUM_COUNTRIES))\n",
        "train_set_mask[train_set_idx] = 1\n",
        "\n",
        "random.shuffle(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMuDqfXgQPEL"
      },
      "source": [
        "class EncoderNet(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, num_edge_features):\n",
        "        super().__init__()\n",
        "\n",
        "        conv_layer_size = 32\n",
        "        self.lstm_layer_size = 32\n",
        "\n",
        "        # graph convolutional layer to create graph representation\n",
        "        conv_lin = torch.nn.Linear(num_edge_features, num_node_features * conv_layer_size)\n",
        "        self.conv = geo.nn.NNConv(num_node_features, conv_layer_size, conv_lin)\n",
        "\n",
        "        # lstm to learn sequential patterns\n",
        "        self.lstm = torch.nn.LSTM(conv_layer_size, self.lstm_layer_size, dropout=0.5)\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        # do entire sequence all at once\n",
        "        batch_size = sequence[0].x.shape[0]\n",
        "\n",
        "        # create graph representation\n",
        "        graph_collection = []\n",
        "        for idx in range(len(sequence)):\n",
        "            x, edge_index, edge_attr = sequence[idx].x, sequence[idx].edge_index, sequence[idx].edge_attr\n",
        "            graph_step = torch.nn.functional.relu(self.conv(x, edge_index, edge_attr))\n",
        "            graph_collection.append(graph_step)\n",
        "        # provide graph representations as sequence to lstm\n",
        "        graph_series = torch.stack(graph_collection)\n",
        "\n",
        "        # recurrent stage\n",
        "        # zeros initial hidden state\n",
        "        initial_h_s = torch.zeros(1, batch_size, self.lstm_layer_size, device=device)\n",
        "        initial_c_s = torch.zeros(1, batch_size, self.lstm_layer_size, device=device)\n",
        "        # we don't care about the output for the encoder, just the hidden state\n",
        "        _, final_hidden = self.lstm(graph_series, (initial_h_s, initial_c_s))\n",
        "\n",
        "        # final activation is relu as this is for regression and the metrics of this dataset are all positive\n",
        "        return final_hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeOjdovxoV3h"
      },
      "source": [
        "class DecoderNet(torch.nn.Module):\n",
        "    def __init__(self, num_output_features):\n",
        "        super().__init__()\n",
        "\n",
        "        lstm_layer_size = 32\n",
        "\n",
        "        # lstm to learn sequential patterns\n",
        "        # auto-regressive so same num input features as final output features\n",
        "        self.lstm = torch.nn.LSTM(num_output_features, lstm_layer_size, dropout=0.5)\n",
        "\n",
        "        # final linear layer to allow full expressivity for regression after tanh activation in lstm\n",
        "        self.final_linear = torch.nn.Linear(lstm_layer_size, num_output_features)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # need to do each recurrent iteration at a time to allow teacher forcing\n",
        "\n",
        "        # recurrent stage\n",
        "        # initial state of lstm is representation of target prior to this sequence\n",
        "        output, hidden = self.lstm(input, hidden)\n",
        "\n",
        "        # final activation is relu as this is for regression and the metrics of this dataset are all positive\n",
        "        return self.final_linear(output), hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yayAVCnUEoNF"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "encoder = EncoderNet(dataset[0].sequence[0].x.shape[1], dataset[0].sequence[0].edge_attr.shape[1]).to(device)\n",
        "decoder = DecoderNet(dataset[0].target.shape[2]).to(device)\n",
        "\n",
        "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "# hyperparameters\n",
        "teacher_forcing_ratio = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF5CSIvxGZOU"
      },
      "source": [
        "def train(split_country=False):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    loss_all = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    if split_country:\n",
        "        set_gen = dataset\n",
        "        set_mask = train_set_mask\n",
        "    else:\n",
        "        set_gen = train_set\n",
        "        set_mask = torch.ones((NUM_COUNTRIES))\n",
        "\n",
        "    for batch in tqdm.tqdm(set_gen):\n",
        "        batch = batch.to(device)\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        # pass input sequence data through encoder\n",
        "        encoder_hidden = encoder(batch.sequence)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
        "\n",
        "        decoder_input = batch.initial.unsqueeze(0)\n",
        "        target_len = batch.target.shape[0]\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        if use_teacher_forcing:\n",
        "            # Teacher forcing: Feed the target as the next input\n",
        "            for idx in range(target_len):\n",
        "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "                # set outputs with missing data to zero so that they don't affect backprop\n",
        "                # good loss for regression problems\n",
        "                loss += torch.nn.functional.smooth_l1_loss(decoder_output * batch.missing_mask * set_mask, batch.target[idx] * set_mask)\n",
        "                decoder_input = batch.target[idx].unsqueeze(0)  # Teacher forcing\n",
        "\n",
        "        else:\n",
        "            # Without teacher forcing: use its own predictions as the next input\n",
        "            for idx in range(target_len):\n",
        "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "                loss += torch.nn.functional.smooth_l1_loss(decoder_output * batch.missing_mask * set_mask, batch.target[idx] * set_mask)\n",
        "                decoder_input = decoder_output\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        loss_all += loss.item() / target_len\n",
        "        num_batches += 1\n",
        "\n",
        "    return loss_all / num_batches\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(loader, state_dict_encoder=None, state_dict_decoder=None, set_mask=None):\n",
        "\n",
        "    if not state_dict_encoder is None:\n",
        "        encoder.load_state_dict(state_dict_encoder)\n",
        "\n",
        "    if not state_dict_decoder is None:\n",
        "        decoder.load_state_dict(state_dict_decoder)\n",
        "\n",
        "    if set_mask is None:\n",
        "        set_mask = torch.ones((NUM_COUNTRIES))\n",
        "\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    num_batches = 0\n",
        "    loss_all = 0\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        # pass input sequence data through encoder\n",
        "        encoder_hidden = encoder(batch.sequence)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoder_input = batch.initial.unsqueeze(0)\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        target_len = batch.target.shape[0]\n",
        "        for idx in range(target_len):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            # good loss for regression problems\n",
        "            loss += torch.nn.functional.smooth_l1_loss(decoder_output * batch.missing_mask * set_mask, batch.target[idx] * set_mask)\n",
        "            decoder_input = decoder_output\n",
        "\n",
        "        loss_all += loss.item() / target_len\n",
        "        num_batches += 1\n",
        "    return loss_all / num_batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WJ5lQMmKW-6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "631c71c6-af46-47b0-c653-bcb26c9e2457"
      },
      "source": [
        "MAX_EPOCHS = 10000\n",
        "min_val_loss = float(\"inf\")\n",
        "epochs_since = 0\n",
        "NUM_NON_DECREASING = 50\n",
        "\n",
        "best_encoder_model = torch.load(os.path.join('/', 'content', 'drive', 'My Drive', 'projects', 'trade_democratization', 'best_model_encode.pkl'))\n",
        "best_decoder_model = torch.load(os.path.join('/', 'content', 'drive', 'My Drive', 'projects', 'trade_democratization', 'best_model_decode.pkl'))\n",
        "\n",
        "encoder.load_state_dict(best_encoder_model)\n",
        "decoder.load_state_dict(best_decoder_model)\n",
        "\n",
        "for epoch in range(MAX_EPOCHS):\n",
        "    train_loss = train(split_country=True)\n",
        "    val_loss = test(dataset, set_mask=val_set_mask)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        print('Epoch: {}, Train Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, train_loss, val_loss))\n",
        "\n",
        "    if val_loss < min_val_loss:\n",
        "        best_encoder_model = copy.deepcopy(encoder.state_dict())\n",
        "        best_decoder_model = copy.deepcopy(decoder.state_dict())\n",
        "        min_val_loss = val_loss\n",
        "        epochs_since = 0\n",
        "\n",
        "    epochs_since += 1\n",
        "    if epochs_since > NUM_NON_DECREASING:\n",
        "        print(\"Early stopping engaged\")\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [07:18<00:00,  1.14it/s]\n",
            "  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Train Loss: 0.0236, Validation Loss: 0.0034\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [07:15<00:00,  1.15it/s]\n",
            "100%|██████████| 500/500 [07:06<00:00,  1.17it/s]\n",
            "100%|██████████| 500/500 [06:54<00:00,  1.21it/s]\n",
            "100%|██████████| 500/500 [06:41<00:00,  1.25it/s]\n",
            "100%|██████████| 500/500 [06:47<00:00,  1.23it/s]\n",
            "  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5, Train Loss: 0.0230, Validation Loss: 0.0031\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [06:48<00:00,  1.23it/s]\n",
            "100%|██████████| 500/500 [06:43<00:00,  1.24it/s]\n",
            "100%|██████████| 500/500 [06:53<00:00,  1.21it/s]\n",
            "100%|██████████| 500/500 [06:48<00:00,  1.23it/s]\n",
            "100%|██████████| 500/500 [06:43<00:00,  1.24it/s]\n",
            "  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10, Train Loss: 0.0227, Validation Loss: 0.0031\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [06:34<00:00,  1.27it/s]\n",
            "100%|██████████| 500/500 [06:44<00:00,  1.24it/s]\n",
            "100%|██████████| 500/500 [06:50<00:00,  1.22it/s]\n",
            "100%|██████████| 500/500 [06:46<00:00,  1.23it/s]\n",
            "100%|██████████| 500/500 [06:50<00:00,  1.22it/s]\n",
            "  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 15, Train Loss: 0.0219, Validation Loss: 0.0030\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [06:51<00:00,  1.22it/s]\n",
            "100%|██████████| 500/500 [06:43<00:00,  1.24it/s]\n",
            "100%|██████████| 500/500 [06:38<00:00,  1.25it/s]\n",
            "100%|██████████| 500/500 [06:37<00:00,  1.26it/s]\n",
            "100%|██████████| 500/500 [06:49<00:00,  1.22it/s]\n",
            "  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 20, Train Loss: 0.0216, Validation Loss: 0.0029\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [06:58<00:00,  1.20it/s]\n",
            "100%|██████████| 500/500 [06:52<00:00,  1.21it/s]\n",
            "100%|██████████| 500/500 [06:44<00:00,  1.24it/s]\n",
            "100%|██████████| 500/500 [06:45<00:00,  1.23it/s]\n",
            "100%|██████████| 500/500 [06:51<00:00,  1.22it/s]\n",
            "  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 25, Train Loss: 0.0216, Validation Loss: 0.0030\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [06:49<00:00,  1.22it/s]\n",
            "100%|██████████| 500/500 [06:36<00:00,  1.26it/s]\n",
            "100%|██████████| 500/500 [06:43<00:00,  1.24it/s]\n",
            "100%|██████████| 500/500 [06:43<00:00,  1.24it/s]\n",
            "100%|██████████| 500/500 [06:39<00:00,  1.25it/s]\n",
            "  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 30, Train Loss: 0.0202, Validation Loss: 0.0028\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [06:41<00:00,  1.24it/s]\n",
            "100%|██████████| 500/500 [06:43<00:00,  1.24it/s]\n",
            "100%|██████████| 500/500 [06:44<00:00,  1.24it/s]\n",
            "100%|██████████| 500/500 [06:40<00:00,  1.25it/s]\n",
            "100%|██████████| 500/500 [06:42<00:00,  1.24it/s]\n",
            "  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 35, Train Loss: 0.0206, Validation Loss: 0.0029\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [06:37<00:00,  1.26it/s]\n",
            "100%|██████████| 500/500 [07:02<00:00,  1.18it/s]\n",
            "100%|██████████| 500/500 [07:25<00:00,  1.12it/s]\n",
            "100%|██████████| 500/500 [07:47<00:00,  1.07it/s]\n",
            "100%|██████████| 500/500 [07:43<00:00,  1.08it/s]\n",
            "  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 40, Train Loss: 0.0202, Validation Loss: 0.0027\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [07:40<00:00,  1.09it/s]\n",
            "100%|██████████| 500/500 [07:42<00:00,  1.08it/s]\n",
            "100%|██████████| 500/500 [07:44<00:00,  1.08it/s]\n",
            "100%|██████████| 500/500 [07:46<00:00,  1.07it/s]\n",
            "100%|██████████| 500/500 [07:37<00:00,  1.09it/s]\n",
            "  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 45, Train Loss: 0.0200, Validation Loss: 0.0026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [07:36<00:00,  1.09it/s]\n",
            "100%|██████████| 500/500 [07:41<00:00,  1.08it/s]\n",
            "100%|██████████| 500/500 [07:39<00:00,  1.09it/s]\n",
            "100%|██████████| 500/500 [07:49<00:00,  1.07it/s]\n",
            "100%|██████████| 500/500 [07:44<00:00,  1.08it/s]\n",
            "  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 50, Train Loss: 0.0204, Validation Loss: 0.0026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [07:47<00:00,  1.07it/s]\n",
            "100%|██████████| 500/500 [07:40<00:00,  1.09it/s]\n",
            "100%|██████████| 500/500 [07:40<00:00,  1.08it/s]\n",
            "100%|██████████| 500/500 [07:43<00:00,  1.08it/s]\n",
            "100%|██████████| 500/500 [07:45<00:00,  1.07it/s]\n",
            "  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 55, Train Loss: 0.0195, Validation Loss: 0.0027\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [07:36<00:00,  1.10it/s]\n",
            "100%|██████████| 500/500 [07:36<00:00,  1.10it/s]\n",
            "100%|██████████| 500/500 [07:40<00:00,  1.09it/s]\n",
            "100%|██████████| 500/500 [07:58<00:00,  1.04it/s]\n",
            "100%|██████████| 500/500 [08:16<00:00,  1.01it/s]\n",
            "  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 60, Train Loss: 0.0197, Validation Loss: 0.0026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [08:23<00:00,  1.01s/it]\n",
            "100%|██████████| 500/500 [08:22<00:00,  1.01s/it]\n",
            "100%|██████████| 500/500 [08:24<00:00,  1.01s/it]\n",
            "100%|██████████| 500/500 [08:25<00:00,  1.01s/it]\n",
            "100%|██████████| 500/500 [08:24<00:00,  1.01s/it]\n",
            "  0%|          | 0/500 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 65, Train Loss: 0.0190, Validation Loss: 0.0026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  4%|▍         | 19/500 [00:18<07:13,  1.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-162ddae95c31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_country\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_set_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-ae75dd186553>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(split_country)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIvsSMuwKjoO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d52d982f-b71b-46e3-b12b-8c80ec835c2f"
      },
      "source": [
        "test_loss = test(dataset, state_dict_encoder=best_encoder_model, state_dict_decoder=best_decoder_model, set_mask=test_set_mask)\n",
        "print('Final Test Loss: {:.4f}'.format(test_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Test Loss: 0.0004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZwrGTO1KouB"
      },
      "source": [
        "torch.save(best_encoder_model, os.path.join('/', 'content', 'drive', 'My Drive', 'projects', 'trade_democratization', 'best_model_encode.pkl'))\n",
        "torch.save(best_decoder_model, os.path.join('/', 'content', 'drive', 'My Drive', 'projects', 'trade_democratization', 'best_model_decode.pkl'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}