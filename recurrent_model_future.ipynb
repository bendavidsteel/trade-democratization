{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "recurrent_model_future.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNFYV1NY+UIt66C3B2WXIEC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bendavidsteel/trade-democratization/blob/master/recurrent_model_future.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Oxcg4dGh326",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da18041d-e6c7-448e-a6fd-07d2fe99a16d"
      },
      "source": [
        "!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install torch-scatter==2.0.4+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-sparse==0.6.5+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-cluster==1.5.5+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-spline-conv==1.2.0+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-geometric"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 24kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 35.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-scatter==2.0.4+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_scatter-2.0.4%2Bcu101-cp36-cp36m-linux_x86_64.whl (12.2MB)\n",
            "\u001b[K     |████████████████████████████████| 12.3MB 370kB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.4\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-sparse==0.6.5+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_sparse-0.6.5%2Bcu101-cp36-cp36m-linux_x86_64.whl (21.6MB)\n",
            "\u001b[K     |████████████████████████████████| 21.6MB 37.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-sparse==0.6.5+cu101) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-sparse==0.6.5+cu101) (1.18.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.5\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-cluster==1.5.5+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_cluster-1.5.5%2Bcu101-cp36-cp36m-linux_x86_64.whl (22.0MB)\n",
            "\u001b[K     |████████████████████████████████| 22.0MB 1.4MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.5.5\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-spline-conv==1.2.0+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_spline_conv-1.2.0%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.3MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3MB 477kB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.0\n",
            "Collecting torch-geometric\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/67/6c0bce6b6e6bc806e25d996e46a686e5a11254d89257983265a988bb02ee/torch_geometric-1.6.1.tar.gz (178kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.5.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.48.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.1.3)\n",
            "Collecting rdflib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/6b/6454aa1db753c0f8bc265a5bd5c10b5721a4bb24160fb4faf758cf6be8a1/rdflib-5.0.0-py3-none-any.whl (231kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.10.0)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n",
            "Collecting ase\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/78/edadb45c7f26f8fbb99da81feadb561c26bb0393b6c5d1ac200ecdc12d61/ase-3.20.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.11.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (0.17.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (50.3.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.7)\n",
            "Collecting isodate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ase->torch-geometric) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->torch-geometric) (1.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (0.10.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-1.6.1-cp36-none-any.whl size=308552 sha256=bc1fb3681a8fad3feecaf2ad3a8a9efce61484f58187cb9b98b0dcfa0cd50f48\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/25/ea/3d71d2088dccc63214fa59259dcc598ded4150a5f8b41d84ff\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: isodate, rdflib, ase, torch-geometric\n",
            "Successfully installed ase-3.20.1 isodate-0.6.0 rdflib-5.0.0 torch-geometric-1.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H85oVjFvqJf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfca7850-73ce-4f94-d473-0444e3ebb044"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrJ0ZsX-qRDP"
      },
      "source": [
        "!cp \"/content/drive/My Drive/projects/trade_democratization/trade/dataset.py\" .\n",
        "!cp \"/content/drive/My Drive/projects/trade_democratization/trade/utils.py\" ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AflAeT2iRDk"
      },
      "source": [
        "import copy\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch_geometric as geo\n",
        "import tqdm\n",
        "\n",
        "import dataset\n",
        "import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOY0o2CNjUoQ"
      },
      "source": [
        "def get_norm_stats(root):\n",
        "    return torch.load(os.path.join(root, \"processed\", \"norm_stats.pt\"))\n",
        "\n",
        "def trade_demo_series_dataset(root):\n",
        "    node_dict = os.path.join(root, \"processed\", \"node_dict.pt\")\n",
        "    \n",
        "    dataset_file_path = os.path.join(root, \"processed\", 'traddem_series.pt')\n",
        "\n",
        "    if os.path.exists(dataset_file_path):\n",
        "        return torch.load(dataset_file_path)\n",
        "\n",
        "    with open(os.path.join(root, \"country_mapping.json\"), \"r\") as f:\n",
        "        country_mapping = json.loads(f.read())\n",
        "\n",
        "    dataset = dataset.TradeDemoYearByYearDataset(root)\n",
        "\n",
        "    node_dicts = torch.load(node_dict)\n",
        "\n",
        "    num_countries = len(country_mapping)\n",
        "    num_node_features = 3 # include GDP and population data, and democracy data from last year\n",
        "    num_targets = 1 # 5 main indicators of democracy from the VDem dataset\n",
        "    num_edge_features = 7 # Trade flow, current colony relationship, ever a colony, distance, maritime distance, common language, and shared border\n",
        "\n",
        "    num_seq_combos = 500\n",
        "\n",
        "    all_sequences = []\n",
        "\n",
        "    for _ in range(num_seq_combos):\n",
        "\n",
        "        encode_len = random.uniform(10, 50) # generate length of input encoded seq\n",
        "        decode_len = random.uniform(10, 50) # generate length of output decoded seq\n",
        "        start_idx = random.uniform(0, len(dataset) - encode_len - decode_len + 1) # generate start idx with available room\n",
        "\n",
        "        # get encoder inputs\n",
        "        sequence_data = []\n",
        "        for year_idx in range(start_idx, start_idx + encode_len):\n",
        "        \n",
        "            x = torch.zeros(num_countries, num_node_features, dtype=torch.float32)\n",
        "            edge_index = torch.zeros(dataset[year_idx].edge_index.shape, dtype=torch.long)\n",
        "\n",
        "            node_dict = node_dicts[year_idx][\"node_mapping\"]\n",
        "            for country_idx, node_idx in node_dict.items():\n",
        "                x[country_idx, :] = dataset[year_idx].x[node_idx, :]\n",
        "                edge_index[dataset[year_idx].edge_index == node_idx] = country_idx\n",
        "\n",
        "            sequence_data.append(geo.data.Data(x=x, \n",
        "                                                edge_index=edge_index, \n",
        "                                                edge_attr=dataset[year_idx].edge_attr))\n",
        "\n",
        "        # get decoder input\n",
        "        initial = torch.zeros(num_countries, num_initial_features)\n",
        "        node_dict = node_dicts[start_idx][\"node_mapping\"]\n",
        "        for country_idx, node_idx in node_dict.items():\n",
        "            initial[country_idx, :] = dataset[start_idx + encode_len].x[node_idx, 2]\n",
        "\n",
        "        # get decoder targets\n",
        "        missing_mask = torch.zeros(decode_len, num_countries, num_targets, dtype=torch.float32)\n",
        "        target = torch.zeros(decode_len, num_countries, num_targets, dtype=torch.float32)\n",
        "\n",
        "        # TODO think about how we can create a missing mask for input data too\n",
        "        # TODO for now we will hope example of missing input data are in the minority and don't effect the output too much\n",
        "\n",
        "        for seq_idx, year_idx in enumerate(range(start_idx + encode_len, start_idx + encode_len + decode_len)):\n",
        "            node_dict = node_dicts[year_idx][\"node_mapping\"]\n",
        "            for country_idx, node_idx in node_dict.items():\n",
        "                missing_mask[seq_idx, country_idx, :] = 1\n",
        "                target[seq_idx, country_idx, :] = dataset[year_idx].y[node_idx, 0]\n",
        "\n",
        "        sequence = utils.Sequence(initial, sequence_data, missing_mask, target)\n",
        "        all_sequences.append(sequence)\n",
        "\n",
        "    torch.save(all_sequences, dataset_file_path)\n",
        "\n",
        "    return all_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cjBed_9Op2z"
      },
      "source": [
        "dataset = trade_demo_series_dataset(os.path.join('/', 'content', 'drive', 'My Drive', 'projects', 'trade_democratization', 'dataset'))\n",
        "\n",
        "# split into three sets\n",
        "# num_train = int(len(dataset) * 0.8)\n",
        "# num_val = int(len(dataset) * 0.1)\n",
        "# num_test = int(len(dataset) * 0.1)\n",
        "\n",
        "# overlapping sequences means there is some potential for biasing the val and test sets\n",
        "# but small size of dataset means having a reasonable sequence length and a strictly non biased val and test set is not possible\n",
        "# train_set = dataset[:num_train]\n",
        "# val_set = dataset[num_train:num_train + num_val]\n",
        "# test_set = dataset[-num_test:]\n",
        "\n",
        "NUM_COUNTRIES = 177\n",
        "\n",
        "# OR we can just split dataset by country\n",
        "# country links are directed so it should be okay to have vertices from same graph split between sets\n",
        "test_set_idx = [0, 30, 120] # small test set of interesting countries\n",
        "val_set_idx = [1, 11, 21, 31, 41, 51, 61, 71, 81, 91, 101, 111, 121, 131, 141, 151, 161, 171] # roughly 10% of rest of set for val\n",
        "train_set_idx = [idx for idx in list(range(NUM_COUNTRIES)) if ((idx not in test_set_idx) or (idx not in val_set_idx))]\n",
        "\n",
        "test_set_mask = torch.zeros((NUM_COUNTRIES))\n",
        "test_set_mask[test_set_idx] = 1\n",
        "\n",
        "val_set_mask = torch.zeros((NUM_COUNTRIES))\n",
        "val_set_mask[val_set_idx] = 1\n",
        "\n",
        "train_set_mask = torch.zeros((NUM_COUNTRIES))\n",
        "train_set_mask[train_set_idx] = 1\n",
        "\n",
        "random.shuffle(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMuDqfXgQPEL"
      },
      "source": [
        "class EncoderNet(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, num_edge_features):\n",
        "        super().__init__()\n",
        "\n",
        "        conv_layer_size = 32\n",
        "        self.lstm_layer_size = 32\n",
        "\n",
        "        # graph convolutional layer to create graph representation\n",
        "        conv_lin = torch.nn.Linear(num_edge_features, num_node_features * conv_layer_size)\n",
        "        self.conv = geo.nn.NNConv(num_node_features, conv_layer_size, conv_lin)\n",
        "\n",
        "        # lstm to learn sequential patterns\n",
        "        self.lstm = torch.nn.LSTM(conv_layer_size, lstm_layer_size, dropout=0.5)\n",
        "\n",
        "    def forward(self, sequence):\n",
        "        # do entire sequence all at once\n",
        "        batch_size = sequence[0].x.shape[0]\n",
        "\n",
        "        # create graph representation\n",
        "        graph_collection = []\n",
        "        for idx in range(len(sequence)):\n",
        "            x, edge_index, edge_attr = sequence[idx].x, sequence[idx].edge_index, sequence[idx].edge_attr\n",
        "            graph_step = torch.nn.functional.relu(self.conv(x, edge_index, edge_attr))\n",
        "            graph_collection.append(graph_step)\n",
        "        # provide graph representations as sequence to lstm\n",
        "        graph_series = torch.stack(graph_collection)\n",
        "\n",
        "        # recurrent stage\n",
        "        # zeros initial hidden state\n",
        "        initial_h_s = torch.zeros(1, batch_size, self.lstm_layer_size, device=device)\n",
        "        initial_c_s = torch.zeros(1, batch_size, self.lstm_layer_size, device=device)\n",
        "        # we don't care about the output for the encoder, just the hidden state\n",
        "        _, final_hidden = self.lstm(graph_series, (initial_h_s, initial_c_s))\n",
        "\n",
        "        # final activation is relu as this is for regression and the metrics of this dataset are all positive\n",
        "        return final_hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeOjdovxoV3h"
      },
      "source": [
        "class DecoderNet(torch.nn.Module):\n",
        "    def __init__(self, num_output_features):\n",
        "        super().__init__()\n",
        "\n",
        "        lstm_layer_size = 8\n",
        "\n",
        "        # lstm to learn sequential patterns\n",
        "        # auto-regressive so same num input features as final output features\n",
        "        self.lstm = torch.nn.LSTM(num_output_features, lstm_layer_size, dropout=0.5)\n",
        "\n",
        "        # final linear layer to allow full expressivity for regression after tanh activation in lstm\n",
        "        self.final_linear = torch.nn.Linear(lstm_layer_size, num_output_features)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # need to do each recurrent iteration at a time to allow teacher forcing\n",
        "\n",
        "        # recurrent stage\n",
        "        # initial state of lstm is representation of target prior to this sequence\n",
        "        output, hidden = self.lstm(input, hidden)\n",
        "\n",
        "        # final activation is relu as this is for regression and the metrics of this dataset are all positive\n",
        "        return self.final_linear(output), hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yayAVCnUEoNF"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "encoder = EncoderNet(dataset[0].sequence[0].x.shape[1], dataset[0].sequence[0].edge_attr.shape[1]).to(device)\n",
        "decoder = DecoderNet(dataset[0].target.shape[2]).to(device)\n",
        "\n",
        "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "# hyperparameters\n",
        "teacher_forcing_ratio = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF5CSIvxGZOU"
      },
      "source": [
        "def train(split_country=False):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    loss_all = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    if split_country:\n",
        "        set_gen = dataset\n",
        "        set_mask = train_set_mask\n",
        "    else:\n",
        "        set_gen = train_set\n",
        "        set_mask = torch.ones((NUM_COUNTRIES))\n",
        "\n",
        "    for sequence in set_gen:\n",
        "        sequence = sequence.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # pass input sequence data through encoder\n",
        "        encoder_hidden = encoder(sequence.sequence)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
        "\n",
        "        decoder_input = sequence.initial\n",
        "\n",
        "        if use_teacher_forcing:\n",
        "            # Teacher forcing: Feed the target as the next input\n",
        "            for idx in range(sequence.target.shape[0]):\n",
        "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "                loss += criterion(decoder_output, sequence.target[idx])\n",
        "                decoder_input = sequence.target[idx]  # Teacher forcing\n",
        "\n",
        "        else:\n",
        "            # Without teacher forcing: use its own predictions as the next input\n",
        "            for di in range(target_length):\n",
        "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "                loss += criterion(decoder_output, sequence.target[idx])\n",
        "                decoder_input = decoder_output\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "\n",
        "        # set outputs with missing data to zero so that they don't affect backprop\n",
        "        # good loss for regression problems\n",
        "        loss = torch.nn.functional.smooth_l1_loss(out * sequence.missing_mask * set_mask, sequence.target * set_mask)\n",
        "        loss.backward()\n",
        "        loss_all += loss.item()\n",
        "        num_batches += 1\n",
        "        optimizer.step()\n",
        "    return loss_all / num_batches\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(loader, state_dict=None, set_mask=None):\n",
        "\n",
        "    if not state_dict is None:\n",
        "        model.load_state_dict(state_dict)\n",
        "\n",
        "    if set_mask is None:\n",
        "        set_mask = torch.ones((NUM_COUNTRIES))\n",
        "\n",
        "    model.eval()\n",
        "    num_batches = 0\n",
        "    loss_all = 0\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        pred = model(batch)\n",
        "        # good loss for regression problems\n",
        "        loss = torch.nn.functional.smooth_l1_loss(pred * batch.missing_mask * set_mask, batch.target * set_mask)\n",
        "        loss_all += loss\n",
        "        num_batches += 1\n",
        "    return loss_all / num_batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WJ5lQMmKW-6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "4f378f98-48f4-49a7-d08f-65ea5668bacc"
      },
      "source": [
        "MAX_EPOCHS = 10000\n",
        "min_val_loss = float(\"inf\")\n",
        "epochs_since = 0\n",
        "NUM_NON_DECREASING = 50\n",
        "for epoch in range(MAX_EPOCHS):\n",
        "    train_loss = train(split_country=True)\n",
        "    val_loss = test(dataset, set_mask=val_set_mask)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        print('Epoch: {}, Train Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, train_loss, val_loss))\n",
        "\n",
        "    if val_loss < min_val_loss:\n",
        "        best_model = copy.deepcopy(model.state_dict())\n",
        "        min_val_loss = val_loss\n",
        "        epochs_since = 0\n",
        "\n",
        "    epochs_since += 1\n",
        "    if epochs_since > NUM_NON_DECREASING:\n",
        "        print(\"Early stopping engaged\")\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-4ef9ac16962d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mNUM_NON_DECREASING\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_country\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_set_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-9d6592d92271>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(split_country)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# good loss for regression problems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing_mask\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mset_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mset_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mloss_all\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mnum_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIvsSMuwKjoO"
      },
      "source": [
        "test_loss = test(dataset, state_dict=best_model, set_mask=test_set_mask)\n",
        "print('Final Test Loss: {:.4f}'.format(test_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZwrGTO1KouB"
      },
      "source": [
        "torch.save(best_model, os.path.join('/', 'content', 'drive', 'My Drive', 'projects', 'trade_democratization', 'best_model_recurrent.pkl'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}