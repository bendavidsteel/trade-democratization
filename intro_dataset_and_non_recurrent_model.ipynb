{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intro_dataset_and_non_recurrent_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMaEJzThudAOtZj+1JSidgn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bendavidsteel/trade-democratization/blob/master/intro_dataset_and_non_recurrent_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HlIailEbTFk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26f1ba23-6ef1-4f4a-ed17-e2d9bb4a0854"
      },
      "source": [
        "import copy\n",
        "import itertools\n",
        "import math\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32JRsdyv5Pg1",
        "colab_type": "text"
      },
      "source": [
        "In this project we will be linking the [V-Dem dataset](https://www.v-dem.net/en) and the [CEPII Trade History dataset](http://www.cepii.fr/cepii/en/bdd_modele/bdd.asp) into a time series graph dataset, in an attempt to investigate how bilateral trade affects the democratization of nations over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC4nveOW2GY4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "outputId": "b23686a5-eb33-4e33-f9f2-a3f6929abc9b"
      },
      "source": [
        "dataset_path = os.path.join('/', 'content', 'drive', 'My Drive', 'projects', 'trade_democratization', 'dataset', 'raw')\n",
        "# here we load the democracy indices from the V-Dem dataset\n",
        "vdem_nodes = pd.read_csv(os.path.join(dataset_path, \"V-Dem-CY-Core-v10.csv\"))\n",
        "vdem_nodes.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>country_name</th>\n",
              "      <th>country_text_id</th>\n",
              "      <th>country_id</th>\n",
              "      <th>year</th>\n",
              "      <th>historical_date</th>\n",
              "      <th>project</th>\n",
              "      <th>historical</th>\n",
              "      <th>histname</th>\n",
              "      <th>codingstart</th>\n",
              "      <th>codingend</th>\n",
              "      <th>codingstart_contemp</th>\n",
              "      <th>codingend_contemp</th>\n",
              "      <th>codingstart_hist</th>\n",
              "      <th>codingend_hist</th>\n",
              "      <th>gapstart1</th>\n",
              "      <th>gapstart2</th>\n",
              "      <th>gapstart3</th>\n",
              "      <th>gapend1</th>\n",
              "      <th>gapend2</th>\n",
              "      <th>gapend3</th>\n",
              "      <th>COWcode</th>\n",
              "      <th>v2x_polyarchy</th>\n",
              "      <th>v2x_polyarchy_codelow</th>\n",
              "      <th>v2x_polyarchy_codehigh</th>\n",
              "      <th>v2x_polyarchy_sd</th>\n",
              "      <th>v2x_libdem</th>\n",
              "      <th>v2x_libdem_codelow</th>\n",
              "      <th>v2x_libdem_codehigh</th>\n",
              "      <th>v2x_libdem_sd</th>\n",
              "      <th>v2x_partipdem</th>\n",
              "      <th>v2x_partipdem_codelow</th>\n",
              "      <th>v2x_partipdem_codehigh</th>\n",
              "      <th>v2x_partipdem_sd</th>\n",
              "      <th>v2x_delibdem</th>\n",
              "      <th>v2x_delibdem_codelow</th>\n",
              "      <th>v2x_delibdem_codehigh</th>\n",
              "      <th>v2x_delibdem_sd</th>\n",
              "      <th>v2x_egaldem</th>\n",
              "      <th>v2x_egaldem_codelow</th>\n",
              "      <th>v2x_egaldem_codehigh</th>\n",
              "      <th>...</th>\n",
              "      <th>v2x_EDcomp_thick</th>\n",
              "      <th>v2x_EDcomp_thick_codelow</th>\n",
              "      <th>v2x_EDcomp_thick_codehigh</th>\n",
              "      <th>v2x_EDcomp_thick_sd</th>\n",
              "      <th>v2x_freexp</th>\n",
              "      <th>v2x_freexp_codelow</th>\n",
              "      <th>v2x_freexp_codehigh</th>\n",
              "      <th>v2x_freexp_sd</th>\n",
              "      <th>v2x_hosabort</th>\n",
              "      <th>v2x_hosinter</th>\n",
              "      <th>v2x_legabort</th>\n",
              "      <th>v2xcl_disc</th>\n",
              "      <th>v2xcl_disc_codelow</th>\n",
              "      <th>v2xcl_disc_codehigh</th>\n",
              "      <th>v2xcl_disc_sd</th>\n",
              "      <th>v2xcl_dmove</th>\n",
              "      <th>v2xcl_dmove_codelow</th>\n",
              "      <th>v2xcl_dmove_codehigh</th>\n",
              "      <th>v2xcl_dmove_sd</th>\n",
              "      <th>v2xcl_slave</th>\n",
              "      <th>v2xcl_slave_codelow</th>\n",
              "      <th>v2xcl_slave_codehigh</th>\n",
              "      <th>v2xcl_slave_sd</th>\n",
              "      <th>v2xel_elecparl</th>\n",
              "      <th>v2xel_elecpres</th>\n",
              "      <th>v2xex_elecleg</th>\n",
              "      <th>v2xlg_leginter</th>\n",
              "      <th>v2xme_altinf</th>\n",
              "      <th>v2xme_altinf_codelow</th>\n",
              "      <th>v2xme_altinf_codehigh</th>\n",
              "      <th>v2xme_altinf_sd</th>\n",
              "      <th>v2xps_party</th>\n",
              "      <th>v2xps_party_codelow</th>\n",
              "      <th>v2xps_party_codehigh</th>\n",
              "      <th>v2x_divparctrl</th>\n",
              "      <th>v2x_feduni</th>\n",
              "      <th>v2xca_academ</th>\n",
              "      <th>v2xca_academ_codelow</th>\n",
              "      <th>v2xca_academ_codehigh</th>\n",
              "      <th>v2xca_academ_sd</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mexico</td>\n",
              "      <td>MEX</td>\n",
              "      <td>3</td>\n",
              "      <td>1789</td>\n",
              "      <td>1789-12-31</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Viceroyalty of New Spain</td>\n",
              "      <td>1088</td>\n",
              "      <td>2019</td>\n",
              "      <td>1900.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>1789.0</td>\n",
              "      <td>1920.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>70.0</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.009</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.277</td>\n",
              "      <td>0.644</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.631</td>\n",
              "      <td>0.385</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.524</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.645</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.134</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.244</td>\n",
              "      <td>0.659</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mexico</td>\n",
              "      <td>MEX</td>\n",
              "      <td>3</td>\n",
              "      <td>1790</td>\n",
              "      <td>1790-12-31</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Viceroyalty of New Spain</td>\n",
              "      <td>1088</td>\n",
              "      <td>2019</td>\n",
              "      <td>1900.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>1789.0</td>\n",
              "      <td>1920.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>70.0</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.009</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.277</td>\n",
              "      <td>0.644</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.631</td>\n",
              "      <td>0.385</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.524</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.645</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.134</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.244</td>\n",
              "      <td>0.659</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mexico</td>\n",
              "      <td>MEX</td>\n",
              "      <td>3</td>\n",
              "      <td>1791</td>\n",
              "      <td>1791-12-31</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Viceroyalty of New Spain</td>\n",
              "      <td>1088</td>\n",
              "      <td>2019</td>\n",
              "      <td>1900.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>1789.0</td>\n",
              "      <td>1920.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>70.0</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.009</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.277</td>\n",
              "      <td>0.644</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.631</td>\n",
              "      <td>0.385</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.524</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.645</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.134</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.244</td>\n",
              "      <td>0.659</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mexico</td>\n",
              "      <td>MEX</td>\n",
              "      <td>3</td>\n",
              "      <td>1792</td>\n",
              "      <td>1792-12-31</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Viceroyalty of New Spain</td>\n",
              "      <td>1088</td>\n",
              "      <td>2019</td>\n",
              "      <td>1900.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>1789.0</td>\n",
              "      <td>1920.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>70.0</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.009</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.277</td>\n",
              "      <td>0.644</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.631</td>\n",
              "      <td>0.385</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.524</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.645</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.134</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.244</td>\n",
              "      <td>0.659</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Mexico</td>\n",
              "      <td>MEX</td>\n",
              "      <td>3</td>\n",
              "      <td>1793</td>\n",
              "      <td>1793-12-31</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Viceroyalty of New Spain</td>\n",
              "      <td>1088</td>\n",
              "      <td>2019</td>\n",
              "      <td>1900.0</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>1789.0</td>\n",
              "      <td>1920.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>70.0</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.009</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.277</td>\n",
              "      <td>0.644</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.631</td>\n",
              "      <td>0.385</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.524</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.645</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.134</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.244</td>\n",
              "      <td>0.659</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1817 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  country_name country_text_id  ...  v2xca_academ_codehigh  v2xca_academ_sd\n",
              "0       Mexico             MEX  ...                    NaN              NaN\n",
              "1       Mexico             MEX  ...                    NaN              NaN\n",
              "2       Mexico             MEX  ...                    NaN              NaN\n",
              "3       Mexico             MEX  ...                    NaN              NaN\n",
              "4       Mexico             MEX  ...                    NaN              NaN\n",
              "\n",
              "[5 rows x 1817 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCznS4Fd5dYV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "59a456e8-f4d9-4112-b062-42ae9e80a6a8"
      },
      "source": [
        "tradhist_gdppop = pd.read_excel(os.path.join(dataset_path, \"TRADHIST_GDP_POP.xlsx\"))\n",
        "tradhist_gdppop.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iso</th>\n",
              "      <th>year</th>\n",
              "      <th>GDP</th>\n",
              "      <th>SOURCE_GDP</th>\n",
              "      <th>POP</th>\n",
              "      <th>SOURCE_POP</th>\n",
              "      <th>SOURCE_SH_PRIM</th>\n",
              "      <th>SOURCE_SH_SECD</th>\n",
              "      <th>SH_PRIM</th>\n",
              "      <th>SH_SECD</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MMR</td>\n",
              "      <td>1827</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3605.399902</td>\n",
              "      <td>MADDISON_IPO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SOM</td>\n",
              "      <td>1827</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1077.427124</td>\n",
              "      <td>MADDISON_IPO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ADEN</td>\n",
              "      <td>1827</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2627.580078</td>\n",
              "      <td>MADDISON_IPO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MOZ</td>\n",
              "      <td>1827</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2319.700684</td>\n",
              "      <td>MADDISON_IPO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PRI</td>\n",
              "      <td>1827</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>305.633331</td>\n",
              "      <td>MADDISON_IPO</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    iso  year  GDP SOURCE_GDP  ...  SOURCE_SH_PRIM SOURCE_SH_SECD SH_PRIM SH_SECD\n",
              "0   MMR  1827  NaN        NaN  ...             NaN            NaN     NaN     NaN\n",
              "1   SOM  1827  NaN        NaN  ...             NaN            NaN     NaN     NaN\n",
              "2  ADEN  1827  NaN        NaN  ...             NaN            NaN     NaN     NaN\n",
              "3   MOZ  1827  NaN        NaN  ...             NaN            NaN     NaN     NaN\n",
              "4   PRI  1827  NaN        NaN  ...             NaN            NaN     NaN     NaN\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoTWmHxV9mQu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "74104941-9f96-4d4c-9634-abb7775cd75d"
      },
      "source": [
        "# now we load the time invariant bilateral data from the CEPII TradHist dataset\n",
        "tradhist_timeinvar = pd.read_excel(os.path.join(dataset_path, \"TRADHIST_GRAVITY_BILATERAL_TIME_INVARIANT.xlsx\"))\n",
        "tradhist_timeinvar.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iso_o</th>\n",
              "      <th>iso_d</th>\n",
              "      <th>Distw</th>\n",
              "      <th>Dist_coord</th>\n",
              "      <th>Evercol</th>\n",
              "      <th>Comlang</th>\n",
              "      <th>Contig</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2SICIL</td>\n",
              "      <td>AUTHUN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>837.066180</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2SICIL</td>\n",
              "      <td>BEL</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1349.682430</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2SICIL</td>\n",
              "      <td>CHL</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11978.672481</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2SICIL</td>\n",
              "      <td>ESP</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1514.852764</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2SICIL</td>\n",
              "      <td>FRA</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1291.794085</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    iso_o   iso_d  Distw    Dist_coord  Evercol  Comlang  Contig\n",
              "0  2SICIL  AUTHUN    NaN    837.066180        0        0       0\n",
              "1  2SICIL     BEL    NaN   1349.682430        0        0       0\n",
              "2  2SICIL     CHL    NaN  11978.672481        0        0       0\n",
              "3  2SICIL     ESP    NaN   1514.852764        0        0       0\n",
              "4  2SICIL     FRA    NaN   1291.794085        0        0       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xklb6hvtDGIa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "a3e4852f-e596-4a0b-d2c2-01f0fae6ce40"
      },
      "source": [
        "# then load the three files that constitute the bilateral historical non trade related data such as distances between countries and colonial status\n",
        "tradhist_timevar_frames = []\n",
        "for idx in range(1, 4):\n",
        "    tradhist_timevar_frames.append(pd.read_excel(os.path.join(dataset_path, \"TRADHIST_GRAVITY_BILATERAL_TIME_VARIANT_{}.xlsx\".format(idx))))\n",
        "tradhist_timevar = pd.concat(tradhist_timevar_frames)\n",
        "tradhist_timevar.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iso_o</th>\n",
              "      <th>iso_d</th>\n",
              "      <th>year</th>\n",
              "      <th>SeaDist_SHRT</th>\n",
              "      <th>SeaDist_2CST</th>\n",
              "      <th>Curcol</th>\n",
              "      <th>Metro</th>\n",
              "      <th>Colo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2SICIL</td>\n",
              "      <td>AUTHUN</td>\n",
              "      <td>1838</td>\n",
              "      <td>1790.884033</td>\n",
              "      <td>1790.884033</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2SICIL</td>\n",
              "      <td>BEL</td>\n",
              "      <td>1834</td>\n",
              "      <td>4463.319824</td>\n",
              "      <td>4463.319824</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2SICIL</td>\n",
              "      <td>BEL</td>\n",
              "      <td>1835</td>\n",
              "      <td>4463.319824</td>\n",
              "      <td>4463.319824</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2SICIL</td>\n",
              "      <td>BEL</td>\n",
              "      <td>1836</td>\n",
              "      <td>4463.319824</td>\n",
              "      <td>4463.319824</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2SICIL</td>\n",
              "      <td>BEL</td>\n",
              "      <td>1837</td>\n",
              "      <td>4463.319824</td>\n",
              "      <td>4463.319824</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    iso_o   iso_d  year  SeaDist_SHRT  SeaDist_2CST  Curcol Metro Colo\n",
              "0  2SICIL  AUTHUN  1838   1790.884033   1790.884033       0   NaN  NaN\n",
              "1  2SICIL     BEL  1834   4463.319824   4463.319824       0   NaN  NaN\n",
              "2  2SICIL     BEL  1835   4463.319824   4463.319824       0   NaN  NaN\n",
              "3  2SICIL     BEL  1836   4463.319824   4463.319824       0   NaN  NaN\n",
              "4  2SICIL     BEL  1837   4463.319824   4463.319824       0   NaN  NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3Kl0JiJIHWL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "840a1b36-9eb7-4786-cb87-d93764877ba4"
      },
      "source": [
        "tradhist_bitrade_frames = []\n",
        "for idx in range(1, 4):\n",
        "    tradhist_bitrade_frames.append(pd.read_excel(os.path.join(dataset_path, \"TRADHIST_BITRADE_BITARIFF_{}.xlsx\".format(idx))))\n",
        "tradhist_bitrade = pd.concat(tradhist_bitrade_frames)\n",
        "tradhist_bitrade.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iso_o</th>\n",
              "      <th>iso_d</th>\n",
              "      <th>year</th>\n",
              "      <th>FLOW</th>\n",
              "      <th>FLOW_0</th>\n",
              "      <th>SOURCE_TF</th>\n",
              "      <th>BITARIFF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2SICIL</td>\n",
              "      <td>AUTHUN</td>\n",
              "      <td>1838</td>\n",
              "      <td>492920.687500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RIC_IP</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2SICIL</td>\n",
              "      <td>BEL</td>\n",
              "      <td>1834</td>\n",
              "      <td>38164.500000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RIC_IP</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2SICIL</td>\n",
              "      <td>BEL</td>\n",
              "      <td>1835</td>\n",
              "      <td>30052.339844</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RIC_IP</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2SICIL</td>\n",
              "      <td>BEL</td>\n",
              "      <td>1836</td>\n",
              "      <td>44906.539062</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RIC_IP</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2SICIL</td>\n",
              "      <td>BEL</td>\n",
              "      <td>1837</td>\n",
              "      <td>33957.269531</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RIC_IP</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    iso_o   iso_d  year           FLOW  FLOW_0 SOURCE_TF  BITARIFF\n",
              "0  2SICIL  AUTHUN  1838  492920.687500     NaN    RIC_IP       NaN\n",
              "1  2SICIL     BEL  1834   38164.500000     NaN    RIC_IP       NaN\n",
              "2  2SICIL     BEL  1835   30052.339844     NaN    RIC_IP       NaN\n",
              "3  2SICIL     BEL  1836   44906.539062     NaN    RIC_IP       NaN\n",
              "4  2SICIL     BEL  1837   33957.269531     NaN    RIC_IP       NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Agoze7A6DWB",
        "colab_type": "text"
      },
      "source": [
        "Previous studies how shown that geographical closeness and colonizer/colony status affects the diffusion of democracy. It seems likely following from this that trade would result in some democratization diffusion effect also."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf9e5qjL6uah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TradHist dataset has data for USSR separate than from Russia\n",
        "# USA : USA\n",
        "# Russia: RUS, USSR (VDem uses RUS only, TradHist uses RUS and USSR)\n",
        "colours = ['r', 'b']\n",
        "\n",
        "# get high level indicators of democracy\n",
        "ctry_demind = {}\n",
        "ctry_demind[\"USA\"] = vdem_nodes[(vdem_nodes['country_text_id'] == \"USA\") & (vdem_nodes['year'] >= vdem_nodes['codingstart_contemp'])][['year', 'v2x_libdem']]\n",
        "ctry_demind[\"RUS\"] = vdem_nodes[(vdem_nodes['country_text_id'] == \"RUS\") & (vdem_nodes['year'] >= vdem_nodes['codingstart_contemp'])][['year', 'v2x_libdem']]\n",
        "\n",
        "\n",
        "# get imports of one country from the other\n",
        "ctry_trade = {}\n",
        "ctry_trade[\"USA\"] = tradhist_bitrade[((tradhist_bitrade['iso_o'] == \"RUS\") | (tradhist_bitrade['iso_o'] == \"USSR\"))\n",
        "                                     & (tradhist_bitrade['iso_d'] == \"USA\") \n",
        "                                     & (tradhist_bitrade['year'] >= 1900)][['year', 'FLOW']].sort_values(by=['year'])\n",
        "ctry_trade[\"RUS\"] = tradhist_bitrade[(tradhist_bitrade['iso_o'] == \"USA\")\n",
        "                                     & ((tradhist_bitrade['iso_d'] == \"RUS\") | (tradhist_bitrade['iso_d'] == \"USSR\"))\n",
        "                                     & (tradhist_bitrade['year'] >= 1900)][['year', 'FLOW']].sort_values(by=['year'])\n",
        "\n",
        "# plot the data\n",
        "fig, ax1 = plt.subplots(figsize=(13, 6))\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "ax1.plot(ctry_demind[\"USA\"].values[:, 0], ctry_demind[\"USA\"].values[:, 1], 'b-', label=\"USA Liberal Democracy Index\")\n",
        "ax2.plot(ctry_trade[\"USA\"].values[:, 0], ctry_trade[\"USA\"].values[:, 1], 'b--', label=\"Imports from Russia to the USA\")\n",
        "ax1.plot(ctry_demind[\"RUS\"].values[:, 0], ctry_demind[\"RUS\"].values[:, 1], 'r-', label=\"Russia Liberal Democracy Index\")\n",
        "ax2.plot(ctry_trade[\"RUS\"].values[:, 0], ctry_trade[\"RUS\"].values[:, 1], 'r--', label=\"Imports from the USA to Russia\")\n",
        "\n",
        "ax1.legend(loc = \"upper left\")\n",
        "ax2.legend(loc = \"upper right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPU1rjWGvJ-_",
        "colab_type": "text"
      },
      "source": [
        "The figure seems to show some non-linear relationship between trade and liberal democracy index. The casuality direction of this is not clear however."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA5q_mFCSW7j",
        "colab_type": "text"
      },
      "source": [
        "These figures show us that normalising the trade data will be important to prevent any relationships being overshadowed by GDP growth, even if GDP growth will likely be another key factor that affects democratization. We will therefore be using GDP and population as a node feature, and normalising trade data by the GDP of a country at that time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbsJzgem6EKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ctry_gdp_trade = {}\n",
        "usa_gdp = tradhist_gdppop[(tradhist_gdppop['iso'] == 'USA') & (tradhist_gdppop['year'] >= 1900)][['year', 'GDP']].sort_values(by=['year'])\n",
        "ctry_gdp_trade['USA'] = pd.merge(usa_gdp, ctry_trade['USA'], how='inner')\n",
        "rus_gdp = tradhist_gdppop[((tradhist_gdppop['iso'] == 'RUS') | (tradhist_gdppop['iso'] == 'USSR')) & (tradhist_gdppop['year'] >= 1900)][['year', 'GDP']].sort_values(by=['year'])\n",
        "ctry_gdp_trade['RUS'] = pd.merge(rus_gdp, ctry_trade['RUS'], how='inner')\n",
        "\n",
        "# plot the data\n",
        "fig, ax1 = plt.subplots(figsize=(13, 6))\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "ax1.plot(ctry_demind[\"USA\"].values[:, 0], ctry_demind[\"USA\"].values[:, 1], 'b-', label=\"USA Liberal Democracy Index\")\n",
        "ax2.plot(ctry_gdp_trade[\"USA\"][['year']].values[:, 0], ctry_gdp_trade[\"USA\"][['FLOW']].values[:, 0] / ctry_gdp_trade[\"USA\"][['GDP']].values[:, 0], 'b--', label=\"GDP normalised imports from Russia to the USA\")\n",
        "ax1.plot(ctry_demind[\"RUS\"].values[:, 0], ctry_demind[\"RUS\"].values[:, 1], 'r-', label=\"Russia Liberal Democracy Index\")\n",
        "ax2.plot(ctry_gdp_trade[\"RUS\"][['year']].values[:, 0], ctry_gdp_trade[\"RUS\"][['FLOW']].values[:, 0] / ctry_gdp_trade[\"RUS\"][['GDP']].values[:, 0], 'r--', label=\"GDP normalised imports from the USA to Russia\")\n",
        "\n",
        "ax1.legend(loc = \"upper left\")\n",
        "ax2.legend(loc = \"upper right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1DmuOhCX0oJ",
        "colab_type": "text"
      },
      "source": [
        "This normalised data does seems to show a significant correlation between Russian imports of American goods with democratization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiIfULs5xSfQ",
        "colab_type": "text"
      },
      "source": [
        "We now need to find a way of linking the two datasets. The trade history dataset uses the ISO country codes. However the V-Dem dataset seems to have its own country numbering system, and the three letter shortened name isn't specified in the reference manual to be the ISO name. Lets look at the union of the country codes for the two datasets and the country codes which are unique to each dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sogvaCPSzEOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vdem_country_codes = set(vdem_nodes['country_text_id'].unique())\n",
        "tradhist_country_codes = set(tradhist_timevar['iso_o'].unique())\n",
        "shared_codes = vdem_country_codes & tradhist_country_codes\n",
        "vdem_unique_codes = vdem_country_codes - shared_codes\n",
        "tradhist_unique_codes = tradhist_country_codes - shared_codes\n",
        "\n",
        "def print_set(code_set, per_line = 20):\n",
        "    code_ordered = sorted(list(code_set))\n",
        "    for i in range(0, len(code_ordered), per_line):\n",
        "        print(code_ordered[i:i + per_line])\n",
        "\n",
        "print(\"Shared Country Codes:\")\n",
        "print_set(shared_codes)\n",
        "print()\n",
        "print(\"VDem Only Country Codes:\")\n",
        "print_set(vdem_unique_codes)\n",
        "print()\n",
        "print(\"TradHist Only Country Codes:\")\n",
        "print_set(tradhist_unique_codes)\n",
        "print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xzX42cQYSSe",
        "colab_type": "text"
      },
      "source": [
        "The major difference seems to be that the V-Dem dataset uses a consistent code for the varying regimes of a 'country', compared to the CEPII dataset using for example USSR for the Soviet Union instead of continuing to use RUS. We need to find if there are any other examples of this and create a mapping system. We will not include trade participants from the trade dataset that are current subareas of sovereignties, as they provide no additional data to correlate with nation democratization given that their owning sovereignty is already featured in the dataset. However we will lump together formerly separate sovereignties that have no additional coding in the V-Dem dataset, such as Prince Edward Island and Canada.\n",
        "\n",
        "For former countries that cover the land of multiple modern countries, we will associate the trade of those countries at the time with all of the modern countries in their former land area."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pizS4fVTbTNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_mapping(vdem_nodes, tradhist_timevar):   \n",
        "\n",
        "    vdem_country_codes = list(vdem_nodes['country_text_id'].unique())\n",
        "    tradhist_country_codes = list(tradhist_timevar['iso_o'].unique())\n",
        "    shared_codes = [code for code in vdem_country_codes if code in tradhist_country_codes]\n",
        "\n",
        "    mapped_codes = [['RUS', 'USSR'], ['YEM', 'ADEN'], ['CAF', 'AOFAEF', 'FRAAEF'], ['TCD', 'AOFAEF', 'FRAAEF'], ['COD', 'AOFAEF', 'FRAAEF'], ['HRV', 'AUTHUN', 'YUG'], ['SVK', 'CZSK', 'AUTHUN'], \n",
        "                ['SVN', 'AUTHUN', 'YUG'], ['UKR', 'AUTHUN', 'USSR'], ['ALB', 'AUTHUN'], ['BIH', 'AUTHUN', 'YUG'], ['MNE', 'AUTHUN', 'YUG'], ['CAN', 'CANPRINCED', 'CANQBCONT', 'NFLD'], \n",
        "                ['CZE', 'CZSK', 'AUTHUN'], ['DDR', 'EDEU'], ['MYS', 'FEDMYS', 'UNFEDMYS', 'GBRBORNEO'], ['BFA', 'FRAAOF'], ['GNQ', 'FRAAOF'], ['LUX', 'ZOLL'], \n",
        "                ['ZZB', 'ZANZ', 'GBRAFRI'], ['ZAF', 'ZAFTRA', 'ZAFORA', 'ZAFNAT', 'ZAPCAF', 'GBRAFRI'], ['MKD', 'YUG'], ['SRB', 'YUG'], ['POL', 'USSR'], ['COM', 'MYT'], ['ROU', 'ROM'], \n",
        "                ['MWI', 'RHOD', 'GBRAFRI'], ['ZMB', 'RHOD', 'GBRAFRI'], ['ZWE', 'RHOD', 'GBRAFRI'], ['SGP', 'STRAITS'], ['DEU', 'WDEU'], ['SML', 'GBRSOM', 'ITASOM'], ['GBR', 'ULSTER'], \n",
        "                ['RWA', 'RWABDI'], ['SOM', 'ITASOM'], ['MAR', 'MARESP'], ['FRA', 'OLDENB'], ['DNK', 'SCHLES'], ['LBN', 'SYRLBN', 'OTTO'], ['SYR', 'SYRLBN'], ['CYP', 'OTTO', 'GBRMEDI'], \n",
        "                ['TUR', 'OTTO'], ['STP', 'PRTAFRI'], ['AGO', 'PRTAFRI'], ['MOZ', 'PRTAFRI'], ['GNB', 'PRTWAFRI'], ['KHM', 'INDOCHI'], ['LAO', 'INDOCHI'], ['VNM', 'INDOCHI'], \n",
        "                ['ERI', 'ITAEAFRI', 'GBRAFRI'], ['TTO', 'GBRWINDIES'], ['SLE', 'GBRWAFRI'], ['GMB', 'GBRWAFRI'], ['TGO', 'GBRWAFRI'], ['EGY', 'OTTO'],\n",
        "                ['PNG', 'GBRPAPUA'], ['MLT', 'GBRMEDI'], ['BGD', 'GBRIND'], ['BTN', 'GBRIND'], ['IND', 'GBRIND'], ['MDV', 'GBRIND'], ['NPL', 'GBRIND'], ['PAK', 'GBRIND'], \n",
        "                ['LKA', 'GBRIND'], ['CMR', 'GBRAFRI', 'FRAAFRI'], ['KEN', 'GBRAFRI'], ['SYC', 'GBRAFRI'], ['SDN', 'GBRAFRI'], ['UGA', 'GBRAFRI'], ['LSO', 'GBRAFRI'], \n",
        "                ['SWZ', 'GBRAFRI']]\n",
        "\n",
        "    # validate my matches\n",
        "    code_count = {}\n",
        "    for codes in mapped_codes:\n",
        "        matched_to_vdem = 0\n",
        "        for code in codes:\n",
        "            if len(code) == 3:\n",
        "                if code in vdem_country_codes:\n",
        "                    if code in code_count:\n",
        "                        code_count[code] += 1\n",
        "                    else:\n",
        "                        code_count[code] = 1\n",
        "                    matched_to_vdem += 1\n",
        "\n",
        "        if matched_to_vdem == 0:\n",
        "            raise ValueError(\"{} country code set matched to no VDem node\".format(codes))\n",
        "        elif matched_to_vdem > 1:\n",
        "            raise ValueError(\"{} country code set matched to more than one VDem node\".format(codes))\n",
        "\n",
        "        if codes[0] not in vdem_country_codes:\n",
        "            raise ValueError(\"VDem code should be first in list {}.\".format(codes))\n",
        "\n",
        "    for code in code_count:\n",
        "        if code_count[code] != 1:\n",
        "            raise ValueError(\"VDem code {} matched to more than one country code set\".format(code))\n",
        "\n",
        "    for code in shared_codes:\n",
        "        if code not in code_count:\n",
        "            mapped_codes.append([code])\n",
        "\n",
        "    return mapped_codes\n",
        "\n",
        "#mapped_codes = get_mapping(vdem_nodes, tradhist_timevar)\n",
        "\n",
        "#print_set(mapped_codes, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ7KhCAK49Ov",
        "colab_type": "text"
      },
      "source": [
        "Now that we have a mapping between the two datasets we can start putting them together. We will only be using data from 1900 to 2014, as before 1900 the VDem dataset is noted to be much less accurate, and trade data is increasingly absent for smaller nations. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQjO9Iw2DIFe",
        "colab_type": "text"
      },
      "source": [
        "This is where we will first use our graph learning library of choice, PyTorch Geometric. PyTorch Geometric provides a base class with which to structure the compilation of a dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBRnbTWdDX1U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "af1a2b33-6e9c-45fa-8a55-60bee20692bc"
      },
      "source": [
        "!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install torch-scatter==2.0.4+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-sparse==0.6.5+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-cluster==1.5.5+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-spline-conv==1.2.0+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-geometric"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.5.1+cu101 in /usr/local/lib/python3.6/dist-packages (1.5.1+cu101)\n",
            "Requirement already satisfied: torchvision==0.6.1+cu101 in /usr/local/lib/python3.6/dist-packages (0.6.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-scatter==2.0.4+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_scatter-2.0.4%2Bcu101-cp36-cp36m-linux_x86_64.whl (12.2MB)\n",
            "\u001b[K     |████████████████████████████████| 12.3MB 24.0MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.4\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-sparse==0.6.5+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_sparse-0.6.5%2Bcu101-cp36-cp36m-linux_x86_64.whl (21.6MB)\n",
            "\u001b[K     |████████████████████████████████| 21.6MB 72.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-sparse==0.6.5+cu101) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-sparse==0.6.5+cu101) (1.18.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.5\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-cluster==1.5.5+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_cluster-1.5.5%2Bcu101-cp36-cp36m-linux_x86_64.whl (22.0MB)\n",
            "\u001b[K     |████████████████████████████████| 22.0MB 1.3MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.5.5\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-spline-conv==1.2.0+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_spline_conv-1.2.0%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.3MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3MB 12.7MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.0\n",
            "Collecting torch-geometric\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/18/93b190226d09958be96919fd50c55d28f83f1a1b9260a2b33499f9d86728/torch_geometric-1.6.0.tar.gz (172kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.5.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.48.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.0.5)\n",
            "Collecting rdflib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/6b/6454aa1db753c0f8bc265a5bd5c10b5721a4bb24160fb4faf758cf6be8a1/rdflib-5.0.0-py3-none-any.whl (231kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 15.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.10.0)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n",
            "Collecting ase\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/70/a8b1a7831193aa228defd805891c534d3e4717c8988147522e673458ddce/ase-3.19.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 17.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.11.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (49.1.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.31.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2020.6.20)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.7)\n",
            "Collecting isodate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (1.12.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from ase->torch-geometric) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->torch-geometric) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ase->torch-geometric) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ase->torch-geometric) (1.2.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-1.6.0-cp36-none-any.whl size=296339 sha256=7582a2b313af414f5c2e43867c451a7c79ae752e41b9ef7c3aad81d55ce77283\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/7f/33/acea5809d8580a7adf60dcd6d04f5fc50a7f983040f68be1ff\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: isodate, rdflib, ase, torch-geometric\n",
            "Successfully installed ase-3.19.1 isodate-0.6.0 rdflib-5.0.0 torch-geometric-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH963gAoAi0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch_geometric as geo\n",
        "import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqJ2meMQOV9a",
        "colab_type": "text"
      },
      "source": [
        "We will initially create a simple model trying to predict next year's democracy indicators from this year's democracy indicators, alongside trade and other geopolitical data. This has the disadvantage compared to a more traditional RNN of not having long term memory, but it will create a good baseline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QEyATteMIFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_last_valid(df, col):\n",
        "    valid_rows = df[df[col].isnull() == False]\n",
        "    if (len(valid_rows) > 0):\n",
        "        return valid_rows.sort_values('year')[col].values[-1]\n",
        "    else:\n",
        "        return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB5Carkl48Iy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TradeDemoYearByYearDataset(geo.data.InMemoryDataset):\n",
        "    def __init__(self, root, transform=None, pre_transform=None):\n",
        "\n",
        "        self.norm_stats = os.path.join(root, \"processed\", \"norm_stats.pt\")\n",
        "        self.node_dict = os.path.join(root, \"processed\", \"node_dict.pt\")\n",
        "        self.year_start = 1901\n",
        "        self.year_end = 2015\n",
        "\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        files = []\n",
        "        # vdem dataset\n",
        "        files.append(\"V-Dem-CY-Core-v10.csv\")\n",
        "        # gdp and population data\n",
        "        files.append(\"TRADHIST_GDP_POP.xlsx\")\n",
        "        # time invariant bilateral data such as distance, common language\n",
        "        files.append(\"TRADHIST_GRAVITY_BILATERAL_TIME_INVARIANT.xlsx\")\n",
        "        # time variant non trade bilateral data such as colony status\n",
        "        for idx in range(1, 4):\n",
        "            files.append(\"TRADHIST_GRAVITY_BILATERAL_TIME_VARIANT_{}.xlsx\".format(idx))\n",
        "        # historical bilateral trade and tariff data\n",
        "        for idx in range(1, 4):\n",
        "            files.append(\"TRADHIST_BITRADE_BITARIFF_{}.xlsx\".format(idx))\n",
        "\n",
        "        return files\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['traddem.pt']\n",
        "\n",
        "    def process(self):\n",
        "        # Read data into Data object.\n",
        "        vdem_nodes = pd.read_csv(os.path.join(self.raw_dir, \"V-Dem-CY-Core-v10.csv\"))\n",
        "\n",
        "        tradhist_gdppop = pd.read_excel(os.path.join(self.raw_dir, \"TRADHIST_GDP_POP.xlsx\"))\n",
        "\n",
        "        tradhist_timeinvar = pd.read_excel(os.path.join(self.raw_dir, \"TRADHIST_GRAVITY_BILATERAL_TIME_INVARIANT.xlsx\"))\n",
        "\n",
        "        tradhist_timevar_frames = []\n",
        "        for idx in range(1, 4):\n",
        "            tradhist_timevar_frames.append(pd.read_excel(os.path.join(self.raw_dir, \"TRADHIST_GRAVITY_BILATERAL_TIME_VARIANT_{}.xlsx\".format(idx))))\n",
        "        tradhist_timevar = pd.concat(tradhist_timevar_frames)\n",
        "\n",
        "        tradhist_bitrade_frames = []\n",
        "        for idx in range(1, 4):\n",
        "            tradhist_bitrade_frames.append(pd.read_excel(os.path.join(self.raw_dir, \"TRADHIST_BITRADE_BITARIFF_{}.xlsx\".format(idx))))\n",
        "        tradhist_bitrade = pd.concat(tradhist_bitrade_frames)\n",
        "\n",
        "        country_mapping = get_mapping(vdem_nodes, tradhist_timevar)\n",
        "\n",
        "        num_countries = len(country_mapping)\n",
        "        num_node_features = 2 + 1 # include GDP and population data, and democracy data from last year\n",
        "        num_node_targets = 1 # averaged 5 main indicators of democracy from the VDem dataset\n",
        "        num_edge_features = 7 # Trade flow, current colony relationship, ever a colony, distance, maritime distance, common language, and shared border\n",
        "\n",
        "        all_years = []\n",
        "        years_metadata = []\n",
        "        year_start = self.year_start\n",
        "        year_end = self.year_end\n",
        "\n",
        "        for year_idx, year in tqdm.tqdm(enumerate(range(year_start, year_end))):\n",
        "\n",
        "            year_metadata = {}\n",
        "            year_metadata[\"year\"] = year\n",
        "\n",
        "            year_edge_attr = []\n",
        "            year_edge_index = []\n",
        "\n",
        "            node_features = []\n",
        "            node_target = []\n",
        "            mapping_to_node_indexes = {}\n",
        "\n",
        "            vdem_this_year = vdem_nodes[(vdem_nodes['year'] == year)]\n",
        "            vdem_last_year = vdem_nodes[(vdem_nodes['year'] == year - 1)]\n",
        "\n",
        "            gdppop_year_span = tradhist_gdppop[tradhist_gdppop['year'].isin(list(range(year_start, year + 1)))]\n",
        "\n",
        "            for country_idx, country_codes in enumerate(country_mapping):\n",
        "                # check if this year and next is coded in VDem\n",
        "                vdem_this_year_cty = vdem_this_year[(vdem_this_year['country_text_id'] == country_codes[0])][['v2x_polyarchy', 'v2x_libdem', 'v2x_partipdem', 'v2x_delibdem', 'v2x_egaldem']]\n",
        "                vdem_last_year_cty = vdem_last_year[(vdem_last_year['country_text_id'] == country_codes[0])][['v2x_polyarchy', 'v2x_libdem', 'v2x_partipdem', 'v2x_delibdem', 'v2x_egaldem']]\n",
        "                if ((len(vdem_this_year_cty) == 0) | \\\n",
        "                    (len(vdem_last_year_cty) == 0) | \\\n",
        "                    (vdem_this_year_cty.isnull().values.any()) | \\\n",
        "                    (vdem_last_year_cty.isnull().values.any())):\n",
        "                    continue\n",
        "\n",
        "                country_features = np.zeros((num_node_features, 1))\n",
        "                country_targets = np.zeros((num_node_targets, 1))\n",
        "\n",
        "                # look for last time there was valid value, use 0 otherwise\n",
        "                gdppop_year_cty = gdppop_year_span[gdppop_year_span['iso'].isin(country_codes)]\n",
        "                country_features[0] = get_last_valid(gdppop_year_cty, 'GDP')\n",
        "                country_features[1] = get_last_valid(gdppop_year_cty, 'POP')\n",
        "\n",
        "                country_features[2] = np.mean(vdem_last_year_cty.values.reshape(-1, 1))\n",
        "                country_targets[0] = np.mean(vdem_this_year_cty.values.reshape(-1, 1))\n",
        "\n",
        "                mapping_to_node_indexes[country_idx] = len(node_features)\n",
        "                node_features.append(np.nan_to_num(country_features))\n",
        "                node_target.append(np.nan_to_num(country_targets))\n",
        "\n",
        "            year_metadata[\"node_mapping\"] = mapping_to_node_indexes\n",
        "\n",
        "            timevar_time_span = tradhist_timevar[tradhist_timevar['year'].isin(list(range(year_start, year + 1)))]\n",
        "            bitrade_time_span = tradhist_bitrade[tradhist_bitrade['year'].isin(list(range(year_start, year + 1)))]\n",
        "\n",
        "            # now that all nodes are in this graph have set indexes, we can add the edges with the correct indexes too\n",
        "            for country_a_idx, node_a_idx in mapping_to_node_indexes.items():\n",
        "\n",
        "                country_codes_a = country_mapping[country_a_idx]\n",
        "\n",
        "                time_invar_cty = tradhist_timeinvar[tradhist_timeinvar['iso_d'].isin(country_codes_a)]\n",
        "\n",
        "                timevar_span_cty = timevar_time_span[timevar_time_span['iso_d'].isin(country_codes_a)]\n",
        "                bitrade_span_cty = bitrade_time_span[bitrade_time_span['iso_d'].isin(country_codes_a)]\n",
        "\n",
        "                # we want to normalise imports to a country by the sum of imports for that year\n",
        "                cty_edges = []\n",
        "                cty_year_imports = 0\n",
        "\n",
        "                for country_b_idx, node_b_idx in mapping_to_node_indexes.items():\n",
        "                    # self links don't really make sense to include in the dataset\n",
        "                    if country_a_idx == country_b_idx:\n",
        "                        continue\n",
        "                    \n",
        "                    country_codes_b = country_mapping[country_b_idx]\n",
        "\n",
        "                    bilateral_attr = np.zeros((num_edge_features, 1))\n",
        "                    # for situations where we have multiple trade links two mapped countries due to how we define a country, we will simply take the last link for now\n",
        "                    time_invar_attrs = time_invar_cty[time_invar_cty['iso_o'].isin(country_codes_b)]\n",
        "                    if ((len(time_invar_attrs) == 0) |\\\n",
        "                        (vdem_this_year_cty.isnull().values.all())):\n",
        "                        continue\n",
        "                    bilateral_attr[:4] = time_invar_attrs[['Dist_coord', 'Evercol', 'Comlang', 'Contig']].values[-1].reshape(-1, 1)\n",
        "\n",
        "                    timevar_span_link = timevar_span_cty[timevar_span_cty['iso_o'].isin(country_codes_b)]\n",
        "                    bitrade_span_link = bitrade_span_cty[bitrade_span_cty['iso_o'].isin(country_codes_b)]\n",
        "\n",
        "                    bilateral_attr[4] = get_last_valid(timevar_span_link, 'SeaDist_2CST')\n",
        "                    bilateral_attr[5] = get_last_valid(timevar_span_link, 'Curcol')\n",
        "                    cty_year_flow = get_last_valid(bitrade_span_link, 'FLOW')\n",
        "                    bilateral_attr[6] = cty_year_flow\n",
        "                    cty_year_imports += cty_year_flow\n",
        "\n",
        "                    edge_index = [node_b_idx, node_a_idx]\n",
        "\n",
        "                    cty_edges.append(np.nan_to_num(bilateral_attr))\n",
        "                    year_edge_index.append(edge_index)\n",
        "\n",
        "                for cty_edge in cty_edges:\n",
        "                    if cty_year_imports > 0:\n",
        "                        cty_edge[6] = cty_edge[6] / cty_year_imports\n",
        "                    year_edge_attr.append(cty_edge)\n",
        "\n",
        "            year_graph = geo.data.Data(x=torch.tensor(node_features, dtype=torch.float32).view(-1, num_node_features), \n",
        "                                       y=torch.tensor(node_target, dtype=torch.float32).view(-1, num_node_targets), \n",
        "                                       edge_index=torch.tensor(year_edge_index, dtype=torch.long).T, \n",
        "                                       edge_attr=torch.tensor(year_edge_attr, dtype=torch.float32).view(-1, num_edge_features))\n",
        "\n",
        "            all_years.append(year_graph)\n",
        "            years_metadata.append(year_metadata)\n",
        "\n",
        "        # get normalization stats\n",
        "        # for completely unbiased test we should only get training set stats but will pass on that for this\n",
        "        stacked_x = torch.cat(list([graph.x for graph in all_years]), 0)\n",
        "        stacked_y = torch.cat(list([graph.y for graph in all_years]), 0)\n",
        "        stacked_attrs = torch.cat(list([graph.edge_attr for graph in all_years]), 0)\n",
        "\n",
        "        x_mean = torch.mean(stacked_x, 0)\n",
        "        x_std = torch.std(stacked_x, 0)\n",
        "        y_mean = torch.mean(stacked_y, 0)\n",
        "        y_std = torch.std(stacked_y, 0)\n",
        "        attr_mean = torch.mean(stacked_attrs, 0)\n",
        "        attr_std = torch.std(stacked_attrs, 0)\n",
        "\n",
        "        for graph in all_years:\n",
        "            graph.x = (graph.x - x_mean) / x_std\n",
        "            graph.y = (graph.y - y_mean) / y_std\n",
        "            graph.edge_attr = (graph.edge_attr - attr_mean) / attr_std\n",
        "\n",
        "        # save stats for later use\n",
        "        torch.save({\"x_mean\": x_mean, \"x_std\": x_std, \"y_mean\": y_mean, \"y_std\": y_std, \"attr_mean\": attr_mean, \"attr_std\": attr_std}, self.norm_stats)\n",
        "        torch.save(years_metadata, self.node_dict)\n",
        "\n",
        "        data, slices = self.collate(all_years)\n",
        "        torch.save((data, slices), self.processed_paths[0])\n",
        "\n",
        "    def get_norm_stats(self):\n",
        "        return torch.load(self.norm_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_LcIz33gK5x",
        "colab_type": "text"
      },
      "source": [
        "Now that the dataset has been created, we need to shuffle, partition, and load the dataset into batches using the PyTorch Geometric data loader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w83aoW2shRyT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "ef86f94c-43d2-40f8-b3ae-34357fabb833"
      },
      "source": [
        "\n",
        "dataset = TradeDemoYearByYearDataset(os.path.join('/', 'content', 'drive', 'My Drive', 'projects', 'trade_democratization', 'dataset'))\n",
        "dataset.shuffle()\n",
        "\n",
        "# split into three sets\n",
        "num_train = int(len(dataset) * 0.8)\n",
        "num_val = int(len(dataset) * 0.1)\n",
        "num_test = int(len(dataset) * 0.1)\n",
        "\n",
        "train_set = dataset[:num_train]\n",
        "val_set = dataset[num_train:num_train+num_val]\n",
        "test_set = dataset[-num_test:]\n",
        "\n",
        "# load into batches\n",
        "train_loader = geo.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "val_loader = geo.data.DataLoader(val_set, batch_size=32, shuffle=True)\n",
        "test_loader = geo.data.DataLoader(test_set, batch_size=32, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "114it [5:04:11, 160.10s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N11bf0Te6F1U",
        "colab_type": "text"
      },
      "source": [
        "Now we will define a simple graph network model that will predict next years democratization level given a range of variables for this year."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG-CsMvR6WIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RegressionGraphNet(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, num_edge_features, num_outputs):\n",
        "        super(RegressionGraphNet, self).__init__()\n",
        "        # we will use the edge conditioned convolution (NNConv) as this allows more than 1 edge feature\n",
        "        # while also not requiring inputs to be scaled between 0 and 1\n",
        "\n",
        "        hidden_layer_size = 5\n",
        "\n",
        "        # NNConv requires a layer to transform the dimensionality of edge features to the required size\n",
        "        lin1 = torch.nn.Linear(num_edge_features, num_node_features * hidden_layer_size)\n",
        "\n",
        "        # arbitrarily using 16 as hidden layer size\n",
        "        self.conv1 = geo.nn.NNConv(num_node_features, hidden_layer_size, lin1)\n",
        "        self.lin1 = torch.nn.Linear(hidden_layer_size, num_outputs)\n",
        "\n",
        "    def forward(self, data):\n",
        "        # weights are by default floats\n",
        "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
        "\n",
        "        x = self.conv1(x, edge_index, edge_attr)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = torch.nn.functional.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.lin1(x)\n",
        "\n",
        "        # final activation is linear as this is for regression\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBb9TEkeELi9",
        "colab_type": "text"
      },
      "source": [
        "We now create an instance of the model and optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEb0DhpBEvVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = RegressionGraphNet(dataset.num_features, dataset.num_edge_features, dataset[0].y.size(1))\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuNBVU5VF6If",
        "colab_type": "text"
      },
      "source": [
        "Define our training and test calls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Nb0OYHAGI9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch)\n",
        "        label = batch.y\n",
        "        # good loss for regression problems\n",
        "        loss = torch.nn.functional.smooth_l1_loss(out, label)\n",
        "        loss.backward()\n",
        "        loss_all += loss.item()\n",
        "        optimizer.step()\n",
        "    return loss_all / num_train\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(loader, state_dict=None):\n",
        "\n",
        "    if not state_dict is None:\n",
        "        model.load_state_dict(state_dict)\n",
        "\n",
        "    model.eval()\n",
        "    num_batches = 0\n",
        "    loss_all = 0\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        pred = model(batch)\n",
        "        label = batch.y\n",
        "        # good loss for regression problems\n",
        "        loss = torch.nn.functional.smooth_l1_loss(pred, label)\n",
        "        loss_all += loss\n",
        "        num_batches += 1\n",
        "    return loss_all / num_batches\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE9GgHFrXL4E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "79a48454-2521-401c-a2a3-d41497fde323"
      },
      "source": [
        "MAX_EPOCHS = 10000\n",
        "min_val_loss = float(\"inf\")\n",
        "epochs_since = 0\n",
        "NUM_NON_DECREASING = 100\n",
        "for epoch in range(MAX_EPOCHS):\n",
        "    train_loss = train()\n",
        "    val_loss = test(val_loader)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        print('Epoch: {}, Train Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, train_loss, val_loss))\n",
        "\n",
        "    if val_loss < min_val_loss:\n",
        "        best_model = copy.deepcopy(model.state_dict())\n",
        "        min_val_loss = val_loss\n",
        "        epochs_since = 0\n",
        "\n",
        "    epochs_since += 1\n",
        "    if epochs_since > NUM_NON_DECREASING:\n",
        "        print(\"Early stopping engaged\")\n",
        "        break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Train Loss: 0.0719, Validation Loss: 2.9355\n",
            "Epoch: 5, Train Loss: 0.0199, Validation Loss: 1.2888\n",
            "Epoch: 10, Train Loss: 0.0149, Validation Loss: 1.0340\n",
            "Epoch: 15, Train Loss: 0.0133, Validation Loss: 0.8857\n",
            "Epoch: 20, Train Loss: 0.0114, Validation Loss: 0.8126\n",
            "Epoch: 25, Train Loss: 0.0104, Validation Loss: 0.7885\n",
            "Epoch: 30, Train Loss: 0.0097, Validation Loss: 0.7737\n",
            "Epoch: 35, Train Loss: 0.0092, Validation Loss: 0.7704\n",
            "Epoch: 40, Train Loss: 0.0088, Validation Loss: 0.7501\n",
            "Epoch: 45, Train Loss: 0.0085, Validation Loss: 0.7340\n",
            "Epoch: 50, Train Loss: 0.0084, Validation Loss: 0.7223\n",
            "Epoch: 55, Train Loss: 0.0081, Validation Loss: 0.7170\n",
            "Epoch: 60, Train Loss: 0.0078, Validation Loss: 0.7009\n",
            "Epoch: 65, Train Loss: 0.0078, Validation Loss: 0.6814\n",
            "Epoch: 70, Train Loss: 0.0076, Validation Loss: 0.6736\n",
            "Epoch: 75, Train Loss: 0.0075, Validation Loss: 0.6621\n",
            "Epoch: 80, Train Loss: 0.0075, Validation Loss: 0.6558\n",
            "Epoch: 85, Train Loss: 0.0074, Validation Loss: 0.6513\n",
            "Epoch: 90, Train Loss: 0.0073, Validation Loss: 0.6380\n",
            "Epoch: 95, Train Loss: 0.0072, Validation Loss: 0.6299\n",
            "Epoch: 100, Train Loss: 0.0071, Validation Loss: 0.6204\n",
            "Epoch: 105, Train Loss: 0.0072, Validation Loss: 0.6127\n",
            "Epoch: 110, Train Loss: 0.0069, Validation Loss: 0.6040\n",
            "Epoch: 115, Train Loss: 0.0069, Validation Loss: 0.5909\n",
            "Epoch: 120, Train Loss: 0.0068, Validation Loss: 0.5645\n",
            "Epoch: 125, Train Loss: 0.0067, Validation Loss: 0.5451\n",
            "Epoch: 130, Train Loss: 0.0066, Validation Loss: 0.5272\n",
            "Epoch: 135, Train Loss: 0.0063, Validation Loss: 0.4831\n",
            "Epoch: 140, Train Loss: 0.0059, Validation Loss: 0.4279\n",
            "Epoch: 145, Train Loss: 0.0059, Validation Loss: 0.4023\n",
            "Epoch: 150, Train Loss: 0.0057, Validation Loss: 0.3543\n",
            "Epoch: 155, Train Loss: 0.0056, Validation Loss: 0.2934\n",
            "Epoch: 160, Train Loss: 0.0056, Validation Loss: 0.3315\n",
            "Epoch: 165, Train Loss: 0.0055, Validation Loss: 0.3317\n",
            "Epoch: 170, Train Loss: 0.0055, Validation Loss: 0.3079\n",
            "Epoch: 175, Train Loss: 0.0054, Validation Loss: 0.3098\n",
            "Epoch: 180, Train Loss: 0.0053, Validation Loss: 0.3246\n",
            "Epoch: 185, Train Loss: 0.0055, Validation Loss: 0.2829\n",
            "Epoch: 190, Train Loss: 0.0053, Validation Loss: 0.3210\n",
            "Epoch: 195, Train Loss: 0.0053, Validation Loss: 0.2978\n",
            "Epoch: 200, Train Loss: 0.0055, Validation Loss: 0.2794\n",
            "Epoch: 205, Train Loss: 0.0053, Validation Loss: 0.3140\n",
            "Epoch: 210, Train Loss: 0.0052, Validation Loss: 0.2988\n",
            "Epoch: 215, Train Loss: 0.0053, Validation Loss: 0.3003\n",
            "Epoch: 220, Train Loss: 0.0052, Validation Loss: 0.3066\n",
            "Epoch: 225, Train Loss: 0.0051, Validation Loss: 0.3315\n",
            "Epoch: 230, Train Loss: 0.0052, Validation Loss: 0.3315\n",
            "Epoch: 235, Train Loss: 0.0050, Validation Loss: 0.3169\n",
            "Epoch: 240, Train Loss: 0.0051, Validation Loss: 0.2773\n",
            "Epoch: 245, Train Loss: 0.0050, Validation Loss: 0.2773\n",
            "Epoch: 250, Train Loss: 0.0050, Validation Loss: 0.3068\n",
            "Epoch: 255, Train Loss: 0.0050, Validation Loss: 0.2934\n",
            "Epoch: 260, Train Loss: 0.0049, Validation Loss: 0.3067\n",
            "Epoch: 265, Train Loss: 0.0047, Validation Loss: 0.2264\n",
            "Epoch: 270, Train Loss: 0.0046, Validation Loss: 0.2456\n",
            "Epoch: 275, Train Loss: 0.0046, Validation Loss: 0.1983\n",
            "Epoch: 280, Train Loss: 0.0046, Validation Loss: 0.2398\n",
            "Epoch: 285, Train Loss: 0.0047, Validation Loss: 0.2039\n",
            "Epoch: 290, Train Loss: 0.0046, Validation Loss: 0.1974\n",
            "Epoch: 295, Train Loss: 0.0046, Validation Loss: 0.2335\n",
            "Epoch: 300, Train Loss: 0.0045, Validation Loss: 0.2865\n",
            "Epoch: 305, Train Loss: 0.0046, Validation Loss: 0.1911\n",
            "Epoch: 310, Train Loss: 0.0044, Validation Loss: 0.2033\n",
            "Epoch: 315, Train Loss: 0.0046, Validation Loss: 0.1826\n",
            "Epoch: 320, Train Loss: 0.0046, Validation Loss: 0.2620\n",
            "Epoch: 325, Train Loss: 0.0046, Validation Loss: 0.1700\n",
            "Epoch: 330, Train Loss: 0.0045, Validation Loss: 0.2192\n",
            "Epoch: 335, Train Loss: 0.0045, Validation Loss: 0.2104\n",
            "Epoch: 340, Train Loss: 0.0044, Validation Loss: 0.2011\n",
            "Epoch: 345, Train Loss: 0.0046, Validation Loss: 0.2428\n",
            "Epoch: 350, Train Loss: 0.0045, Validation Loss: 0.2074\n",
            "Epoch: 355, Train Loss: 0.0044, Validation Loss: 0.2157\n",
            "Epoch: 360, Train Loss: 0.0045, Validation Loss: 0.2136\n",
            "Epoch: 365, Train Loss: 0.0043, Validation Loss: 0.1665\n",
            "Epoch: 370, Train Loss: 0.0043, Validation Loss: 0.2118\n",
            "Epoch: 375, Train Loss: 0.0046, Validation Loss: 0.2557\n",
            "Epoch: 380, Train Loss: 0.0045, Validation Loss: 0.2367\n",
            "Epoch: 385, Train Loss: 0.0044, Validation Loss: 0.2708\n",
            "Epoch: 390, Train Loss: 0.0043, Validation Loss: 0.2166\n",
            "Epoch: 395, Train Loss: 0.0043, Validation Loss: 0.1966\n",
            "Epoch: 400, Train Loss: 0.0042, Validation Loss: 0.1991\n",
            "Epoch: 405, Train Loss: 0.0044, Validation Loss: 0.1702\n",
            "Epoch: 410, Train Loss: 0.0044, Validation Loss: 0.1830\n",
            "Epoch: 415, Train Loss: 0.0043, Validation Loss: 0.1959\n",
            "Epoch: 420, Train Loss: 0.0043, Validation Loss: 0.1951\n",
            "Epoch: 425, Train Loss: 0.0042, Validation Loss: 0.2062\n",
            "Epoch: 430, Train Loss: 0.0041, Validation Loss: 0.2281\n",
            "Epoch: 435, Train Loss: 0.0042, Validation Loss: 0.1535\n",
            "Epoch: 440, Train Loss: 0.0042, Validation Loss: 0.1924\n",
            "Epoch: 445, Train Loss: 0.0042, Validation Loss: 0.1405\n",
            "Epoch: 450, Train Loss: 0.0039, Validation Loss: 0.1829\n",
            "Epoch: 455, Train Loss: 0.0041, Validation Loss: 0.1659\n",
            "Epoch: 460, Train Loss: 0.0041, Validation Loss: 0.1708\n",
            "Epoch: 465, Train Loss: 0.0041, Validation Loss: 0.1484\n",
            "Epoch: 470, Train Loss: 0.0041, Validation Loss: 0.1931\n",
            "Epoch: 475, Train Loss: 0.0041, Validation Loss: 0.1767\n",
            "Epoch: 480, Train Loss: 0.0042, Validation Loss: 0.1646\n",
            "Epoch: 485, Train Loss: 0.0040, Validation Loss: 0.1448\n",
            "Epoch: 490, Train Loss: 0.0041, Validation Loss: 0.1559\n",
            "Epoch: 495, Train Loss: 0.0041, Validation Loss: 0.1839\n",
            "Epoch: 500, Train Loss: 0.0040, Validation Loss: 0.1825\n",
            "Epoch: 505, Train Loss: 0.0040, Validation Loss: 0.1843\n",
            "Epoch: 510, Train Loss: 0.0041, Validation Loss: 0.1232\n",
            "Epoch: 515, Train Loss: 0.0041, Validation Loss: 0.1666\n",
            "Epoch: 520, Train Loss: 0.0041, Validation Loss: 0.1556\n",
            "Epoch: 525, Train Loss: 0.0040, Validation Loss: 0.1833\n",
            "Epoch: 530, Train Loss: 0.0040, Validation Loss: 0.1395\n",
            "Epoch: 535, Train Loss: 0.0042, Validation Loss: 0.1464\n",
            "Epoch: 540, Train Loss: 0.0041, Validation Loss: 0.2033\n",
            "Epoch: 545, Train Loss: 0.0040, Validation Loss: 0.1146\n",
            "Epoch: 550, Train Loss: 0.0040, Validation Loss: 0.1582\n",
            "Epoch: 555, Train Loss: 0.0041, Validation Loss: 0.1395\n",
            "Epoch: 560, Train Loss: 0.0040, Validation Loss: 0.1886\n",
            "Epoch: 565, Train Loss: 0.0040, Validation Loss: 0.1319\n",
            "Epoch: 570, Train Loss: 0.0040, Validation Loss: 0.1803\n",
            "Epoch: 575, Train Loss: 0.0040, Validation Loss: 0.1568\n",
            "Epoch: 580, Train Loss: 0.0040, Validation Loss: 0.1846\n",
            "Epoch: 585, Train Loss: 0.0041, Validation Loss: 0.1712\n",
            "Epoch: 590, Train Loss: 0.0041, Validation Loss: 0.1391\n",
            "Epoch: 595, Train Loss: 0.0041, Validation Loss: 0.1454\n",
            "Epoch: 600, Train Loss: 0.0041, Validation Loss: 0.1428\n",
            "Epoch: 605, Train Loss: 0.0040, Validation Loss: 0.1271\n",
            "Epoch: 610, Train Loss: 0.0040, Validation Loss: 0.1562\n",
            "Epoch: 615, Train Loss: 0.0040, Validation Loss: 0.1188\n",
            "Epoch: 620, Train Loss: 0.0041, Validation Loss: 0.1223\n",
            "Epoch: 625, Train Loss: 0.0040, Validation Loss: 0.1290\n",
            "Epoch: 630, Train Loss: 0.0040, Validation Loss: 0.1252\n",
            "Epoch: 635, Train Loss: 0.0038, Validation Loss: 0.1393\n",
            "Epoch: 640, Train Loss: 0.0041, Validation Loss: 0.1388\n",
            "Epoch: 645, Train Loss: 0.0042, Validation Loss: 0.1467\n",
            "Epoch: 650, Train Loss: 0.0042, Validation Loss: 0.1303\n",
            "Epoch: 655, Train Loss: 0.0040, Validation Loss: 0.1498\n",
            "Epoch: 660, Train Loss: 0.0040, Validation Loss: 0.1468\n",
            "Epoch: 665, Train Loss: 0.0042, Validation Loss: 0.1146\n",
            "Epoch: 670, Train Loss: 0.0041, Validation Loss: 0.1188\n",
            "Epoch: 675, Train Loss: 0.0040, Validation Loss: 0.1209\n",
            "Epoch: 680, Train Loss: 0.0041, Validation Loss: 0.1460\n",
            "Epoch: 685, Train Loss: 0.0041, Validation Loss: 0.1415\n",
            "Epoch: 690, Train Loss: 0.0042, Validation Loss: 0.1365\n",
            "Epoch: 695, Train Loss: 0.0040, Validation Loss: 0.1605\n",
            "Epoch: 700, Train Loss: 0.0039, Validation Loss: 0.1595\n",
            "Epoch: 705, Train Loss: 0.0041, Validation Loss: 0.1512\n",
            "Epoch: 710, Train Loss: 0.0039, Validation Loss: 0.1360\n",
            "Epoch: 715, Train Loss: 0.0039, Validation Loss: 0.1505\n",
            "Epoch: 720, Train Loss: 0.0040, Validation Loss: 0.1506\n",
            "Epoch: 725, Train Loss: 0.0039, Validation Loss: 0.1452\n",
            "Epoch: 730, Train Loss: 0.0040, Validation Loss: 0.1720\n",
            "Epoch: 735, Train Loss: 0.0040, Validation Loss: 0.1385\n",
            "Epoch: 740, Train Loss: 0.0039, Validation Loss: 0.1113\n",
            "Epoch: 745, Train Loss: 0.0041, Validation Loss: 0.1193\n",
            "Epoch: 750, Train Loss: 0.0041, Validation Loss: 0.1212\n",
            "Epoch: 755, Train Loss: 0.0041, Validation Loss: 0.1196\n",
            "Epoch: 760, Train Loss: 0.0039, Validation Loss: 0.1604\n",
            "Epoch: 765, Train Loss: 0.0040, Validation Loss: 0.1371\n",
            "Epoch: 770, Train Loss: 0.0040, Validation Loss: 0.1323\n",
            "Epoch: 775, Train Loss: 0.0040, Validation Loss: 0.1408\n",
            "Epoch: 780, Train Loss: 0.0040, Validation Loss: 0.1233\n",
            "Epoch: 785, Train Loss: 0.0041, Validation Loss: 0.1473\n",
            "Epoch: 790, Train Loss: 0.0041, Validation Loss: 0.1372\n",
            "Epoch: 795, Train Loss: 0.0040, Validation Loss: 0.1307\n",
            "Epoch: 800, Train Loss: 0.0040, Validation Loss: 0.1258\n",
            "Epoch: 805, Train Loss: 0.0040, Validation Loss: 0.1136\n",
            "Epoch: 810, Train Loss: 0.0040, Validation Loss: 0.1244\n",
            "Epoch: 815, Train Loss: 0.0040, Validation Loss: 0.1631\n",
            "Epoch: 820, Train Loss: 0.0040, Validation Loss: 0.1099\n",
            "Epoch: 825, Train Loss: 0.0039, Validation Loss: 0.1272\n",
            "Epoch: 830, Train Loss: 0.0040, Validation Loss: 0.1345\n",
            "Epoch: 835, Train Loss: 0.0040, Validation Loss: 0.1172\n",
            "Epoch: 840, Train Loss: 0.0040, Validation Loss: 0.1201\n",
            "Epoch: 845, Train Loss: 0.0040, Validation Loss: 0.1360\n",
            "Epoch: 850, Train Loss: 0.0041, Validation Loss: 0.1555\n",
            "Epoch: 855, Train Loss: 0.0040, Validation Loss: 0.1046\n",
            "Epoch: 860, Train Loss: 0.0041, Validation Loss: 0.1332\n",
            "Epoch: 865, Train Loss: 0.0040, Validation Loss: 0.1018\n",
            "Epoch: 870, Train Loss: 0.0039, Validation Loss: 0.1438\n",
            "Epoch: 875, Train Loss: 0.0041, Validation Loss: 0.1396\n",
            "Epoch: 880, Train Loss: 0.0040, Validation Loss: 0.1802\n",
            "Epoch: 885, Train Loss: 0.0040, Validation Loss: 0.1264\n",
            "Epoch: 890, Train Loss: 0.0040, Validation Loss: 0.1237\n",
            "Epoch: 895, Train Loss: 0.0040, Validation Loss: 0.1740\n",
            "Epoch: 900, Train Loss: 0.0039, Validation Loss: 0.1524\n",
            "Epoch: 905, Train Loss: 0.0039, Validation Loss: 0.1361\n",
            "Epoch: 910, Train Loss: 0.0040, Validation Loss: 0.1076\n",
            "Epoch: 915, Train Loss: 0.0039, Validation Loss: 0.1155\n",
            "Epoch: 920, Train Loss: 0.0040, Validation Loss: 0.1204\n",
            "Epoch: 925, Train Loss: 0.0039, Validation Loss: 0.1209\n",
            "Epoch: 930, Train Loss: 0.0040, Validation Loss: 0.1228\n",
            "Epoch: 935, Train Loss: 0.0040, Validation Loss: 0.1566\n",
            "Epoch: 940, Train Loss: 0.0039, Validation Loss: 0.1466\n",
            "Epoch: 945, Train Loss: 0.0040, Validation Loss: 0.1420\n",
            "Epoch: 950, Train Loss: 0.0039, Validation Loss: 0.1309\n",
            "Epoch: 955, Train Loss: 0.0040, Validation Loss: 0.1210\n",
            "Epoch: 960, Train Loss: 0.0039, Validation Loss: 0.1203\n",
            "Epoch: 965, Train Loss: 0.0039, Validation Loss: 0.1345\n",
            "Epoch: 970, Train Loss: 0.0040, Validation Loss: 0.1238\n",
            "Epoch: 975, Train Loss: 0.0040, Validation Loss: 0.0988\n",
            "Epoch: 980, Train Loss: 0.0041, Validation Loss: 0.1094\n",
            "Epoch: 985, Train Loss: 0.0038, Validation Loss: 0.1752\n",
            "Epoch: 990, Train Loss: 0.0039, Validation Loss: 0.1217\n",
            "Epoch: 995, Train Loss: 0.0040, Validation Loss: 0.1192\n",
            "Epoch: 1000, Train Loss: 0.0039, Validation Loss: 0.1238\n",
            "Epoch: 1005, Train Loss: 0.0039, Validation Loss: 0.1757\n",
            "Epoch: 1010, Train Loss: 0.0039, Validation Loss: 0.1147\n",
            "Epoch: 1015, Train Loss: 0.0038, Validation Loss: 0.1379\n",
            "Early stopping engaged\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jxo5JQayXOtA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "304ebe62-b6ce-4c0c-cb20-29650af0fdb8"
      },
      "source": [
        "test_loss = test(test_loader, state_dict=best_model)\n",
        "print('Final Test Loss: {:.4f}'.format(test_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Test Loss: 0.1286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZPhJQSsPim4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(best_model, os.path.join('/', 'content', 'drive', 'My Drive', 'projects', 'trade_democratization', 'best_model_non_recurrent.pkl'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFv1nR6_nj2b",
        "colab_type": "text"
      },
      "source": [
        "After tuning hyperparameters this seems to be a good final test set loss. "
      ]
    }
  ]
}