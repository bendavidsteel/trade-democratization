{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trade_diffusion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOo3GQj2r2W21U4a7IYPduf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bendavidsteel/trade-democratization/blob/master/trade_diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HlIailEbTFk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "f9d0d141-fa1f-4eab-97a9-13d53c44b0dc"
      },
      "source": [
        "import itertools\n",
        "import math\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32JRsdyv5Pg1",
        "colab_type": "text"
      },
      "source": [
        "In this project we will be linking the [V-Dem dataset](https://www.v-dem.net/en) and the [CEPII Trade History dataset](http://www.cepii.fr/cepii/en/bdd_modele/bdd.asp) into a time series graph dataset, in an attempt to investigate how bilateral trade affects the democratization of nations over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC4nveOW2GY4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "0f38f7f4-3ea0-4b54-e6dc-f28a2fac5c88"
      },
      "source": [
        "dataset_path = os.path.join('/', 'content', 'drive', 'My Drive', 'projects', 'trade_democratization', 'dataset', 'raw')\n",
        "# here we load the democracy indices from the V-Dem dataset\n",
        "vdem_nodes = pd.read_csv(os.path.join(dataset_path, \"V-Dem-CY-Core-v10.csv\"))\n",
        "vdem_nodes.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5744ed597d27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'content'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'My Drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'projects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'trade_democratization'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dataset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# here we load the democracy indices from the V-Dem dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvdem_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"V-Dem-CY-Core-v10.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvdem_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2037\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2038\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m     \"\"\"\n\u001b[1;32m    544\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCznS4Fd5dYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tradhist_gdppop = pd.read_excel(os.path.join(dataset_path, \"TRADHIST_GDP_POP.xlsx\"))\n",
        "tradhist_gdppop.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoTWmHxV9mQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now we load the time invariant bilateral data from the CEPII TradHist dataset\n",
        "tradhist_timeinvar = pd.read_excel(os.path.join(dataset_path, \"TRADHIST_GRAVITY_BILATERAL_TIME_INVARIANT.xlsx\"))\n",
        "tradhist_timeinvar.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xklb6hvtDGIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# then load the three files that constitute the bilateral historical non trade related data such as distances between countries and colonial status\n",
        "tradhist_timevar_frames = []\n",
        "for idx in range(1, 4):\n",
        "    tradhist_timevar_frames.append(pd.read_excel(os.path.join(dataset_path, \"TRADHIST_GRAVITY_BILATERAL_TIME_VARIANT_{}.xlsx\".format(idx))))\n",
        "tradhist_timevar = pd.concat(tradhist_timevar_frames)\n",
        "tradhist_timevar.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3Kl0JiJIHWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tradhist_bitrade_frames = []\n",
        "for idx in range(1, 4):\n",
        "    tradhist_bitrade_frames.append(pd.read_excel(os.path.join(dataset_path, \"TRADHIST_BITRADE_BITARIFF_{}.xlsx\".format(idx))))\n",
        "tradhist_bitrade = pd.concat(tradhist_bitrade_frames)\n",
        "tradhist_bitrade.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Agoze7A6DWB",
        "colab_type": "text"
      },
      "source": [
        "Previous studies how shown that geographical closeness and colonizer/colony status affects the diffusion of democracy. It seems likely following from this that trade would result in some democratization diffusion effect also."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf9e5qjL6uah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TradHist dataset has data for USSR separate than from Russia\n",
        "# USA : USA\n",
        "# Russia: RUS, USSR (VDem uses RUS only, TradHist uses RUS and USSR)\n",
        "colours = ['r', 'b']\n",
        "\n",
        "# get high level indicators of democracy\n",
        "ctry_demind = {}\n",
        "ctry_demind[\"USA\"] = vdem_nodes[(vdem_nodes['country_text_id'] == \"USA\") & (vdem_nodes['year'] >= vdem_nodes['codingstart_contemp'])][['year', 'v2x_libdem']]\n",
        "ctry_demind[\"RUS\"] = vdem_nodes[(vdem_nodes['country_text_id'] == \"RUS\") & (vdem_nodes['year'] >= vdem_nodes['codingstart_contemp'])][['year', 'v2x_libdem']]\n",
        "\n",
        "\n",
        "# get imports of one country from the other\n",
        "ctry_trade = {}\n",
        "ctry_trade[\"USA\"] = tradhist_bitrade[((tradhist_bitrade['iso_o'] == \"RUS\") | (tradhist_bitrade['iso_o'] == \"USSR\"))\n",
        "                                     & (tradhist_bitrade['iso_d'] == \"USA\") \n",
        "                                     & (tradhist_bitrade['year'] >= 1900)][['year', 'FLOW']].sort_values(by=['year'])\n",
        "ctry_trade[\"RUS\"] = tradhist_bitrade[(tradhist_bitrade['iso_o'] == \"USA\")\n",
        "                                     & ((tradhist_bitrade['iso_d'] == \"RUS\") | (tradhist_bitrade['iso_d'] == \"USSR\"))\n",
        "                                     & (tradhist_bitrade['year'] >= 1900)][['year', 'FLOW']].sort_values(by=['year'])\n",
        "\n",
        "# plot the data\n",
        "fig, ax1 = plt.subplots(figsize=(13, 6))\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "ax1.plot(ctry_demind[\"USA\"].values[:, 0], ctry_demind[\"USA\"].values[:, 1], 'b-', label=\"USA Liberal Democracy Index\")\n",
        "ax2.plot(ctry_trade[\"USA\"].values[:, 0], ctry_trade[\"USA\"].values[:, 1], 'b--', label=\"Imports from Russia to the USA\")\n",
        "ax1.plot(ctry_demind[\"RUS\"].values[:, 0], ctry_demind[\"RUS\"].values[:, 1], 'r-', label=\"Russia Liberal Democracy Index\")\n",
        "ax2.plot(ctry_trade[\"RUS\"].values[:, 0], ctry_trade[\"RUS\"].values[:, 1], 'r--', label=\"Imports from the USA to Russia\")\n",
        "\n",
        "ax1.legend(loc = \"upper left\")\n",
        "ax2.legend(loc = \"upper right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPU1rjWGvJ-_",
        "colab_type": "text"
      },
      "source": [
        "The figure seems to show some non-linear relationship between trade and liberal democracy index. The casuality direction of this is not clear however."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA5q_mFCSW7j",
        "colab_type": "text"
      },
      "source": [
        "These figures show us that normalising the trade data will be important to prevent any relationships being overshadowed by GDP growth, even if GDP growth will likely be another key factor that affects democratization. We will therefore be using GDP and population as a node feature, and normalising trade data by the GDP of a country at that time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbsJzgem6EKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ctry_gdp_trade = {}\n",
        "usa_gdp = tradhist_gdppop[(tradhist_gdppop['iso'] == 'USA') & (tradhist_gdppop['year'] >= 1900)][['year', 'GDP']].sort_values(by=['year'])\n",
        "ctry_gdp_trade['USA'] = pd.merge(usa_gdp, ctry_trade['USA'], how='inner')\n",
        "rus_gdp = tradhist_gdppop[((tradhist_gdppop['iso'] == 'RUS') | (tradhist_gdppop['iso'] == 'USSR')) & (tradhist_gdppop['year'] >= 1900)][['year', 'GDP']].sort_values(by=['year'])\n",
        "ctry_gdp_trade['RUS'] = pd.merge(rus_gdp, ctry_trade['RUS'], how='inner')\n",
        "\n",
        "# plot the data\n",
        "fig, ax1 = plt.subplots(figsize=(13, 6))\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "ax1.plot(ctry_demind[\"USA\"].values[:, 0], ctry_demind[\"USA\"].values[:, 1], 'b-', label=\"USA Liberal Democracy Index\")\n",
        "ax2.plot(ctry_gdp_trade[\"USA\"][['year']].values[:, 0], ctry_gdp_trade[\"USA\"][['FLOW']].values[:, 0] / ctry_gdp_trade[\"USA\"][['GDP']].values[:, 0], 'b--', label=\"GDP normalised imports from Russia to the USA\")\n",
        "ax1.plot(ctry_demind[\"RUS\"].values[:, 0], ctry_demind[\"RUS\"].values[:, 1], 'r-', label=\"Russia Liberal Democracy Index\")\n",
        "ax2.plot(ctry_gdp_trade[\"RUS\"][['year']].values[:, 0], ctry_gdp_trade[\"RUS\"][['FLOW']].values[:, 0] / ctry_gdp_trade[\"RUS\"][['GDP']].values[:, 0], 'r--', label=\"GDP normalised imports from the USA to Russia\")\n",
        "\n",
        "ax1.legend(loc = \"upper left\")\n",
        "ax2.legend(loc = \"upper right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1DmuOhCX0oJ",
        "colab_type": "text"
      },
      "source": [
        "This normalised data does seems to show a significant correlation between Russian imports of American goods with democratization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiIfULs5xSfQ",
        "colab_type": "text"
      },
      "source": [
        "We now need to find a way of linking the two datasets. The trade history dataset uses the ISO country codes. However the V-Dem dataset seems to have its own country numbering system, and the three letter shortened name isn't specified in the reference manual to be the ISO name. Lets look at the union of the country codes for the two datasets and the country codes which are unique to each dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sogvaCPSzEOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vdem_country_codes = set(vdem_nodes['country_text_id'].unique())\n",
        "tradhist_country_codes = set(tradhist_timevar['iso_o'].unique())\n",
        "shared_codes = vdem_country_codes & tradhist_country_codes\n",
        "vdem_unique_codes = vdem_country_codes - shared_codes\n",
        "tradhist_unique_codes = tradhist_country_codes - shared_codes\n",
        "\n",
        "def print_set(code_set, per_line = 20):\n",
        "    code_ordered = sorted(list(code_set))\n",
        "    for i in range(0, len(code_ordered), per_line):\n",
        "        print(code_ordered[i:i + per_line])\n",
        "\n",
        "print(\"Shared Country Codes:\")\n",
        "print_set(shared_codes)\n",
        "print()\n",
        "print(\"VDem Only Country Codes:\")\n",
        "print_set(vdem_unique_codes)\n",
        "print()\n",
        "print(\"TradHist Only Country Codes:\")\n",
        "print_set(tradhist_unique_codes)\n",
        "print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xzX42cQYSSe",
        "colab_type": "text"
      },
      "source": [
        "The major difference seems to be that the V-Dem dataset uses a consistent code for the varying regimes of a 'country', compared to the CEPII dataset using for example USSR for the Soviet Union instead of continuing to use RUS. We need to find if there are any other examples of this and create a mapping system. We will not include trade participants from the trade dataset that are current subareas of sovereignties, as they provide no additional data to correlate with nation democratization given that their owning sovereignty is already featured in the dataset. However we will lump together formerly separate sovereignties that have no additional coding in the V-Dem dataset, such as Prince Edward Island and Canada.\n",
        "\n",
        "For former countries that cover the land of multiple modern countries, we will associate the trade of those countries at the time with all of the modern countries in their former land area."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pizS4fVTbTNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_mapping(vdem_nodes, tradhist_timevar):   \n",
        "\n",
        "    vdem_country_codes = set(vdem_nodes['country_text_id'].unique())\n",
        "    tradhist_country_codes = set(tradhist_timevar['iso_o'].unique())\n",
        "    shared_codes = vdem_country_codes & tradhist_country_codes\n",
        "\n",
        "    mapped_codes = [['RUS', 'USSR'], ['YEM', 'ADEN'], ['CAF', 'AOFAEF', 'FRAAEF'], ['TCD', 'AOFAEF', 'FRAAEF'], ['COD', 'AOFAEF', 'FRAAEF'], ['HRV', 'AUTHUN', 'YUG'], ['SVK', 'CZSK', 'AUTHUN'], \n",
        "                ['SVN', 'AUTHUN', 'YUG'], ['UKR', 'AUTHUN', 'USSR'], ['ALB', 'AUTHUN'], ['BIH', 'AUTHUN', 'YUG'], ['MNE', 'AUTHUN', 'YUG'], ['CAN', 'CANPRINCED', 'CANQBCONT', 'NFLD'], \n",
        "                ['CZE', 'CZSK', 'AUTHUN'], ['DDR', 'EDEU'], ['MYS', 'FEDMYS', 'UNFEDMYS', 'GBRBORNEO'], ['BFA', 'FRAAOF'], ['GNQ', 'FRAAOF'], ['LUX', 'ZOLL'], \n",
        "                ['ZZB', 'ZANZ', 'GBRAFRI'], ['ZAF', 'ZAFTRA', 'ZAFORA', 'ZAFNAT', 'ZAPCAF', 'GBRAFRI'], ['MKD', 'YUG'], ['SRB', 'YUG'], ['POL', 'USSR'], ['COM', 'MYT'], ['ROU', 'ROM'], \n",
        "                ['MWI', 'RHOD', 'GBRAFRI'], ['ZMB', 'RHOD', 'GBRAFRI'], ['ZWE', 'RHOD', 'GBRAFRI'], ['SGP', 'STRAITS'], ['DEU', 'WDEU'], ['SML', 'GBRSOM', 'ITASOM'], ['GBR', 'ULSTER'], \n",
        "                ['RWA', 'RWABDI'], ['SOM', 'ITASOM'], ['MAR', 'MARESP'], ['FRA', 'OLDENB'], ['DNK', 'SCHLES'], ['LBN', 'SYRLBN', 'OTTO'], ['SYR', 'SYRLBN'], ['CYP', 'OTTO', 'GBRMEDI'], \n",
        "                ['TUR', 'OTTO'], ['STP', 'PRTAFRI'], ['AGO', 'PRTAFRI'], ['MOZ', 'PRTAFRI'], ['GNB', 'PRTWAFRI'], ['KHM', 'INDOCHI'], ['LAO', 'INDOCHI'], ['VNM', 'INDOCHI'], \n",
        "                ['ERI', 'ITAEAFRI', 'GBRAFRI'], ['TTO', 'GBRWINDIES'], ['SLE', 'GBRWAFRI'], ['GMB', 'GBRWAFRI'], ['TGO', 'GBRWAFRI'], ['EGY', 'OTTO'],\n",
        "                ['PNG', 'GBRPAPUA'], ['MLT', 'GBRMEDI'], ['BGD', 'GBRIND'], ['BTN', 'GBRIND'], ['IND', 'GBRIND'], ['MDV', 'GBRIND'], ['NPL', 'GBRIND'], ['PAK', 'GBRIND'], \n",
        "                ['LKA', 'GBRIND'], ['CMR', 'GBRAFRI', 'FRAAFRI'], ['KEN', 'GBRAFRI'], ['SYC', 'GBRAFRI'], ['SDN', 'GBRAFRI'], ['UGA', 'GBRAFRI'], ['LSO', 'GBRAFRI'], \n",
        "                ['SWZ', 'GBRAFRI']]\n",
        "\n",
        "    # validate my matches\n",
        "    code_count = {}\n",
        "    for codes in mapped_codes:\n",
        "        matched_to_vdem = 0\n",
        "        for code in codes:\n",
        "            if len(code) == 3:\n",
        "                if code in vdem_country_codes:\n",
        "                    if code in code_count:\n",
        "                        code_count[code] += 1\n",
        "                    else:\n",
        "                        code_count[code] = 1\n",
        "                    matched_to_vdem += 1\n",
        "\n",
        "        if matched_to_vdem == 0:\n",
        "            raise ValueError(\"{} country code set matched to no VDem node\".format(codes))\n",
        "        elif matched_to_vdem > 1:\n",
        "            raise ValueError(\"{} country code set matched to more than one VDem node\".format(codes))\n",
        "\n",
        "        if codes[0] not in vdem_country_codes:\n",
        "            raise ValueError(\"VDem code should be first in list {}.\".format(codes))\n",
        "\n",
        "    for code in code_count:\n",
        "        if code_count[code] != 1:\n",
        "            raise ValueError(\"VDem code {} matched to more than one country code set\".format(code))\n",
        "\n",
        "    for code in shared_codes:\n",
        "        if code not in code_count:\n",
        "            mapped_codes.append([code])\n",
        "\n",
        "    return mapped_codes\n",
        "\n",
        "#mapped_codes = get_mapping(vdem_nodes, tradhist_timevar)\n",
        "\n",
        "#print_set(mapped_codes, 10)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ7KhCAK49Ov",
        "colab_type": "text"
      },
      "source": [
        "Now that we have a mapping between the two datasets we can start putting them together. We will only be using data from 1900 to 2014, as before 1900 the VDem dataset is noted to be much less accurate, and trade data is increasingly absent for smaller nations. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQjO9Iw2DIFe",
        "colab_type": "text"
      },
      "source": [
        "This is where we will first use our graph learning library of choice, PyTorch Geometric. PyTorch Geometric provides a base class with which to structure the compilation of a dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBRnbTWdDX1U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c6f3104d-2060-443c-cf00-7b0013d056f6"
      },
      "source": [
        "!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install torch-scatter==2.0.4+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-sparse==0.6.5+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-cluster==1.5.5+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-spline-conv==1.2.0+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-geometric"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.5.1+cu101 in /usr/local/lib/python3.6/dist-packages (1.5.1+cu101)\n",
            "Requirement already satisfied: torchvision==0.6.1+cu101 in /usr/local/lib/python3.6/dist-packages (0.6.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-scatter==2.0.4+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_scatter-2.0.4%2Bcu101-cp36-cp36m-linux_x86_64.whl (12.2MB)\n",
            "\u001b[K     |████████████████████████████████| 12.3MB 3.8MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.4\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-sparse==0.6.5+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_sparse-0.6.5%2Bcu101-cp36-cp36m-linux_x86_64.whl (21.6MB)\n",
            "\u001b[K     |████████████████████████████████| 21.6MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-sparse==0.6.5+cu101) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-sparse==0.6.5+cu101) (1.18.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.5\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-cluster==1.5.5+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_cluster-1.5.5%2Bcu101-cp36-cp36m-linux_x86_64.whl (22.0MB)\n",
            "\u001b[K     |████████████████████████████████| 22.0MB 58.5MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.5.5\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-spline-conv==1.2.0+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_spline_conv-1.2.0%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.3MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3MB 2.1MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.0\n",
            "Collecting torch-geometric\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/18/93b190226d09958be96919fd50c55d28f83f1a1b9260a2b33499f9d86728/torch_geometric-1.6.0.tar.gz (172kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.5.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.48.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.0.5)\n",
            "Collecting rdflib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/6b/6454aa1db753c0f8bc265a5bd5c10b5721a4bb24160fb4faf758cf6be8a1/rdflib-5.0.0-py3-none-any.whl (231kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.10.0)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n",
            "Collecting ase\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/70/a8b1a7831193aa228defd805891c534d3e4717c8988147522e673458ddce/ase-3.19.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 11.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.11.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (49.1.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.31.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2020.6.20)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.7)\n",
            "Collecting isodate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (1.12.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from ase->torch-geometric) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->torch-geometric) (1.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ase->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ase->torch-geometric) (0.10.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-1.6.0-cp36-none-any.whl size=296339 sha256=9c9d4c8201cb00615ed96e37b891ee874c10558aecaae4d6de30875582a79d50\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/7f/33/acea5809d8580a7adf60dcd6d04f5fc50a7f983040f68be1ff\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: isodate, rdflib, ase, torch-geometric\n",
            "Successfully installed ase-3.19.1 isodate-0.6.0 rdflib-5.0.0 torch-geometric-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH963gAoAi0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch_geometric as geo\n",
        "import tqdm"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqJ2meMQOV9a",
        "colab_type": "text"
      },
      "source": [
        "We will initially create a simple model trying to predict next year's democracy indicators from this year's democracy indicators, alongside trade and other geopolitical data. This has the disadvantage compared to a more traditional RNN of not having long term memory, but it will create a good baseline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QEyATteMIFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_last_valid(df, col):\n",
        "    valid_rows = df[df[col].isnull() == False]\n",
        "    if (len(valid_rows) > 0):\n",
        "        return valid_rows.sort_values('year')[col].values[-1]\n",
        "    else:\n",
        "        return 0"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB5Carkl48Iy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TradeDemoYearByYearDataset(geo.data.InMemoryDataset):\n",
        "    def __init__(self, root, transform=None, pre_transform=None):\n",
        "\n",
        "        self.norm_stats = os.path.join(root, \"processed\", \"norm_stats.pt\")\n",
        "        self.node_dict = os.path.join(root, \"processed\", \"node_dict.pt\")\n",
        "        self.year_start = 1900\n",
        "        self.year_end = 2015\n",
        "\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        files = []\n",
        "        # vdem dataset\n",
        "        files.append(\"V-Dem-CY-Core-v10.csv\")\n",
        "        # gdp and population data\n",
        "        files.append(\"TRADHIST_GDP_POP.xlsx\")\n",
        "        # time invariant bilateral data such as distance, common language\n",
        "        files.append(\"TRADHIST_GRAVITY_BILATERAL_TIME_INVARIANT.xlsx\")\n",
        "        # time variant non trade bilateral data such as colony status\n",
        "        for idx in range(1, 4):\n",
        "            files.append(\"TRADHIST_GRAVITY_BILATERAL_TIME_VARIANT_{}.xlsx\".format(idx))\n",
        "        # historical bilateral trade and tariff data\n",
        "        for idx in range(1, 4):\n",
        "            files.append(\"TRADHIST_BITRADE_BITARIFF_{}.xlsx\".format(idx))\n",
        "\n",
        "        return files\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['traddem.pt']\n",
        "\n",
        "    def process(self):\n",
        "        # Read data into Data object.\n",
        "        vdem_nodes = pd.read_csv(os.path.join(self.raw_dir, \"V-Dem-CY-Core-v10.csv\"))\n",
        "\n",
        "        tradhist_gdppop = pd.read_excel(os.path.join(self.raw_dir, \"TRADHIST_GDP_POP.xlsx\"))\n",
        "\n",
        "        tradhist_timeinvar = pd.read_excel(os.path.join(self.raw_dir, \"TRADHIST_GRAVITY_BILATERAL_TIME_INVARIANT.xlsx\"))\n",
        "\n",
        "        tradhist_timevar_frames = []\n",
        "        for idx in range(1, 4):\n",
        "            tradhist_timevar_frames.append(pd.read_excel(os.path.join(self.raw_dir, \"TRADHIST_GRAVITY_BILATERAL_TIME_VARIANT_{}.xlsx\".format(idx))))\n",
        "        tradhist_timevar = pd.concat(tradhist_timevar_frames)\n",
        "\n",
        "        tradhist_bitrade_frames = []\n",
        "        for idx in range(1, 4):\n",
        "            tradhist_bitrade_frames.append(pd.read_excel(os.path.join(self.raw_dir, \"TRADHIST_BITRADE_BITARIFF_{}.xlsx\".format(idx))))\n",
        "        tradhist_bitrade = pd.concat(tradhist_bitrade_frames)\n",
        "\n",
        "        country_mapping = get_mapping(vdem_nodes, tradhist_timevar)\n",
        "\n",
        "        num_countries = len(country_mapping)\n",
        "        num_node_features = 2 + 1 # include GDP and population data, and democracy data from last year\n",
        "        num_node_targets = 1 # averaged 5 main indicators of democracy from the VDem dataset\n",
        "        num_edge_features = 7 # Trade flow, current colony relationship, ever a colony, distance, maritime distance, common language, and shared border\n",
        "\n",
        "        all_years = []\n",
        "        years_metadata = []\n",
        "        year_start = self.year_start\n",
        "        year_end = self.year_end\n",
        "\n",
        "        for year_idx, year in tqdm.tqdm(enumerate(range(year_start, year_end))):\n",
        "\n",
        "            year_metadata = {}\n",
        "            year_metadata[\"year\"] = year\n",
        "\n",
        "            year_edge_attr = []\n",
        "            year_edge_index = []\n",
        "\n",
        "            node_features = []\n",
        "            node_target = []\n",
        "            mapping_to_node_indexes = {}\n",
        "\n",
        "            vdem_this_year = vdem_nodes[(vdem_nodes['year'] == year)]\n",
        "            vdem_next_year = vdem_nodes[(vdem_nodes['year'] == year + 1)]\n",
        "\n",
        "            gdppop_year_span = tradhist_gdppop[tradhist_gdppop['year'].isin(list(range(year_start, year + 1)))]\n",
        "\n",
        "            for country_idx, country_codes in enumerate(country_mapping):\n",
        "                # check if this year and next is coded in VDem\n",
        "                vdem_this_year_cty = vdem_this_year[(vdem_this_year['country_text_id'] == country_codes[0])][['v2x_polyarchy', 'v2x_libdem', 'v2x_partipdem', 'v2x_delibdem', 'v2x_egaldem']]\n",
        "                vdem_next_year_cty = vdem_next_year[(vdem_next_year['country_text_id'] == country_codes[0])][['v2x_polyarchy', 'v2x_libdem', 'v2x_partipdem', 'v2x_delibdem', 'v2x_egaldem']]\n",
        "                if ((len(vdem_this_year_cty) == 0) | \\\n",
        "                    (len(vdem_next_year_cty) == 0) | \\\n",
        "                    (vdem_this_year_cty.isnull().values.any()) | \\\n",
        "                    (vdem_next_year_cty.isnull().values.any())):\n",
        "                    continue\n",
        "\n",
        "                country_features = np.zeros((num_node_features, 1))\n",
        "                country_targets = np.zeros((num_node_targets, 1))\n",
        "\n",
        "                # look for last time there was valid value, use 0 otherwise\n",
        "                gdppop_year_cty = gdppop_year_span[gdppop_year_span['iso'].isin(country_codes)]\n",
        "                country_features[0] = get_last_valid(gdppop_year_cty, 'GDP')\n",
        "                country_features[1] = get_last_valid(gdppop_year_cty, 'POP')\n",
        "\n",
        "                country_features[2] = np.mean(vdem_this_year_cty.values.reshape(-1, 1))\n",
        "                country_targets[0] = np.mean(vdem_next_year_cty.values.reshape(-1, 1))\n",
        "\n",
        "                mapping_to_node_indexes[country_idx] = len(node_features)\n",
        "                node_features.append(np.nan_to_num(country_features))\n",
        "                node_target.append(np.nan_to_num(country_targets))\n",
        "\n",
        "            \n",
        "            year_metadata[\"node_mapping\"] = mapping_to_node_indexes\n",
        "\n",
        "            timevar_time_span = tradhist_timevar[tradhist_timevar['year'].isin(list(range(year_start, year + 1)))]\n",
        "            bitrade_time_span = tradhist_bitrade[tradhist_bitrade['year'].isin(list(range(year_start, year + 1)))]\n",
        "\n",
        "            # now that all nodes are in this graph have set indexes, we can add the edges with the correct indexes too\n",
        "            for country_a_idx, node_a_idx in mapping_to_node_indexes.items():\n",
        "\n",
        "                country_codes_a = country_mapping[country_a_idx]\n",
        "\n",
        "                time_invar_cty = tradhist_timeinvar[tradhist_timeinvar['iso_d'].isin(country_codes_a)]\n",
        "\n",
        "                timevar_span_cty = timevar_time_span[timevar_time_span['iso_d'].isin(country_codes_a)]\n",
        "                bitrade_span_cty = bitrade_time_span[bitrade_time_span['iso_d'].isin(country_codes_a)]\n",
        "\n",
        "                # we want to normalise imports to a country by the sum of imports for that year\n",
        "                cty_edges = []\n",
        "                cty_year_imports = 0\n",
        "\n",
        "                for country_b_idx, node_b_idx in mapping_to_node_indexes.items():\n",
        "                    # self links don't really make sense to include in the dataset\n",
        "                    if country_a_idx == country_b_idx:\n",
        "                        continue\n",
        "                    \n",
        "                    country_codes_b = country_mapping[country_b_idx]\n",
        "\n",
        "                    bilateral_attr = np.zeros((num_edge_features, 1))\n",
        "                    # for situations where we have multiple trade links two mapped countries due to how we define a country, we will simply take the last link for now\n",
        "                    time_invar_attrs = time_invar_cty[time_invar_cty['iso_o'].isin(country_codes_b)]\n",
        "                    if ((len(time_invar_attrs) == 0) |\\\n",
        "                        (vdem_this_year_cty.isnull().values.all())):\n",
        "                        continue\n",
        "                    bilateral_attr[:4] = time_invar_attrs[['Dist_coord', 'Evercol', 'Comlang', 'Contig']].values[-1].reshape(-1, 1)\n",
        "\n",
        "                    timevar_span_link = timevar_span_cty[timevar_span_cty['iso_o'].isin(country_codes_b)]\n",
        "                    bitrade_span_link = bitrade_span_cty[bitrade_span_cty['iso_o'].isin(country_codes_b)]\n",
        "\n",
        "                    bilateral_attr[4] = get_last_valid(timevar_span_link, 'SeaDist_2CST')\n",
        "                    bilateral_attr[5] = get_last_valid(timevar_span_link, 'Curcol')\n",
        "                    cty_year_flow = get_last_valid(bitrade_span_link, 'FLOW')\n",
        "                    bilateral_attr[6] = cty_year_flow\n",
        "                    cty_year_imports += cty_year_flow\n",
        "\n",
        "                    edge_index = [node_b_idx, node_a_idx]\n",
        "\n",
        "                    cty_edges.append(np.nan_to_num(bilateral_attr))\n",
        "                    year_edge_index.append(edge_index)\n",
        "\n",
        "                for cty_edge in cty_edges:\n",
        "                    if cty_year_imports > 0:\n",
        "                        cty_edge[6] = cty_edge[6] / cty_year_imports\n",
        "                    year_edge_attr.append(cty_edge)\n",
        "\n",
        "            year_graph = geo.data.Data(x=torch.tensor(node_features, dtype=torch.float32).view(-1, num_node_features), \n",
        "                                       y=torch.tensor(node_target, dtype=torch.float32).view(-1, num_node_targets), \n",
        "                                       edge_index=torch.tensor(year_edge_index, dtype=torch.long).T, \n",
        "                                       edge_attr=torch.tensor(year_edge_attr, dtype=torch.float32).view(-1, num_edge_features))\n",
        "\n",
        "            all_years.append(year_graph)\n",
        "            years_metadata.append(year_metadata)\n",
        "\n",
        "        # get normalization stats\n",
        "        # for completely unbiased test we should only get training set stats but will pass on that for this\n",
        "        stacked_x = torch.cat(list([graph.x for graph in all_years]), 0)\n",
        "        stacked_y = torch.cat(list([graph.y for graph in all_years]), 0)\n",
        "        stacked_attrs = torch.cat(list([graph.edge_attr for graph in all_years]), 0)\n",
        "\n",
        "        x_mean = torch.mean(stacked_x, 0)\n",
        "        x_std = torch.std(stacked_x, 0)\n",
        "        y_mean = torch.mean(stacked_y, 0)\n",
        "        y_std = torch.std(stacked_y, 0)\n",
        "        attr_mean = torch.mean(stacked_attrs, 0)\n",
        "        attr_std = torch.std(stacked_attrs, 0)\n",
        "\n",
        "        for graph in all_years:\n",
        "            graph.x = (graph.x - x_mean) / x_std\n",
        "            graph.y = (graph.y - y_mean) / y_std\n",
        "            graph.edge_attr = (graph.edge_attr - attr_mean) / attr_std\n",
        "\n",
        "        # save stats for later use\n",
        "        torch.save({\"x_mean\": x_mean, \"x_std\": x_std, \"y_mean\": y_mean, \"y_std\": y_std, \"attr_mean\": attr_mean, \"attr_std\": attr_std}, self.norm_stats)\n",
        "        torch.save(years_metadata, self.node_dict)\n",
        "\n",
        "        data, slices = self.collate(all_years)\n",
        "        torch.save((data, slices), self.processed_paths[0])\n",
        "\n",
        "    def get_norm_stats(self):\n",
        "        return torch.load(self.norm_stats)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_LcIz33gK5x",
        "colab_type": "text"
      },
      "source": [
        "Now that the dataset has been created, we need to shuffle, partition, and load the dataset into batches using the PyTorch Geometric data loader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w83aoW2shRyT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "81bd3b19-e3a6-428b-f677-78dbaad52ad1"
      },
      "source": [
        "\n",
        "dataset = TradeDemoYearByYearDataset(os.path.join('/', 'content', 'drive', 'My Drive', 'projects', 'trade_democratization', 'dataset'))\n",
        "dataset.shuffle()\n",
        "\n",
        "# split into three sets\n",
        "num_train = int(len(dataset) * 0.8)\n",
        "num_val = int(len(dataset) * 0.1)\n",
        "num_test = int(len(dataset) * 0.1)\n",
        "\n",
        "train_set = dataset[:num_train]\n",
        "val_set = dataset[num_train:num_train+num_val]\n",
        "test_set = dataset[-num_test:]\n",
        "\n",
        "# load into batches\n",
        "train_loader = geo.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "val_loader = geo.data.DataLoader(val_set, batch_size=32, shuffle=True)\n",
        "test_loader = geo.data.DataLoader(test_set, batch_size=32, shuffle=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "115it [5:04:53, 159.07s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N11bf0Te6F1U",
        "colab_type": "text"
      },
      "source": [
        "Now we will define a simple graph network model that will predict next years democratization level given a range of variables for this year."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG-CsMvR6WIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RegressionGraphNet(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, num_edge_features, num_outputs):\n",
        "        super(RegressionGraphNet, self).__init__()\n",
        "        # we will use the edge conditioned convolution (NNConv) as this allows more than 1 edge feature\n",
        "        # while also not requiring inputs to be scaled between 0 and 1\n",
        "\n",
        "        hidden_layer_size = 5\n",
        "\n",
        "        # NNConv requires a layer to transform the dimensionality of edge features to the required size\n",
        "        lin1 = torch.nn.Linear(num_edge_features, num_node_features * hidden_layer_size)\n",
        "\n",
        "        # arbitrarily using 16 as hidden layer size\n",
        "        self.conv1 = geo.nn.NNConv(num_node_features, hidden_layer_size, lin1)\n",
        "        self.lin1 = torch.nn.Linear(hidden_layer_size, num_outputs)\n",
        "\n",
        "    def forward(self, data):\n",
        "        # weights are by default floats\n",
        "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
        "\n",
        "        x = self.conv1(x, edge_index, edge_attr)\n",
        "        x = torch.nn.functional.relu(x)\n",
        "        x = torch.nn.functional.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.lin1(x)\n",
        "\n",
        "        # final activation is linear as this is for regression\n",
        "        return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBb9TEkeELi9",
        "colab_type": "text"
      },
      "source": [
        "We now create an instance of the model and optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEb0DhpBEvVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = RegressionGraphNet(dataset.num_features, dataset.num_edge_features, dataset[0].y.size(1))\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuNBVU5VF6If",
        "colab_type": "text"
      },
      "source": [
        "Define our training and test calls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Nb0OYHAGI9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch)\n",
        "        label = batch.y\n",
        "        # good loss for regression problems\n",
        "        loss = torch.nn.functional.smooth_l1_loss(out, label)\n",
        "        loss.backward()\n",
        "        loss_all += loss.item()\n",
        "        optimizer.step()\n",
        "    return loss_all / num_train\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(loader, state_dict=None):\n",
        "\n",
        "    if not state_dict is None:\n",
        "        model.load_state_dict(state_dict)\n",
        "\n",
        "    model.eval()\n",
        "    num_batches = 0\n",
        "    loss_all = 0\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        pred = model(batch)\n",
        "        label = batch.y\n",
        "        # good loss for regression problems\n",
        "        loss = torch.nn.functional.smooth_l1_loss(pred, label)\n",
        "        loss_all += loss\n",
        "        num_batches += 1\n",
        "    return loss_all / num_batches\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE9GgHFrXL4E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7d08fb28-2ec7-441b-847d-69e6a3fc805b"
      },
      "source": [
        "MAX_EPOCHS = 10000\n",
        "min_val_loss = float(\"inf\")\n",
        "epochs_since = 0\n",
        "NUM_NON_DECREASING = 100\n",
        "for epoch in range(MAX_EPOCHS):\n",
        "    train_loss = train()\n",
        "    val_loss = test(val_loader)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        print('Epoch: {}, Train Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, train_loss, val_loss))\n",
        "\n",
        "    if val_loss < min_val_loss:\n",
        "        best_model = model.state_dict()\n",
        "        min_val_loss = val_loss\n",
        "        epochs_since = 0\n",
        "\n",
        "    epochs_since += 1\n",
        "    if epochs_since > NUM_NON_DECREASING:\n",
        "        print(\"Early stopping engaged\")\n",
        "        break\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Train Loss: 0.1038, Validation Loss: 11.5231\n",
            "Epoch: 5, Train Loss: 0.0300, Validation Loss: 4.0601\n",
            "Epoch: 10, Train Loss: 0.0158, Validation Loss: 1.6941\n",
            "Epoch: 15, Train Loss: 0.0127, Validation Loss: 1.0809\n",
            "Epoch: 20, Train Loss: 0.0112, Validation Loss: 0.8896\n",
            "Epoch: 25, Train Loss: 0.0105, Validation Loss: 0.8127\n",
            "Epoch: 30, Train Loss: 0.0101, Validation Loss: 0.7715\n",
            "Epoch: 35, Train Loss: 0.0097, Validation Loss: 0.7431\n",
            "Epoch: 40, Train Loss: 0.0095, Validation Loss: 0.7212\n",
            "Epoch: 45, Train Loss: 0.0092, Validation Loss: 0.6973\n",
            "Epoch: 50, Train Loss: 0.0089, Validation Loss: 0.7054\n",
            "Epoch: 55, Train Loss: 0.0087, Validation Loss: 0.7250\n",
            "Epoch: 60, Train Loss: 0.0085, Validation Loss: 0.7194\n",
            "Epoch: 65, Train Loss: 0.0083, Validation Loss: 0.7018\n",
            "Epoch: 70, Train Loss: 0.0083, Validation Loss: 0.6803\n",
            "Epoch: 75, Train Loss: 0.0081, Validation Loss: 0.6568\n",
            "Epoch: 80, Train Loss: 0.0080, Validation Loss: 0.6292\n",
            "Epoch: 85, Train Loss: 0.0077, Validation Loss: 0.5973\n",
            "Epoch: 90, Train Loss: 0.0076, Validation Loss: 0.5803\n",
            "Epoch: 95, Train Loss: 0.0075, Validation Loss: 0.5725\n",
            "Epoch: 100, Train Loss: 0.0075, Validation Loss: 0.5667\n",
            "Epoch: 105, Train Loss: 0.0075, Validation Loss: 0.5543\n",
            "Epoch: 110, Train Loss: 0.0073, Validation Loss: 0.5611\n",
            "Epoch: 115, Train Loss: 0.0074, Validation Loss: 0.5634\n",
            "Epoch: 120, Train Loss: 0.0072, Validation Loss: 0.5576\n",
            "Epoch: 125, Train Loss: 0.0070, Validation Loss: 0.5475\n",
            "Epoch: 130, Train Loss: 0.0070, Validation Loss: 0.5377\n",
            "Epoch: 135, Train Loss: 0.0070, Validation Loss: 0.5247\n",
            "Epoch: 140, Train Loss: 0.0070, Validation Loss: 0.5138\n",
            "Epoch: 145, Train Loss: 0.0068, Validation Loss: 0.5064\n",
            "Epoch: 150, Train Loss: 0.0068, Validation Loss: 0.4943\n",
            "Epoch: 155, Train Loss: 0.0067, Validation Loss: 0.4890\n",
            "Epoch: 160, Train Loss: 0.0067, Validation Loss: 0.4703\n",
            "Epoch: 165, Train Loss: 0.0064, Validation Loss: 0.4566\n",
            "Epoch: 170, Train Loss: 0.0064, Validation Loss: 0.4515\n",
            "Epoch: 175, Train Loss: 0.0063, Validation Loss: 0.4427\n",
            "Epoch: 180, Train Loss: 0.0062, Validation Loss: 0.4258\n",
            "Epoch: 185, Train Loss: 0.0060, Validation Loss: 0.4141\n",
            "Epoch: 190, Train Loss: 0.0060, Validation Loss: 0.4046\n",
            "Epoch: 195, Train Loss: 0.0059, Validation Loss: 0.3875\n",
            "Epoch: 200, Train Loss: 0.0059, Validation Loss: 0.3799\n",
            "Epoch: 205, Train Loss: 0.0059, Validation Loss: 0.3554\n",
            "Epoch: 210, Train Loss: 0.0059, Validation Loss: 0.3366\n",
            "Epoch: 215, Train Loss: 0.0057, Validation Loss: 0.3412\n",
            "Epoch: 220, Train Loss: 0.0058, Validation Loss: 0.3430\n",
            "Epoch: 225, Train Loss: 0.0057, Validation Loss: 0.3263\n",
            "Epoch: 230, Train Loss: 0.0059, Validation Loss: 0.3237\n",
            "Epoch: 235, Train Loss: 0.0057, Validation Loss: 0.3031\n",
            "Epoch: 240, Train Loss: 0.0057, Validation Loss: 0.3016\n",
            "Epoch: 245, Train Loss: 0.0056, Validation Loss: 0.3172\n",
            "Epoch: 250, Train Loss: 0.0057, Validation Loss: 0.3060\n",
            "Epoch: 255, Train Loss: 0.0057, Validation Loss: 0.2852\n",
            "Epoch: 260, Train Loss: 0.0056, Validation Loss: 0.2968\n",
            "Epoch: 265, Train Loss: 0.0056, Validation Loss: 0.2826\n",
            "Epoch: 270, Train Loss: 0.0055, Validation Loss: 0.2860\n",
            "Epoch: 275, Train Loss: 0.0057, Validation Loss: 0.2841\n",
            "Epoch: 280, Train Loss: 0.0056, Validation Loss: 0.2935\n",
            "Epoch: 285, Train Loss: 0.0055, Validation Loss: 0.2931\n",
            "Epoch: 290, Train Loss: 0.0055, Validation Loss: 0.2950\n",
            "Epoch: 295, Train Loss: 0.0057, Validation Loss: 0.2885\n",
            "Epoch: 300, Train Loss: 0.0056, Validation Loss: 0.2901\n",
            "Epoch: 305, Train Loss: 0.0054, Validation Loss: 0.2622\n",
            "Epoch: 310, Train Loss: 0.0055, Validation Loss: 0.2781\n",
            "Epoch: 315, Train Loss: 0.0053, Validation Loss: 0.2835\n",
            "Epoch: 320, Train Loss: 0.0054, Validation Loss: 0.2961\n",
            "Epoch: 325, Train Loss: 0.0054, Validation Loss: 0.3018\n",
            "Epoch: 330, Train Loss: 0.0054, Validation Loss: 0.2857\n",
            "Epoch: 335, Train Loss: 0.0053, Validation Loss: 0.2780\n",
            "Epoch: 340, Train Loss: 0.0054, Validation Loss: 0.2744\n",
            "Epoch: 345, Train Loss: 0.0054, Validation Loss: 0.2822\n",
            "Epoch: 350, Train Loss: 0.0052, Validation Loss: 0.2831\n",
            "Epoch: 355, Train Loss: 0.0053, Validation Loss: 0.2906\n",
            "Epoch: 360, Train Loss: 0.0053, Validation Loss: 0.2755\n",
            "Epoch: 365, Train Loss: 0.0052, Validation Loss: 0.2659\n",
            "Epoch: 370, Train Loss: 0.0053, Validation Loss: 0.2702\n",
            "Epoch: 375, Train Loss: 0.0053, Validation Loss: 0.2579\n",
            "Epoch: 380, Train Loss: 0.0053, Validation Loss: 0.3004\n",
            "Epoch: 385, Train Loss: 0.0053, Validation Loss: 0.2800\n",
            "Epoch: 390, Train Loss: 0.0053, Validation Loss: 0.2473\n",
            "Epoch: 395, Train Loss: 0.0052, Validation Loss: 0.2634\n",
            "Epoch: 400, Train Loss: 0.0054, Validation Loss: 0.2796\n",
            "Epoch: 405, Train Loss: 0.0053, Validation Loss: 0.2607\n",
            "Epoch: 410, Train Loss: 0.0054, Validation Loss: 0.2615\n",
            "Epoch: 415, Train Loss: 0.0053, Validation Loss: 0.2432\n",
            "Epoch: 420, Train Loss: 0.0051, Validation Loss: 0.2647\n",
            "Epoch: 425, Train Loss: 0.0054, Validation Loss: 0.2736\n",
            "Epoch: 430, Train Loss: 0.0053, Validation Loss: 0.2833\n",
            "Epoch: 435, Train Loss: 0.0052, Validation Loss: 0.2675\n",
            "Epoch: 440, Train Loss: 0.0053, Validation Loss: 0.2338\n",
            "Epoch: 445, Train Loss: 0.0051, Validation Loss: 0.2805\n",
            "Epoch: 450, Train Loss: 0.0052, Validation Loss: 0.2670\n",
            "Epoch: 455, Train Loss: 0.0052, Validation Loss: 0.2732\n",
            "Epoch: 460, Train Loss: 0.0052, Validation Loss: 0.2641\n",
            "Epoch: 465, Train Loss: 0.0051, Validation Loss: 0.2828\n",
            "Epoch: 470, Train Loss: 0.0051, Validation Loss: 0.2673\n",
            "Epoch: 475, Train Loss: 0.0052, Validation Loss: 0.2823\n",
            "Epoch: 480, Train Loss: 0.0051, Validation Loss: 0.2817\n",
            "Epoch: 485, Train Loss: 0.0051, Validation Loss: 0.2639\n",
            "Epoch: 490, Train Loss: 0.0051, Validation Loss: 0.2812\n",
            "Epoch: 495, Train Loss: 0.0051, Validation Loss: 0.2575\n",
            "Epoch: 500, Train Loss: 0.0051, Validation Loss: 0.2616\n",
            "Epoch: 505, Train Loss: 0.0050, Validation Loss: 0.2844\n",
            "Epoch: 510, Train Loss: 0.0051, Validation Loss: 0.2525\n",
            "Epoch: 515, Train Loss: 0.0050, Validation Loss: 0.2941\n",
            "Epoch: 520, Train Loss: 0.0052, Validation Loss: 0.2722\n",
            "Epoch: 525, Train Loss: 0.0050, Validation Loss: 0.2817\n",
            "Epoch: 530, Train Loss: 0.0051, Validation Loss: 0.2848\n",
            "Epoch: 535, Train Loss: 0.0050, Validation Loss: 0.2764\n",
            "Epoch: 540, Train Loss: 0.0049, Validation Loss: 0.2440\n",
            "Early stopping engaged\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jxo5JQayXOtA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8734fec-6530-49bf-e108-eb7a403372d6"
      },
      "source": [
        "test_loss = test(test_loader, state_dict=best_model)\n",
        "print('Final Test Loss: {:.4f}'.format(test_loss))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Test Loss: 0.2970\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZPhJQSsPim4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(best_model, os.path.join('/', 'content', 'drive', 'My Drive', 'projects', 'trade_democratization', 'best_model_non_recurrent.pkl'))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFv1nR6_nj2b",
        "colab_type": "text"
      },
      "source": [
        "After tuning hyperparameters this seems to be a good final test set loss. "
      ]
    }
  ]
}