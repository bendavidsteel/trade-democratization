{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rl_agent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOniiMPVORVSbghDz8e1Fuz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bendavidsteel/trade-democratization/blob/master/rl_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbj4bASTAhCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install torch-scatter==2.0.4+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-sparse==0.6.5+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-cluster==1.5.5+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-spline-conv==1.2.0+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-geometric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtqspZ44BGMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import itertools\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch_geometric as geo\n",
        "import tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh-O73yTvN8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self):\n",
        "        return random.choice(self.memory)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)                 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TufYnaM6Qr3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RecurGraphNet(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, num_edge_features, num_output_features):\n",
        "        super().__init__()\n",
        "\n",
        "        conv_layer_size = 32\n",
        "        lstm_layer_size = 32\n",
        "\n",
        "        # graph convolutional layer to create graph representation\n",
        "        conv_lin = torch.nn.Linear(num_edge_features, num_node_features * conv_layer_size)\n",
        "        self.conv = geo.nn.NNConv(num_node_features, conv_layer_size, conv_lin)\n",
        "\n",
        "        # lstm to learn sequential patterns\n",
        "        self.lstm = torch.nn.LSTM(conv_layer_size, lstm_layer_size, dropout=0.5)\n",
        "\n",
        "        # initial trainable hidden state for lstm\n",
        "        self.lstm_h_s = torch.nn.Linear(num_output_features, lstm_layer_size)\n",
        "        self.lstm_c_s = torch.nn.Linear(num_output_features, lstm_layer_size)\n",
        "\n",
        "        # final linear layer to allow full expressivity for regression after tanh activation in lstm\n",
        "        self.final_linear = torch.nn.Linear(lstm_layer_size, num_output_features)\n",
        "\n",
        "    def reset(self, initial):\n",
        "        self.initial = initial\n",
        "        self.new_seq = True\n",
        "\n",
        "    def forward(self, input):\n",
        "        # create graph representation\n",
        "        graph_step = torch.nn.functional.relu(self.conv(input.x, input.edge_index, input.edge_attr))\n",
        "\n",
        "        # recurrent stage\n",
        "        # initial state of lstm is representation of target prior to this sequence\n",
        "        if self.new_seq:\n",
        "            self.new_seq = False\n",
        "            self.hs = self.lstm_h_s(initial).unsqueeze(0)\n",
        "            self.cs = self.lstm_c_s(initial).unsqueeze(0)\n",
        "\n",
        "        lstm_output, (self.hs, self.cs) = self.lstm(graph_step.unsqueeze(0), (self.hs, self.cs))\n",
        "\n",
        "        return self.final_linear(lstm_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqBv2fPEB1dK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NationEnvironment():\n",
        "    def __init__(self, num_countries):\n",
        "        num_node_features = 2\n",
        "        num_edge_features = 7\n",
        "        num_output_features = 1\n",
        "        env_model = RecurGraphNet(num_node_features, num_edge_features, num_output_features)\n",
        "        self.reset()\n",
        "\n",
        "        self.num_foreign_actions = 5\n",
        "        self.num_domestic_actions = 4\n",
        "        \n",
        "    def reset(self):\n",
        "        self.initial_demo = torch.rand(num_countries, 1, dtype=torch.float32)\n",
        "        # start with up to 1 thousand gdp and 1 million pop\n",
        "        gdp = 1000000000 * torch.rand(num_countries, 1, dtype=torch.float32)\n",
        "        pop = 1000000 * torch.rand(num_countries, 1, dtype=torch.float32)\n",
        "        self.node_features = torch.concat([gdp,\n",
        "                                           pop], dim=1)\n",
        "\n",
        "        # establish country ally clusters\n",
        "        self.clusters = []\n",
        "        cluster_edges = []\n",
        "        num_clusters = num_countries // 10\n",
        "        for cluster_idx in range(num_clusters):\n",
        "            cluster = random.sample(list(range(num_countries)), random.randint(2, num_countries // 5))\n",
        "            self.clusters.append(cluster)\n",
        "            for edge in list(itertools.permutations(cluster, 2)):\n",
        "                cluster_edges.append(edge)\n",
        "\n",
        "        # starting with number of links on average anywhere between 1 and 5\n",
        "        num_edges = (num_countries * random.randint(1, 5)) + len(cluster_edges)\n",
        "        self.edge_indexes = torch.randint(num_countries, (2, num_edges), dtype=torch.long)\n",
        "\n",
        "        for idx in range(len(cluster_edges)):\n",
        "            self.edge_indexes[0, idx] = cluster_edges[idx][0]\n",
        "            self.edge_indexes[1, idx] = cluster_edges[idx][1]\n",
        "\n",
        "        # ensure no self links\n",
        "        for idx in range(self.edge_indexes.shape[1]):\n",
        "            if self.edge_indexes[0,idx] == self.edge_indexes[1,idx]:\n",
        "                if self.edge_indexes[1,idx] == num_countries:\n",
        "                    self.edge_indexes[1,idx] -= 1\n",
        "                else:\n",
        "                    self.edge_indexes[1,idx] += 1\n",
        "\n",
        "        # ever col -> curr col\n",
        "        #           -> common language\n",
        "        ever_col = (torch.rand(num_edges, 1) > 0.98).type(torch.FloatTensor)\n",
        "        curr_col = ((torch.rand(num_edges, 1) > 0.5) & ever_col).type(torch.FloatTensor)\n",
        "        com_lang = ((torch.rand(num_edges, 1) > 0.9) | (0.5 * ever_col)).type(torch.FloatTensor)\n",
        "        # distance -> distance by sea\n",
        "        #          -> shared borders\n",
        "        #          -> trade\n",
        "        coor_dis = 15000 * torch.rand(num_edges, 1, dtype=torch.float32)\n",
        "        sea_dist = dist * ((2.5 * torch.rand(num_edges, 1, dtype=torch.float32)) + 1)\n",
        "        trad_imp = coor_dis * 10000 * torch.rand(num_edges, 1, dtype=torch.float32)\n",
        "        shar_bor = (((coor_dis < 1000) * (torch.rand(num_edges, 1) > 0.5)) | ((coor_dis < 2000) * (torch.rand(num_edges, 1) > 0.7)) | ((coor_dis < 5000) * (torch.rand(num_edges, 1) > 0.9))).type(torch.FloatTensor)\n",
        "        # order of edge features is distance, ever a colony, common language, shared borders, distance by sea, current colony, imports\n",
        "        self.edge_features = torch.concat([coor_dis,\n",
        "                                           ever_col,\n",
        "                                           com_lang,\n",
        "                                           shar_bor,\n",
        "                                           sea_dist,\n",
        "                                           curr_col,\n",
        "                                           trad_imp], dim=1)\n",
        "        \n",
        "        self.env_model.reset(self.initial_demo)\n",
        "        \n",
        "    def establish_trade(self, agent_id, target_id):\n",
        "        # ensure no self links\n",
        "        if agent_id == target_id:\n",
        "            return\n",
        "\n",
        "        # origin country index comes first\n",
        "        trade_link = np.array([target_id, agent_id]).reshape((2,1))\n",
        "        for idx in range(self.edge_indexes.shape[1]):\n",
        "            if (self.edge_indexes[0,idx] == trade_link[0,0]) && (self.edge_indexes[1,idx] == trade_link[1,0]):\n",
        "                # trade link already established\n",
        "                return\n",
        "\n",
        "        self.edge_indexes = torch.cat((self.edge_indexes, trade_link), dim=1)\n",
        "\n",
        "    def increase_imports(self, agent_id, target_id):\n",
        "        self.scale_imports(agent_id, target_id, 1.05, 1.3)\n",
        "\n",
        "    def decrease_imports(self, agent_id, target_id):\n",
        "        self.scale_imports(agent_id, target_id, 0.7, 0.95)\n",
        "\n",
        "    def scale_imports(self, agent_id, target_id, lower_bound, upper_bound):\n",
        "        link_idx = -1\n",
        "        for idx in range(self.edge_indexes.shape[1]):\n",
        "            if (self.edge_indexes[0,idx] == target_id) && (self.edge_indexes[1,idx] == agent_id):\n",
        "                link_idx = idx\n",
        "                break\n",
        "\n",
        "        if link_idx == -1:\n",
        "            return\n",
        "        \n",
        "        self.edge_features[idx, 6] = self.edge_features[idx, 6] * random.uniform(lower_bound, upper_bound)\n",
        "\n",
        "    def colonize(self, agent_id, target_id):\n",
        "        # check if there is a link with this country\n",
        "        # and ensure no other country has already colonized the target country\n",
        "        link_idx = -1\n",
        "        already_colonised = False\n",
        "        for idx in range(self.edge_indexes.shape[1]):\n",
        "            if (self.edge_indexes[0,idx] == target_id) && (self.edge_indexes[1,idx] == agent_id):\n",
        "                link_idx = idx\n",
        "\n",
        "            if self.edge_indexes[0,idx] == target_id:\n",
        "                if self.edge_features[idx, 5] == 1:\n",
        "                    already_colonised = True\n",
        "\n",
        "        if link_idx == -1 | already_colonised:\n",
        "            return\n",
        "\n",
        "        # colonizing country needs to be bigger\n",
        "        if (self.node_features[agent_id, 0] > 1.2 * self.node_features[target_id, 0]) &&\n",
        "           (self.node_features[agent_id, 1] > 1.1 * self.node_features[target_id, 1]):\n",
        "            self.edge_features[link_idx, 5] = 1\n",
        "            self.edge_features[link_idx, 1] = 1\n",
        "\n",
        "    def decolonize(self, agent_id, target_id):\n",
        "        # check if there is a link with this country\n",
        "        link_idx = -1\n",
        "        for idx in range(self.edge_indexes.shape[1]):\n",
        "            if (self.edge_indexes[0,idx] == target_id) && (self.edge_indexes[1,idx] == agent_id):\n",
        "                link_idx = idx\n",
        "                break\n",
        "\n",
        "        if link_idx == -1:\n",
        "            return\n",
        "\n",
        "        if self.edge_features[link_idx, 5] == 1:\n",
        "            self.edge_features[link_idx, 5] = 0\n",
        "\n",
        "    def increase_gdp(self, agent_id):\n",
        "        self.node_features[agent_id, 0] += 0.2 * self.node_features[agent_id, 0] * (random.random() + 0.5)\n",
        "\n",
        "    def decrease_gdp(self, agent_id):\n",
        "        self.node_features[agent_id, 0] -= 0.2 * self.node_features[agent_id, 0] * (random.random() + 0.5)\n",
        "\n",
        "    def increase_pop(self, agent_id):\n",
        "        self.node_features[agent_id, 1] += 0.2 * self.node_features[agent_id, 1] * (random.random() + 0.5)\n",
        "\n",
        "    def decrease_pop(self, agent_id):\n",
        "        self.node_features[agent_id, 1] -= 0.2 * self.node_features[agent_id, 1] * (random.random() + 0.5)\n",
        "\n",
        "    def step(self):\n",
        "        # gdp and pop fluctuations\n",
        "        self.node_features[:, 0] += 0.05 * self.node_features[:, 0] * (torch.rand(num_countries, 1, dtype=torch.float32) - 0.5)\n",
        "        self.node_features[:, 1] += 0.05 * self.node_features[:, 1] * (torch.rand(num_countries, 1, dtype=torch.float32) - 0.5)\n",
        "\n",
        "        # colonized countries can flip to having a common language\n",
        "        one_feat_shape = self.edge_features[:, 2].shape\n",
        "        self.edge_features[:, 2] = torch.min(torch.ones(one_feat_shape), self.edge_features[:, 2] + (self.edge_features[:, 5] * (torch.rand(num_edges, 1) > 0.9)))\n",
        "\n",
        "        # sea distance can shorten\n",
        "        self.edge_features[:, 4] = torch.min(self.edge_features[:, 0] * 1.5, self.edge_features[:, 4] * torch.max(torch.ones(one_feat_shape), 0.8 + torch.rand(one_feat_shape) * 20))\n",
        "\n",
        "        # TODO apply scaling\n",
        "\n",
        "        data = geo.data.Data(x=self.node_features, edge_index=self.edge_indexes, edge_attr=self.edge_features)\n",
        "        self.node_demo = self.env_model(data)\n",
        "\n",
        "    def get_reward(self, agent_id):\n",
        "        for cluster in self.clusters:\n",
        "            if agent_id in cluster:\n",
        "                agent_cluster = cluster\n",
        "\n",
        "        reward = 0\n",
        "        for country_idx in range(num_countries):\n",
        "            demo = self.node_demo[country_idx]\n",
        "            if country_idx == agent_id:\n",
        "                reward += 2 * demo\n",
        "            elif country_idx in agent_cluster:\n",
        "                reward += demo\n",
        "            else:\n",
        "                reward -= demo\n",
        "                \n",
        "        return reward\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrmC8VlOJ7K4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RecurGraphAgent(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, num_edge_features, num_node_output_features, num_graph_output_features):\n",
        "        super().__init__()\n",
        "\n",
        "        conv_layer_size = 32\n",
        "        lstm_layer_size = 32\n",
        "\n",
        "        # graph convolutional layer to create graph representation\n",
        "        conv_lin = torch.nn.Linear(num_edge_features, num_node_features * conv_layer_size)\n",
        "        self.conv = geo.nn.NNConv(num_node_features, conv_layer_size, conv_lin)\n",
        "\n",
        "        # lstm to learn sequential patterns\n",
        "        self.lstm = torch.nn.LSTM(conv_layer_size, lstm_layer_size, dropout=0.5)\n",
        "\n",
        "        # initial trainable hidden state for lstm\n",
        "        self.lstm_h_s = torch.nn.Linear(num_output_features, lstm_layer_size)\n",
        "        self.lstm_c_s = torch.nn.Linear(num_output_features, lstm_layer_size)\n",
        "\n",
        "        # graph pooling layer\n",
        "        self.pool = geo.nn.GlobalAttention(gate_nn = torch.nn.Sequential(torch.nn.Linear(lstm_layer_size, 2*lstm_layer_size), torch.nn.BatchNorm1d(2*lstm_layer_size), torch.nn.ReLU(), torch.nn.Linear(2*lstm_layer_size, 1)))\n",
        "\n",
        "        # final graph output\n",
        "        self.final_graph_linear = torch.nn.Linear(lstm_layer_size, num_graph_output_features)\n",
        "\n",
        "        # final linear layer to allow full expressivity for regression after tanh activation in lstm\n",
        "        self.final_node_linear = torch.nn.Linear(lstm_layer_size, num_node_output_features)\n",
        "\n",
        "    def reset(self, initial):\n",
        "        self.initial = initial\n",
        "        self.new_seq = True\n",
        "\n",
        "    def forward(self, input):\n",
        "        # create graph representation\n",
        "        graph_step = torch.nn.functional.relu(self.conv(input.x, input.edge_index, input.edge_attr))\n",
        "\n",
        "        # recurrent stage\n",
        "        # initial state of lstm is representation of target prior to this sequence\n",
        "        if self.new_seq:\n",
        "            self.new_seq = False\n",
        "            self.hs = self.lstm_h_s(initial).unsqueeze(0)\n",
        "            self.cs = self.lstm_c_s(initial).unsqueeze(0)\n",
        "\n",
        "        lstm_output, (self.hs, self.cs) = self.lstm(graph_step.unsqueeze(0), (self.hs, self.cs))\n",
        "\n",
        "        graph_pool = self.pool(lstm_output)\n",
        "        final_graph = self.final_graph_linear(graph_pool)\n",
        "\n",
        "        final_node = self.final_node_linear(lstm_output)\n",
        "        node_flattened = final_node.view(-1)\n",
        "\n",
        "        return torch.nn.functional.softmax(node_flattened), torch.nn.functional.softmax(final_graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk2DceslIkWD",
        "colab_type": "text"
      },
      "source": [
        "We want to be able to allow multiple actions per turn. We have previously defined a model with branching outputs. We will consider the predicted Q value to be the sum of the model outputs that are chosen as actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av0VHgnn1HTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NationAgent():\n",
        "    def __init__(self, agent_id, ally_countries, num_countries, num_node_actions, num_global_actions):\n",
        "        # more node features because we will add indicator of self country and ally countries\n",
        "        num_node_features, num_edge_features = 4, 7\n",
        "\n",
        "        # create two DQNs for stable learning\n",
        "        self.policy_net = RecurGraphAgent(num_node_features, num_edge_features, num_node_actions, num_global_actions)\n",
        "        self.target_net = RecurGraphAgent(num_node_features, num_edge_features, num_node_actions, num_global_actions)\n",
        "        self.optimizer = torch.optim.RMSprop(self.policy_net.parameters())\n",
        "\n",
        "        # ensure they match\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "        self.agent_id = agent_id\n",
        "        # create node data with features for self and ally countries\n",
        "        # using -0.1 and 0.9 as approximation of normalization\n",
        "        self.node_features = -0.1 * torch.ones((num_countries, 4), dtype=torch.float32)\n",
        "        self.node_features[self.agent_id, 2] = 0.9\n",
        "        for ally_idx in ally_countries:\n",
        "            self.node_features[ally_idx, 3] = 0.9\n",
        "\n",
        "    def reset(self, state_dict, ally_countries, demo_initial):\n",
        "        self.policy_net.load_state_dict(state_dict)\n",
        "        self.target_net.load_state_dict(state_dict)\n",
        "\n",
        "        self.policy_net.reset(demo_initial)\n",
        "        self.target_net.reset(demo_initial)\n",
        "\n",
        "        self.node_features = -0.1 * torch.ones((num_countries, 4), dtype=torch.float32)\n",
        "        self.node_features[self.agent_id, 2] = 0.9\n",
        "        for ally_idx in ally_countries:\n",
        "            self.node_features[ally_idx, 3] = 0.9\n",
        "\n",
        "    def get_state(self):\n",
        "        return self.policy_net.state_dict()\n",
        "\n",
        "    def select_action(self, state, eps_threshold):\n",
        "\n",
        "        # add in country specific state\n",
        "        self.node_features[:, :2] = state.x\n",
        "        state.x = self.node_features\n",
        "\n",
        "        sample = random.random()\n",
        "        if sample > eps_threshold:\n",
        "            with torch.no_grad():\n",
        "                # t.max(1) will return largest column value of each row.\n",
        "                # second column on max result is index of where max element was\n",
        "                # found, so we pick action with the larger expected reward.\n",
        "                foreign_output, domestic_output = self.policy_net(state)\n",
        "                foreign_action = torch.argmax(foreign_output)\n",
        "                domestic_action = torch.argmax(domestic_output)\n",
        "                return foreign_action, domestic_action\n",
        "        else:\n",
        "            return torch.tensor(random.randrange(num_node_actions), device=device, dtype=torch.long), torch.tensor(random.randrange(num_global_actions), device=device, dtype=torch.long)\n",
        "\n",
        "    def optimize(self, reward):\n",
        "        # single transition because i haven't worked out how to make batches work with net yet\n",
        "        transition = memory.sample()\n",
        "\n",
        "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "        # columns of actions taken. These are the actions which would've been taken\n",
        "        # for each batch state according to policy_net\n",
        "        foreign_output, domestic_output = self.policy_net(transition.state)\n",
        "        state_action_values = foreign_output[transition.action.foreign] + domestic_output[transition.action.domestic]\n",
        "\n",
        "        # Compute V(s_{t+1}) for all next states.\n",
        "        # Expected values of actions for non_final_next_states are computed based\n",
        "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "        # This is merged based on the mask, such that we'll have either the expected\n",
        "        # state value or 0 in case the state was final.\n",
        "        if transition.next_state is None:\n",
        "            next_state_value = 0\n",
        "        else:\n",
        "            foreign_output, domestic_output = self.target_net(transition.state)\n",
        "            next_state_value = foreign_output.max().detach() + domestic_output.max().detach()\n",
        "        # Compute the expected Q values\n",
        "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "        # Compute Huber loss\n",
        "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
        "\n",
        "        # Optimize the model\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        for param in self.policy_net.parameters():\n",
        "            # prevent exploding gradients\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "        optimizer.step()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh1peXrZy4XT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InternationalAgentCollection():\n",
        "    def __init__(self, ally_groups, num_countries, num_node_actions, num_global_actions):\n",
        "        # create agents\n",
        "        self.agents = []\n",
        "        for agent_idx in range(num_countries):\n",
        "            agent_ally_group = []\n",
        "            for ally_group in ally_groups:\n",
        "                if agent_idx in ally_group:\n",
        "                    agent_ally_group = ally_group\n",
        "\n",
        "            new_agent = NationAgent(agent_id, agent_ally_group, num_countries, num_node_actions, num_global_actions)\n",
        "            self.agents.append(new_agent)\n",
        "\n",
        "    def reset(self, ally_groups, demo_initial):\n",
        "        # get state dict from all agents\n",
        "        all_agent_states = []\n",
        "        for agent in self.agents:\n",
        "            all_agent_states.append(agent.get_state())\n",
        "\n",
        "        # average them\n",
        "        new_state_dict = all_agent_states[0]\n",
        "        for key in new_state_dict:\n",
        "            for idx in range(1, len(all_agent_states)):\n",
        "                new_state_dict[key] += all_agent_states[idx][key]\n",
        "            new_state_dict[key] = new_state_dict[key] / len(all_agent_states)\n",
        "\n",
        "        # and then apply averaged state dict to agents\n",
        "        for agent_idx, agent in enumerate(self.agents):\n",
        "            agent_ally_group = []\n",
        "            for ally_group in ally_groups:\n",
        "                if agent_idx in ally_group:\n",
        "                    agent_ally_group = ally_group\n",
        "            # reset each individual agent\n",
        "            agent.reset(new_state_dict, agent_ally_group, demo_initial)\n",
        "\n",
        "    def select_actions(self, state, eps_threshold):\n",
        "        agent_actions = []\n",
        "        for agent in self.agents:\n",
        "            action = agent.select_action(state, eps_threshold)\n",
        "            agent_actions.append(action)\n",
        "        return agent_actions\n",
        "\n",
        "    def optimize(self, rewards):\n",
        "        assert len(rewards) == len(self.agents)\n",
        "        for reward, agent in zip(rewards, self.agents):\n",
        "            agent.optimize(reward)\n",
        "\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e311ws9wXNm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mediator():\n",
        "    def __init__(self, num_countries):\n",
        "        self.env = NationEnvironment(num_countries)\n",
        "        self.agents = InternationalAgentCollection(env.clusters, num_countries, env.num_foreign_actions, env.num_domestic_actions)\n",
        "\n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        self.agents.reset(env.clusters, env.normed_initial_demo)\n",
        "\n",
        "    def apply_actions(self):\n",
        "        actions = self.agents.select_actions(env.normed_state, eps_threshold)\n",
        "        for agent_idx in range(len(actions)):\n",
        "            foreign_action, domestic_action = actions[agent_idx]\n",
        "            foreign_target_idx = math.floor(foreign_action / env.num_foreign_actions)\n",
        "            foreign_target_action = foreign_action % env.num_foreign_actions\n",
        "\n",
        "            if foreign_target_action == 0:\n",
        "                env.establish_trade(agent_idx, foreign_target_idx)\n",
        "            elif foreign_target_action == 1:\n",
        "                env.increase_imports(agent_idx, foreign_target_idx)\n",
        "            elif foreign_target_action == 2:\n",
        "                env.decrease_imports(agent_idx, foreign_target_idx)\n",
        "            elif foreign_target_action == 3:\n",
        "                env.colonize(agent_idx, foreign_target_idx)\n",
        "            elif foreign_target_action == 4:\n",
        "                env.decolonize(agent_idx, foreign_target_idx)\n",
        "\n",
        "            if domestic_action == 0:\n",
        "                env.increase_gdp(agent_idx)\n",
        "            elif domestic_action == 1:\n",
        "                env.decrease_gdp(agent_idx)\n",
        "            elif domestic_action == 2:\n",
        "                env.increase_pop(agent_idx)\n",
        "            elif domestic_action == 3:\n",
        "                env.decrease_pop(agent_idx)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fdjRdDVvYlX",
        "colab_type": "text"
      },
      "source": [
        "An object representing our environment, mapping (state, action) pairs to their (next_state, reward) result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RdIvoL_CJsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbjwwBCpv6KH",
        "colab_type": "text"
      },
      "source": [
        "Things To Try Next:\n",
        "\n",
        "*   Dueling DQN\n"
      ]
    }
  ]
}