{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rl_agent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNNviPzbNps87oWXS0aURp/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bendavidsteel/trade-democratization/blob/master/rl_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbj4bASTAhCZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "15c35427-7fcd-41fe-d718-76d51cda94f3"
      },
      "source": [
        "!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install torch-scatter==2.0.4+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-sparse==0.6.5+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-cluster==1.5.5+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-spline-conv==1.2.0+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-geometric"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 24kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 38.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "  Found existing installation: torchvision 0.7.0+cu101\n",
            "    Uninstalling torchvision-0.7.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-scatter==2.0.4+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_scatter-2.0.4%2Bcu101-cp36-cp36m-linux_x86_64.whl (12.2MB)\n",
            "\u001b[K     |████████████████████████████████| 12.3MB 19.5MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.4\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-sparse==0.6.5+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_sparse-0.6.5%2Bcu101-cp36-cp36m-linux_x86_64.whl (21.6MB)\n",
            "\u001b[K     |████████████████████████████████| 21.6MB 155kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-sparse==0.6.5+cu101) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-sparse==0.6.5+cu101) (1.18.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.5\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-cluster==1.5.5+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_cluster-1.5.5%2Bcu101-cp36-cp36m-linux_x86_64.whl (22.0MB)\n",
            "\u001b[K     |████████████████████████████████| 22.0MB 184kB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.5.5\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-spline-conv==1.2.0+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_spline_conv-1.2.0%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.3MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3MB 1.9MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.0\n",
            "Collecting torch-geometric\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/67/6c0bce6b6e6bc806e25d996e46a686e5a11254d89257983265a988bb02ee/torch_geometric-1.6.1.tar.gz (178kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.5.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.48.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.0.5)\n",
            "Collecting rdflib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/6b/6454aa1db753c0f8bc265a5bd5c10b5721a4bb24160fb4faf758cf6be8a1/rdflib-5.0.0-py3-none-any.whl (231kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 14.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.10.0)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n",
            "Collecting ase\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/78/edadb45c7f26f8fbb99da81feadb561c26bb0393b6c5d1ac200ecdc12d61/ase-3.20.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 12.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.11.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (50.3.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.31.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.7)\n",
            "Collecting isodate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ase->torch-geometric) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->torch-geometric) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (1.2.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-1.6.1-cp36-none-any.whl size=308552 sha256=de6c55056d62451207cdb7618a4d63c8c400737251df714a8730135d3f3543d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/25/ea/3d71d2088dccc63214fa59259dcc598ded4150a5f8b41d84ff\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: isodate, rdflib, ase, torch-geometric\n",
            "Successfully installed ase-3.20.1 isodate-0.6.0 rdflib-5.0.0 torch-geometric-1.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtqspZ44BGMu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "591f9da4-cfaa-4dc8-d156-27d22f78ca20"
      },
      "source": [
        "import collections\n",
        "import copy\n",
        "import itertools\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import statistics\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch_geometric as geo\n",
        "import tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fdjRdDVvYlX",
        "colab_type": "text"
      },
      "source": [
        "An object representing our environment, mapping (state, action) pairs to their (next_state, reward) result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh-O73yTvN8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Action = collections.namedtuple('Action', ('foreign', 'domestic'))\n",
        "Transition = collections.namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory():\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, transition):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = transition\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self):\n",
        "        return random.choice(self.memory)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)                 "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TufYnaM6Qr3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RecurGraphNet(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, num_edge_features, num_output_features):\n",
        "        super().__init__()\n",
        "\n",
        "        conv_layer_size = 32\n",
        "        lstm_layer_size = 32\n",
        "\n",
        "        # graph convolutional layer to create graph representation\n",
        "        conv_lin = torch.nn.Linear(num_edge_features, num_node_features * conv_layer_size)\n",
        "        self.conv = geo.nn.NNConv(num_node_features, conv_layer_size, conv_lin)\n",
        "\n",
        "        # lstm to learn sequential patterns\n",
        "        self.lstm = torch.nn.LSTM(conv_layer_size, lstm_layer_size, dropout=0.5)\n",
        "\n",
        "        # initial trainable hidden state for lstm\n",
        "        self.lstm_h_s = torch.nn.Linear(num_output_features, lstm_layer_size)\n",
        "        self.lstm_c_s = torch.nn.Linear(num_output_features, lstm_layer_size)\n",
        "\n",
        "        # final linear layer to allow full expressivity for regression after tanh activation in lstm\n",
        "        self.final_linear = torch.nn.Linear(lstm_layer_size, num_output_features)\n",
        "\n",
        "    def reset(self, initial):\n",
        "        self.initial = initial\n",
        "        self.new_seq = True\n",
        "\n",
        "    def forward(self, input):\n",
        "        # create graph representation\n",
        "        graph_step = torch.nn.functional.relu(self.conv(input.x, input.edge_index, input.edge_attr))\n",
        "\n",
        "        # recurrent stage\n",
        "        # initial state of lstm is representation of target prior to this sequence\n",
        "        if self.new_seq:\n",
        "            self.new_seq = False\n",
        "            self.hs = self.lstm_h_s(initial).unsqueeze(0)\n",
        "            self.cs = self.lstm_c_s(initial).unsqueeze(0)\n",
        "\n",
        "        lstm_output, (self.hs, self.cs) = self.lstm(graph_step.unsqueeze(0), (self.hs, self.cs))\n",
        "\n",
        "        return self.final_linear(lstm_output)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqBv2fPEB1dK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NationEnvironment():\n",
        "    def __init__(self, num_countries, device):\n",
        "        root = os.path.join('/', 'content', 'drive', 'My Drive', 'projects', 'trade_democratization')\n",
        "        self.norm_stats = torch.load(os.path.join(root, \"dataset\", \"processed\", \"norm_stats.pt\"))\n",
        "        best_model = torch.load(os.path.join(root, 'best_model_recurrent.pkl'))\n",
        "\n",
        "        self.num_countries = num_countries\n",
        "        self.device = device\n",
        "\n",
        "        num_node_features = 2\n",
        "        num_edge_features = 7\n",
        "        num_output_features = 1\n",
        "        self.env_model = RecurGraphNet(num_node_features, num_edge_features, num_output_features).to(device)\n",
        "        self.env_model.load_state_dict(best_model)\n",
        "        self.reset()\n",
        "\n",
        "        self.num_foreign_actions = 5\n",
        "        self.num_domestic_actions = 4\n",
        "        \n",
        "    def reset(self):\n",
        "        self.initial_demo = torch.rand(self.num_countries, 1, dtype=torch.float32)\n",
        "        self.norm_initial_demo = (self.initial_demo - self.norm_stats[\"y_mean\"]) / self.norm_stats[\"y_std\"]\n",
        "\n",
        "        # start with up to 1 thousand gdp and 1 million pop\n",
        "        gdp = 1000000000 * torch.rand(self.num_countries, 1, dtype=torch.float32)\n",
        "        pop = 1000000 * torch.rand(self.num_countries, 1, dtype=torch.float32)\n",
        "        self.node_features = torch.cat([gdp,\n",
        "                                        pop], dim=1)\n",
        "\n",
        "        # establish country ally clusters\n",
        "        self.clusters = []\n",
        "        cluster_edges = []\n",
        "        num_clusters = self.num_countries // 10\n",
        "        for cluster_idx in range(num_clusters):\n",
        "            cluster = random.sample(list(range(self.num_countries)), random.randint(2, self.num_countries // 5))\n",
        "            self.clusters.append(cluster)\n",
        "            for edge in list(itertools.permutations(cluster, 2)):\n",
        "                cluster_edges.append(edge)\n",
        "\n",
        "        # starting with number of links on average anywhere between 1 and 5\n",
        "        num_edges = (self.num_countries * random.randint(1, 5)) + len(cluster_edges)\n",
        "        self.edge_indexes = torch.randint(self.num_countries, (2, num_edges), dtype=torch.long)\n",
        "\n",
        "        for idx in range(len(cluster_edges)):\n",
        "            self.edge_indexes[0, idx] = cluster_edges[idx][0]\n",
        "            self.edge_indexes[1, idx] = cluster_edges[idx][1]\n",
        "\n",
        "        # ensure no self links\n",
        "        for idx in range(self.edge_indexes.shape[1]):\n",
        "            if self.edge_indexes[0,idx] == self.edge_indexes[1,idx]:\n",
        "                if self.edge_indexes[1,idx] == self.num_countries:\n",
        "                    self.edge_indexes[1,idx] -= 1\n",
        "                else:\n",
        "                    self.edge_indexes[1,idx] += 1\n",
        "\n",
        "        # ever col -> curr col\n",
        "        #          -> common language\n",
        "        ever_col = (torch.rand(num_edges, 1) > 0.98)\n",
        "        curr_col = ((torch.rand(num_edges, 1) > 0.5) * ever_col)\n",
        "        com_lang = ((torch.rand(num_edges, 1) > 0.9) | ((torch.rand(num_edges, 1) > 0.5) * ever_col))\n",
        "        # distance -> distance by sea\n",
        "        #          -> shared borders\n",
        "        #          -> trade\n",
        "        coor_dis = 15000 * torch.rand(num_edges, 1, dtype=torch.float32)\n",
        "        sea_dist = coor_dis * ((2.5 * torch.rand(num_edges, 1, dtype=torch.float32)) + 1)\n",
        "        trad_imp = coor_dis * 10000 * torch.rand(num_edges, 1, dtype=torch.float32)\n",
        "        shar_bor = (((coor_dis < 1000) * (torch.rand(num_edges, 1) > 0.5)) | ((coor_dis < 2000) * (torch.rand(num_edges, 1) > 0.7)) | ((coor_dis < 5000) * (torch.rand(num_edges, 1) > 0.9))).float()\n",
        "        # order of edge features is distance, ever a colony, common language, shared borders, distance by sea, current colony, imports\n",
        "        self.edge_features = torch.cat([coor_dis.float(),\n",
        "                                        ever_col.float(),\n",
        "                                        com_lang.float(),\n",
        "                                        shar_bor.float(),\n",
        "                                        sea_dist.float(),\n",
        "                                        curr_col.float(),\n",
        "                                        trad_imp.float()], dim=1)\n",
        "        \n",
        "        self.env_model.reset(self.norm_initial_demo)\n",
        "\n",
        "        self.create_normed_state()\n",
        "\n",
        "        \n",
        "    def establish_trade(self, agent_id, target_id):\n",
        "        # ensure no self links\n",
        "        if agent_id == target_id:\n",
        "            return\n",
        "\n",
        "        # origin country index comes first\n",
        "        trade_link = np.array([target_id, agent_id]).reshape((2,1))\n",
        "        for idx in range(self.edge_indexes.shape[1]):\n",
        "            if ((self.edge_indexes[0,idx] == trade_link[0,0]) and (self.edge_indexes[1,idx] == trade_link[1,0])):\n",
        "                # trade link already established\n",
        "                return\n",
        "\n",
        "        self.edge_indexes = torch.cat((self.edge_indexes, trade_link), dim=1)\n",
        "\n",
        "    def increase_imports(self, agent_id, target_id):\n",
        "        self.scale_imports(agent_id, target_id, 1.05, 1.3)\n",
        "\n",
        "    def decrease_imports(self, agent_id, target_id):\n",
        "        self.scale_imports(agent_id, target_id, 0.7, 0.95)\n",
        "\n",
        "    def scale_imports(self, agent_id, target_id, lower_bound, upper_bound):\n",
        "        link_idx = -1\n",
        "        for idx in range(self.edge_indexes.shape[1]):\n",
        "            if (self.edge_indexes[0,idx] == target_id) and (self.edge_indexes[1,idx] == agent_id):\n",
        "                link_idx = idx\n",
        "                break\n",
        "\n",
        "        if link_idx == -1:\n",
        "            return\n",
        "        \n",
        "        self.edge_features[idx, 6] = self.edge_features[idx, 6] * random.uniform(lower_bound, upper_bound)\n",
        "\n",
        "    def colonize(self, agent_id, target_id):\n",
        "        # check if there is a link with this country\n",
        "        # and ensure no other country has already colonized the target country\n",
        "        link_idx = -1\n",
        "        already_colonised = False\n",
        "        for idx in range(self.edge_indexes.shape[1]):\n",
        "            if (self.edge_indexes[0,idx] == target_id) and (self.edge_indexes[1,idx] == agent_id):\n",
        "                link_idx = idx\n",
        "\n",
        "            if self.edge_indexes[0,idx] == target_id:\n",
        "                if self.edge_features[idx, 5] == 1:\n",
        "                    already_colonised = True\n",
        "\n",
        "        if link_idx == -1 | already_colonised:\n",
        "            return\n",
        "\n",
        "        # colonizing country needs to be bigger\n",
        "        if (self.node_features[agent_id, 0] > 1.2 * self.node_features[target_id, 0]) and \\\n",
        "           (self.node_features[agent_id, 1] > 1.1 * self.node_features[target_id, 1]):\n",
        "            self.edge_features[link_idx, 5] = 1\n",
        "            self.edge_features[link_idx, 1] = 1\n",
        "\n",
        "    def decolonize(self, agent_id, target_id):\n",
        "        # check if there is a link with this country\n",
        "        link_idx = -1\n",
        "        for idx in range(self.edge_indexes.shape[1]):\n",
        "            if (self.edge_indexes[0,idx] == target_id) and (self.edge_indexes[1,idx] == agent_id):\n",
        "                link_idx = idx\n",
        "                break\n",
        "\n",
        "        if link_idx == -1:\n",
        "            return\n",
        "\n",
        "        if self.edge_features[link_idx, 5] == 1:\n",
        "            self.edge_features[link_idx, 5] = 0\n",
        "\n",
        "    def increase_gdp(self, agent_id):\n",
        "        self.node_features[agent_id, 0] += 0.2 * self.node_features[agent_id, 0] * (random.random() + 0.5)\n",
        "\n",
        "    def decrease_gdp(self, agent_id):\n",
        "        self.node_features[agent_id, 0] -= 0.2 * self.node_features[agent_id, 0] * (random.random() + 0.5)\n",
        "\n",
        "    def increase_pop(self, agent_id):\n",
        "        self.node_features[agent_id, 1] += 0.2 * self.node_features[agent_id, 1] * (random.random() + 0.5)\n",
        "\n",
        "    def decrease_pop(self, agent_id):\n",
        "        self.node_features[agent_id, 1] -= 0.2 * self.node_features[agent_id, 1] * (random.random() + 0.5)\n",
        "\n",
        "    def step(self):\n",
        "        # gdp and pop fluctuations\n",
        "        self.node_features[:, 0] += 0.05 * self.node_features[:, 0] * (torch.rand(self.num_countries, 1, dtype=torch.float32) - 0.5)\n",
        "        self.node_features[:, 1] += 0.05 * self.node_features[:, 1] * (torch.rand(self.num_countries, 1, dtype=torch.float32) - 0.5)\n",
        "\n",
        "        # colonized countries can flip to having a common language\n",
        "        one_feat_shape = self.edge_features[:, 2].shape\n",
        "        self.edge_features[:, 2] = torch.min(torch.ones(one_feat_shape), self.edge_features[:, 2] + (self.edge_features[:, 5] * (torch.rand(num_edges, 1) > 0.9)))\n",
        "\n",
        "        # sea distance can shorten\n",
        "        self.edge_features[:, 4] = torch.min(self.edge_features[:, 0] * 1.5, self.edge_features[:, 4] * torch.max(torch.ones(one_feat_shape), 0.8 + torch.rand(one_feat_shape) * 20))\n",
        "\n",
        "        data = geo.data.Data(x=self.node_features, edge_index=self.edge_indexes, edge_attr=self.edge_features)\n",
        "        self.node_demo = self.env_model(data)\n",
        "\n",
        "        self.create_normed_state()\n",
        "        \n",
        "    def create_normed_state(self):\n",
        "        self.norm_state = geo.data.Data(x = (self.node_features.clone() - self.norm_stats[\"x_mean\"][:2]) / self.norm_stats[\"x_std\"][:2],\n",
        "                                        edge_index = self.edge_indexes,\n",
        "                                        edge_attr = (self.edge_features.clone() - self.norm_stats[\"attr_mean\"]) / self.norm_stats[\"attr_std\"])\n",
        "\n",
        "    def get_reward(self, agent_id):\n",
        "        for cluster in self.clusters:\n",
        "            if agent_id in cluster:\n",
        "                agent_cluster = cluster\n",
        "\n",
        "        reward = 0\n",
        "        for country_idx in range(self.num_countries):\n",
        "            demo = self.node_demo[country_idx]\n",
        "            if country_idx == agent_id:\n",
        "                reward += 2 * demo\n",
        "            elif country_idx in agent_cluster:\n",
        "                reward += demo\n",
        "            else:\n",
        "                reward -= demo\n",
        "                \n",
        "        return reward\n",
        "\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrmC8VlOJ7K4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RecurGraphAgent(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, num_edge_features, num_node_output_features, num_graph_output_features):\n",
        "        super().__init__()\n",
        "\n",
        "        conv_layer_size = 32\n",
        "        lstm_layer_size = 32\n",
        "\n",
        "        # graph convolutional layer to create graph representation\n",
        "        conv_lin = torch.nn.Linear(num_edge_features, num_node_features * conv_layer_size)\n",
        "        self.conv = geo.nn.NNConv(num_node_features, conv_layer_size, conv_lin)\n",
        "\n",
        "        # lstm to learn sequential patterns\n",
        "        self.lstm = torch.nn.LSTM(conv_layer_size, lstm_layer_size, dropout=0.5)\n",
        "\n",
        "        # initial trainable hidden state for lstm\n",
        "        self.lstm_h_s = torch.nn.Linear(1, lstm_layer_size)\n",
        "        self.lstm_c_s = torch.nn.Linear(1, lstm_layer_size)\n",
        "\n",
        "        # graph pooling layer\n",
        "        self.pool = geo.nn.GlobalAttention(gate_nn = torch.nn.Sequential(torch.nn.Linear(lstm_layer_size, 2*lstm_layer_size), torch.nn.BatchNorm1d(2*lstm_layer_size), torch.nn.ReLU(), torch.nn.Linear(2*lstm_layer_size, 1)))\n",
        "\n",
        "        # final graph output\n",
        "        self.final_graph_linear = torch.nn.Linear(lstm_layer_size, num_graph_output_features)\n",
        "\n",
        "        # final linear layer to allow full expressivity for regression after tanh activation in lstm\n",
        "        self.final_node_linear = torch.nn.Linear(lstm_layer_size, num_node_output_features)\n",
        "\n",
        "    def reset(self, initial):\n",
        "        self.initial = initial\n",
        "        self.new_seq = True\n",
        "\n",
        "    def forward(self, input):\n",
        "        # create graph representation\n",
        "        graph_step = torch.nn.functional.relu(self.conv(input.x, input.edge_index, input.edge_attr))\n",
        "\n",
        "        # recurrent stage\n",
        "        # initial state of lstm is representation of target prior to this sequence\n",
        "        if self.new_seq:\n",
        "            self.new_seq = False\n",
        "            self.hs = self.lstm_h_s(initial).unsqueeze(0)\n",
        "            self.cs = self.lstm_c_s(initial).unsqueeze(0)\n",
        "\n",
        "        lstm_output, (self.hs, self.cs) = self.lstm(graph_step.unsqueeze(0), (self.hs, self.cs))\n",
        "\n",
        "        graph_pool = self.pool(lstm_output)\n",
        "        final_graph = self.final_graph_linear(graph_pool)\n",
        "\n",
        "        final_node = self.final_node_linear(lstm_output)\n",
        "        node_flattened = final_node.view(-1)\n",
        "\n",
        "        return torch.nn.functional.softmax(node_flattened), torch.nn.functional.softmax(final_graph)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk2DceslIkWD",
        "colab_type": "text"
      },
      "source": [
        "We want to be able to allow multiple actions per turn. We have previously defined a model with branching outputs. We will consider the predicted Q value to be the sum of the model outputs that are chosen as actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av0VHgnn1HTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NationAgent():\n",
        "    def __init__(self, agent_id, num_countries, replay_capacity, num_node_actions, num_global_actions, device):\n",
        "        # more node features because we will add indicator of self country and ally countries\n",
        "        num_node_features, num_edge_features = 4, 7\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        # create two DQNs for stable learning\n",
        "        self.policy_net = RecurGraphAgent(num_node_features, num_edge_features, num_node_actions, num_global_actions).to(device)\n",
        "        self.target_net = RecurGraphAgent(num_node_features, num_edge_features, num_node_actions, num_global_actions).to(device)\n",
        "        self.optimizer = torch.optim.RMSprop(self.policy_net.parameters())\n",
        "\n",
        "        self.memory = ReplayMemory(replay_capacity)\n",
        "\n",
        "        # ensure they match\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "        self.agent_id = agent_id\n",
        "        self.num_countries = num_countries\n",
        "        self.num_node_actions = num_node_actions\n",
        "        self.num_global_actions = num_global_actions\n",
        "\n",
        "\n",
        "    def reset(self, state_dict, ally_countries, demo_initial):\n",
        "        self.policy_net.load_state_dict(state_dict)\n",
        "        self.target_net.load_state_dict(state_dict)\n",
        "\n",
        "        self.policy_net.reset(demo_initial)\n",
        "        self.target_net.reset(demo_initial)\n",
        "\n",
        "        # create node data with features for self and ally countries\n",
        "        # using -0.1 and 0.9 as approximation of normalization\n",
        "        self.node_features = -0.1 * torch.ones((self.num_countries, 4), dtype=torch.float32)\n",
        "        self.node_features[self.agent_id, 2] = 0.9\n",
        "        for ally_idx in ally_countries:\n",
        "            self.node_features[ally_idx, 3] = 0.9\n",
        "\n",
        "    def get_state(self):\n",
        "        return self.policy_net.state_dict()\n",
        "\n",
        "    def select_action(self, state, eps_threshold):\n",
        "\n",
        "        # add in country specific state\n",
        "        self.node_features[:, :2] = state.x\n",
        "        state.x = self.node_features\n",
        "\n",
        "        sample = random.random()\n",
        "        if sample > eps_threshold:\n",
        "            with torch.no_grad():\n",
        "                # t.max(1) will return largest column value of each row.\n",
        "                # second column on max result is index of where max element was\n",
        "                # found, so we pick action with the larger expected reward.\n",
        "                foreign_output, domestic_output = self.policy_net(state)\n",
        "                foreign_action = torch.argmax(foreign_output)\n",
        "                domestic_action = torch.argmax(domestic_output)\n",
        "                return foreign_action, domestic_action\n",
        "        else:\n",
        "            return torch.tensor(random.randrange(self.num_node_actions), device=self.device, dtype=torch.long), torch.tensor(random.randrange(self.num_global_actions), device=self.device, dtype=torch.long)\n",
        "\n",
        "    def add_transition(self, transition):\n",
        "        self.memory.push(transition)\n",
        "\n",
        "    def optimize(self):\n",
        "        # single transition because i haven't worked out how to make batches work with net yet\n",
        "        transition = self.memory.sample()\n",
        "\n",
        "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "        # columns of actions taken. These are the actions which would've been taken\n",
        "        # for each batch state according to policy_net\n",
        "        foreign_output, domestic_output = self.policy_net(transition.state)\n",
        "        state_action_values = foreign_output[transition.action.foreign] + domestic_output[transition.action.domestic]\n",
        "\n",
        "        # Compute V(s_{t+1}) for all next states.\n",
        "        # Expected values of actions for non_final_next_states are computed based\n",
        "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "        # This is merged based on the mask, such that we'll have either the expected\n",
        "        # state value or 0 in case the state was final.\n",
        "        if transition.next_state is None:\n",
        "            next_state_value = 0\n",
        "        else:\n",
        "            foreign_output, domestic_output = self.target_net(transition.state)\n",
        "            next_state_value = foreign_output.max().detach() + domestic_output.max().detach()\n",
        "        # Compute the expected Q values\n",
        "        expected_state_action_values = (next_state_values * GAMMA) + state.reward\n",
        "\n",
        "        # Compute Huber loss\n",
        "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
        "\n",
        "        # Optimize the model\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        for param in self.policy_net.parameters():\n",
        "            # prevent exploding gradients\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "        optimizer.step()\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh1peXrZy4XT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InternationalAgentCollection():\n",
        "    def __init__(self, num_countries, replay_capacity, num_node_actions, num_global_actions, device):\n",
        "        self.device = device\n",
        "\n",
        "        # create agents\n",
        "        self.agents = []\n",
        "        for agent_id in range(num_countries):\n",
        "            new_agent = NationAgent(agent_id, num_countries, replay_capacity, num_node_actions, num_global_actions, device)\n",
        "            self.agents.append(new_agent)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.agents[idx]\n",
        "\n",
        "    def reset(self, ally_groups, demo_initial):\n",
        "        new_state_dict = self.get_state()\n",
        "\n",
        "        # and then apply averaged state dict to agents\n",
        "        for agent_idx, agent in enumerate(self.agents):\n",
        "            agent_ally_group = []\n",
        "            for ally_group in ally_groups:\n",
        "                if agent_idx in ally_group:\n",
        "                    agent_ally_group = ally_group\n",
        "            # reset each individual agent\n",
        "            agent.reset(new_state_dict, agent_ally_group, demo_initial)\n",
        "\n",
        "    def get_state(self):\n",
        "        # get state dict from all agents\n",
        "        all_agent_states = []\n",
        "        for agent in self.agents:\n",
        "            all_agent_states.append(agent.get_state())\n",
        "\n",
        "        # average them\n",
        "        new_state_dict = all_agent_states[0]\n",
        "        for key in new_state_dict:\n",
        "            for idx in range(1, len(all_agent_states)):\n",
        "                new_state_dict[key] += all_agent_states[idx][key]\n",
        "            new_state_dict[key] = new_state_dict[key] / len(all_agent_states)\n",
        "\n",
        "        return new_state_dict\n",
        "\n",
        "    def select_actions(self, state, eps_threshold):\n",
        "        agent_actions = []\n",
        "        for agent in self.agents:\n",
        "            action = agent.select_action(state, eps_threshold)\n",
        "            agent_actions.append(action)\n",
        "        return agent_actions\n",
        "\n",
        "    def optimize(self):\n",
        "        for agent in self.agents:\n",
        "            agent.optimize()\n",
        "\n",
        "            "
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekP7B3gH529M",
        "colab_type": "text"
      },
      "source": [
        "Function mapping action index choices to actions in the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e311ws9wXNm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def apply_actions(actions, env):\n",
        "    actions = self.agents.select_actions(env.normed_state, eps_threshold)\n",
        "    for agent_idx in range(len(actions)):\n",
        "        foreign_action, domestic_action = actions[agent_idx]\n",
        "        foreign_target_idx = math.floor(foreign_action / env.num_foreign_actions)\n",
        "        foreign_target_action = foreign_action % env.num_foreign_actions\n",
        "\n",
        "        if foreign_target_action == 0:\n",
        "            env.establish_trade(agent_idx, foreign_target_idx)\n",
        "        elif foreign_target_action == 1:\n",
        "            env.increase_imports(agent_idx, foreign_target_idx)\n",
        "        elif foreign_target_action == 2:\n",
        "            env.decrease_imports(agent_idx, foreign_target_idx)\n",
        "        elif foreign_target_action == 3:\n",
        "            env.colonize(agent_idx, foreign_target_idx)\n",
        "        elif foreign_target_action == 4:\n",
        "            env.decolonize(agent_idx, foreign_target_idx)\n",
        "\n",
        "        if domestic_action == 0:\n",
        "            env.increase_gdp(agent_idx)\n",
        "        elif domestic_action == 1:\n",
        "            env.decrease_gdp(agent_idx)\n",
        "        elif domestic_action == 2:\n",
        "            env.increase_pop(agent_idx)\n",
        "        elif domestic_action == 3:\n",
        "            env.decrease_pop(agent_idx)\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn8WL9tE58Ss",
        "colab_type": "text"
      },
      "source": [
        "Defining constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_maLVjcwkb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 50\n",
        "NUM_EPISODES = 100\n",
        "REPLAY_CAPACITY = 20\n",
        "\n",
        "NUM_COUNTRIES = 100\n",
        "NUM_YEARS_PER_ROUND = 200"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm7Q1Ldx9jyY",
        "colab_type": "text"
      },
      "source": [
        "Main training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RdIvoL_CJsh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "592e6bd3-899c-4f7f-e123-2418bc2dcc0f"
      },
      "source": [
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "env = NationEnvironment(NUM_COUNTRIES, device)\n",
        "agents = InternationalAgentCollection(NUM_COUNTRIES, REPLAY_CAPACITY, env.num_foreign_actions, env.num_domestic_actions, device)\n",
        "\n",
        "for i_episode in range(NUM_EPISODES):\n",
        "    # Initialize the environment and state\n",
        "    env.reset()\n",
        "    agents.reset(env.clusters, env.norm_initial_demo)\n",
        "\n",
        "    # reward stats\n",
        "    reward_mean = 0\n",
        "    reward_var = 0\n",
        "\n",
        "    with tqdm.tqdm(range(NUM_YEARS_PER_ROUND)) as years:\n",
        "        for year in years:\n",
        "            years.set_postfix(str=\"Reward Mean: %i, Reward Var: %i\" % (reward_mean, reward_var))\n",
        "\n",
        "            # get state at start of round\n",
        "            state = copy.deepcopy(env.norm_state)\n",
        "\n",
        "            eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "                            math.exp(-1. * i_episode / EPS_DECAY)\n",
        "\n",
        "            # Select and perform an action\n",
        "            actions = agents.select_actions(env.norm_state, eps_threshold)\n",
        "            apply_actions(actions, env)\n",
        "\n",
        "            # let environment take step\n",
        "            env.step()\n",
        "\n",
        "            # Observe new state\n",
        "            next_state = copy.deepcopy(env.norm_state)\n",
        "\n",
        "            rewards = []\n",
        "            # Store the transition in memory\n",
        "            for agent_id in range(NUM_COUNTRIES):\n",
        "                # get the reward\n",
        "                reward = env.get_reward(agent_id)\n",
        "                rewards.append(reward)\n",
        "                action = Action(foreign = actions[agent_id][0],\n",
        "                                domestic = actions[agent_id][1])\n",
        "                transition = Transition(state = state,\n",
        "                                        action = action,\n",
        "                                        next_state = env.normed_state,\n",
        "                                        reward = reward)\n",
        "                agents[idx].add_transition(transition)\n",
        "\n",
        "            reward_mean = statistics.mean(rewards)\n",
        "            reward_var = statistics.variance(rewards)\n",
        "\n",
        "            # Perform one step of the optimization (on the target network)\n",
        "            agents.optimize()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/200 [00:00<?, ?it/s, str=Reward Mean: 0, Reward Var: 0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-4f8e5c9299c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# Select and perform an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mapply_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-289930b1918c>\u001b[0m in \u001b[0;36mselect_actions\u001b[0;34m(self, state, eps_threshold)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0magent_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0magent_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0magent_actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-4c7a92294c05>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, state, eps_threshold)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# add in country specific state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (2) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [100, 2].  Tensor sizes: [100, 4]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj0NUs3XutNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(agents.get_state(), os.path.join('/', 'content', 'drive', 'My Drive', 'projects', 'trade_democratization', 'best_agent.pkl'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbjwwBCpv6KH",
        "colab_type": "text"
      },
      "source": [
        "Things To Try Next:\n",
        "\n",
        "*   Weights only shared in cluster\n",
        "*   Dueling DQN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e99Nj-fcAKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}