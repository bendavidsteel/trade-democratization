{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trade_diffusion_recurrent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOAThisLo1uq2TIkJ6J82Yb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bendavidsteel/trade-democratization/blob/master/trade_diffusion_recurrent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Oxcg4dGh326",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f2f47438-c8f2-45a6-b30f-2fe2664bdcdf"
      },
      "source": [
        "!pip install torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install torch-scatter==2.0.4+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-sparse==0.6.5+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-cluster==1.5.5+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-spline-conv==1.2.0+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-geometric"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.5.1+cu101 in /usr/local/lib/python3.6/dist-packages (1.5.1+cu101)\n",
            "Requirement already satisfied: torchvision==0.6.1+cu101 in /usr/local/lib/python3.6/dist-packages (0.6.1+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1+cu101) (7.0.0)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-scatter==2.0.4+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_scatter-2.0.4%2Bcu101-cp36-cp36m-linux_x86_64.whl (12.2MB)\n",
            "\u001b[K     |████████████████████████████████| 12.3MB 517kB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.4\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-sparse==0.6.5+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_sparse-0.6.5%2Bcu101-cp36-cp36m-linux_x86_64.whl (21.6MB)\n",
            "\u001b[K     |████████████████████████████████| 21.6MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-sparse==0.6.5+cu101) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-sparse==0.6.5+cu101) (1.18.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.5\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-cluster==1.5.5+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_cluster-1.5.5%2Bcu101-cp36-cp36m-linux_x86_64.whl (22.0MB)\n",
            "\u001b[K     |████████████████████████████████| 22.0MB 1.3MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.5.5\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-spline-conv==1.2.0+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_spline_conv-1.2.0%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.3MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3MB 796kB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.0\n",
            "Collecting torch-geometric\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/18/93b190226d09958be96919fd50c55d28f83f1a1b9260a2b33499f9d86728/torch_geometric-1.6.0.tar.gz (172kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.5.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.48.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.0.5)\n",
            "Collecting rdflib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/6b/6454aa1db753c0f8bc265a5bd5c10b5721a4bb24160fb4faf758cf6be8a1/rdflib-5.0.0-py3-none-any.whl (231kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.10.0)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n",
            "Collecting ase\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/70/a8b1a7831193aa228defd805891c534d3e4717c8988147522e673458ddce/ase-3.19.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.11.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (49.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.7)\n",
            "Collecting isodate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (1.12.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from ase->torch-geometric) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->torch-geometric) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ase->torch-geometric) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ase->torch-geometric) (1.2.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-1.6.0-cp36-none-any.whl size=296339 sha256=fa60c12beecf20b4e46e8bc4b64605203c272e29eb908c2eba4ee5796ed96dd6\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/7f/33/acea5809d8580a7adf60dcd6d04f5fc50a7f983040f68be1ff\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: isodate, rdflib, ase, torch-geometric\n",
            "Successfully installed ase-3.19.1 isodate-0.6.0 rdflib-5.0.0 torch-geometric-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AflAeT2iRDk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b48d1b7f-a4b8-490c-933b-d3865bf79153"
      },
      "source": [
        "import copy\n",
        "import itertools\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch_geometric as geo\n",
        "import tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_WwOarnipJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_mapping(vdem_nodes, tradhist_timevar):   \n",
        "\n",
        "    vdem_country_codes = list(vdem_nodes['country_text_id'].unique())\n",
        "    tradhist_country_codes = list(tradhist_timevar['iso_o'].unique())\n",
        "    shared_codes = [code for code in vdem_country_codes if code in tradhist_country_codes]\n",
        "\n",
        "    mapped_codes = [['RUS', 'USSR'], ['YEM', 'ADEN'], ['CAF', 'AOFAEF', 'FRAAEF'], ['TCD', 'AOFAEF', 'FRAAEF'], ['COD', 'AOFAEF', 'FRAAEF'], ['HRV', 'AUTHUN', 'YUG'], ['SVK', 'CZSK', 'AUTHUN'], \n",
        "                ['SVN', 'AUTHUN', 'YUG'], ['UKR', 'AUTHUN', 'USSR'], ['ALB', 'AUTHUN'], ['BIH', 'AUTHUN', 'YUG'], ['MNE', 'AUTHUN', 'YUG'], ['CAN', 'CANPRINCED', 'CANQBCONT', 'NFLD'], \n",
        "                ['CZE', 'CZSK', 'AUTHUN'], ['DDR', 'EDEU'], ['MYS', 'FEDMYS', 'UNFEDMYS', 'GBRBORNEO'], ['BFA', 'FRAAOF'], ['GNQ', 'FRAAOF'], ['LUX', 'ZOLL'], \n",
        "                ['ZZB', 'ZANZ', 'GBRAFRI'], ['ZAF', 'ZAFTRA', 'ZAFORA', 'ZAFNAT', 'ZAPCAF', 'GBRAFRI'], ['MKD', 'YUG'], ['SRB', 'YUG'], ['POL', 'USSR'], ['COM', 'MYT'], ['ROU', 'ROM'], \n",
        "                ['MWI', 'RHOD', 'GBRAFRI'], ['ZMB', 'RHOD', 'GBRAFRI'], ['ZWE', 'RHOD', 'GBRAFRI'], ['SGP', 'STRAITS'], ['DEU', 'WDEU'], ['SML', 'GBRSOM', 'ITASOM'], ['GBR', 'ULSTER'], \n",
        "                ['RWA', 'RWABDI'], ['SOM', 'ITASOM'], ['MAR', 'MARESP'], ['FRA', 'OLDENB'], ['DNK', 'SCHLES'], ['LBN', 'SYRLBN', 'OTTO'], ['SYR', 'SYRLBN'], ['CYP', 'OTTO', 'GBRMEDI'], \n",
        "                ['TUR', 'OTTO'], ['STP', 'PRTAFRI'], ['AGO', 'PRTAFRI'], ['MOZ', 'PRTAFRI'], ['GNB', 'PRTWAFRI'], ['KHM', 'INDOCHI'], ['LAO', 'INDOCHI'], ['VNM', 'INDOCHI'], \n",
        "                ['ERI', 'ITAEAFRI', 'GBRAFRI'], ['TTO', 'GBRWINDIES'], ['SLE', 'GBRWAFRI'], ['GMB', 'GBRWAFRI'], ['TGO', 'GBRWAFRI'], ['EGY', 'OTTO'],\n",
        "                ['PNG', 'GBRPAPUA'], ['MLT', 'GBRMEDI'], ['BGD', 'GBRIND'], ['BTN', 'GBRIND'], ['IND', 'GBRIND'], ['MDV', 'GBRIND'], ['NPL', 'GBRIND'], ['PAK', 'GBRIND'], \n",
        "                ['LKA', 'GBRIND'], ['CMR', 'GBRAFRI', 'FRAAFRI'], ['KEN', 'GBRAFRI'], ['SYC', 'GBRAFRI'], ['SDN', 'GBRAFRI'], ['UGA', 'GBRAFRI'], ['LSO', 'GBRAFRI'], \n",
        "                ['SWZ', 'GBRAFRI']]\n",
        "\n",
        "    # validate my matches\n",
        "    code_count = {}\n",
        "    for codes in mapped_codes:\n",
        "        matched_to_vdem = 0\n",
        "        for code in codes:\n",
        "            if len(code) == 3:\n",
        "                if code in vdem_country_codes:\n",
        "                    if code in code_count:\n",
        "                        code_count[code] += 1\n",
        "                    else:\n",
        "                        code_count[code] = 1\n",
        "                    matched_to_vdem += 1\n",
        "\n",
        "        if matched_to_vdem == 0:\n",
        "            raise ValueError(\"{} country code set matched to no VDem node\".format(codes))\n",
        "        elif matched_to_vdem > 1:\n",
        "            raise ValueError(\"{} country code set matched to more than one VDem node\".format(codes))\n",
        "\n",
        "        if codes[0] not in vdem_country_codes:\n",
        "            raise ValueError(\"VDem code should be first in list {}.\".format(codes))\n",
        "\n",
        "    for code in code_count:\n",
        "        if code_count[code] != 1:\n",
        "            raise ValueError(\"VDem code {} matched to more than one country code set\".format(code))\n",
        "\n",
        "    for code in shared_codes:\n",
        "        if code not in code_count:\n",
        "            mapped_codes.append([code])\n",
        "\n",
        "    return mapped_codes"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YegMqC81x7wq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sequence():\n",
        "    def __init__(self, initial, sequence, missing_mask, target):\n",
        "        self.initial = initial\n",
        "        self.sequence = sequence\n",
        "        self.missing_mask = missing_mask\n",
        "        self.target = target\n",
        "\n",
        "    def to(self, device):\n",
        "        self.initial = self.initial.to(device)\n",
        "        self.missing_mask = self.missing_mask.to(device)\n",
        "        self.target = self.target.to(device)\n",
        "        for idx in range(len(self.sequence)):\n",
        "            self.sequence[idx] = self.sequence[idx].to(device)\n",
        "        return self"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyVZZkdkrSxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TradeDemoYearByYearDataset(geo.data.InMemoryDataset):\n",
        "    def __init__(self, root, transform=None, pre_transform=None):\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['traddem.pt']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOY0o2CNjUoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_norm_stats(root):\n",
        "    return torch.load(os.path.join(root, \"processed\", \"norm_stats.pt\"))\n",
        "\n",
        "def trade_demo_series_dataset(root, sequence_len=10):\n",
        "    node_dict = os.path.join(root, \"processed\", \"node_dict.pt\")\n",
        "    \n",
        "    dataset_file_path = os.path.join(root, \"processed\", 'traddem_series.pt')\n",
        "\n",
        "    if os.path.exists(dataset_file_path):\n",
        "        return torch.load(dataset_file_path)\n",
        "\n",
        "    # Read data into Data object.\n",
        "    vdem_nodes = pd.read_csv(os.path.join(root, \"raw\", \"V-Dem-CY-Core-v10.csv\"))\n",
        "\n",
        "    tradhist_timevar_frames = []\n",
        "    for idx in range(1, 4):\n",
        "        tradhist_timevar_frames.append(pd.read_excel(os.path.join(root, \"raw\", \"TRADHIST_GRAVITY_BILATERAL_TIME_VARIANT_{}.xlsx\".format(idx))))\n",
        "    tradhist_timevar = pd.concat(tradhist_timevar_frames)\n",
        "\n",
        "    country_mapping = get_mapping(vdem_nodes, tradhist_timevar)\n",
        "\n",
        "    dataset = TradeDemoYearByYearDataset(root)\n",
        "\n",
        "    node_dicts = torch.load(node_dict)\n",
        "\n",
        "    num_countries = len(country_mapping)\n",
        "    num_initial_features = 1\n",
        "    num_node_features = 2 # include GDP and population data, and democracy data from last year\n",
        "    num_targets = 1 # 5 main indicators of democracy from the VDem dataset\n",
        "    num_edge_features = 7 # Trade flow, current colony relationship, ever a colony, distance, maritime distance, common language, and shared border\n",
        "\n",
        "    all_sequences = []\n",
        "\n",
        "    for start_idx in tqdm.tqdm(range(len(dataset) - sequence_len + 1)):\n",
        "\n",
        "        initial = torch.zeros(num_countries, num_initial_features)\n",
        "        node_dict = node_dicts[start_idx][\"node_mapping\"]\n",
        "        for country_idx, node_idx in node_dict.items():\n",
        "            initial[country_idx, :] = dataset[start_idx].x[node_idx, 2]\n",
        "\n",
        "        missing_mask = torch.zeros(sequence_len, num_countries, num_targets, dtype=torch.float32)\n",
        "        target = torch.zeros(sequence_len, num_countries, num_targets, dtype=torch.float32)\n",
        "\n",
        "        sequence_data = []\n",
        "        for seq_idx, year_idx in enumerate(range(start_idx, start_idx + sequence_len)):\n",
        "        \n",
        "            x = torch.zeros(num_countries, num_node_features, dtype=torch.float32)\n",
        "            edge_index = torch.zeros(dataset[year_idx].edge_index.shape, dtype=torch.long)\n",
        "\n",
        "            node_dict = node_dicts[year_idx][\"node_mapping\"]\n",
        "            for country_idx, node_idx in node_dict.items():\n",
        "                x[country_idx, :] = dataset[year_idx].x[node_idx, :2]\n",
        "                edge_index[dataset[year_idx].edge_index == node_idx] = country_idx\n",
        "\n",
        "                missing_mask[seq_idx, country_idx, :] = 1\n",
        "                target[seq_idx, country_idx, :] = dataset[year_idx].y[node_idx, 0]\n",
        "\n",
        "            sequence_data.append(geo.data.Data(x=x, \n",
        "                                                edge_index=edge_index, \n",
        "                                                edge_attr=dataset[year_idx].edge_attr))\n",
        "\n",
        "        sequence = Sequence(initial, sequence_data, missing_mask, target)\n",
        "        all_sequences.append(sequence)\n",
        "\n",
        "    torch.save(all_sequences, dataset_file_path)\n",
        "\n",
        "    return all_sequences"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cjBed_9Op2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequence_len = 10\n",
        "dataset = trade_demo_series_dataset(os.path.join('/', 'content', 'drive', 'My Drive', 'projects', 'trade_democratization', 'dataset'), sequence_len=sequence_len)\n",
        "\n",
        "# split into three sets\n",
        "num_train = int(len(dataset) * 0.8)\n",
        "num_val = int(len(dataset) * 0.1)\n",
        "num_test = int(len(dataset) * 0.1)\n",
        "\n",
        "# overlapping sequences means there is some potential for biasing the val and test sets\n",
        "# but small size of dataset means having a reasonable sequence length and a strictly non biased val and test set is not possible\n",
        "train_set = dataset[:num_train]\n",
        "val_set = dataset[num_train:num_train + num_val]\n",
        "test_set = dataset[-num_test:]\n",
        "\n",
        "random.shuffle(train_set)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMuDqfXgQPEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RecurGraphNet(torch.nn.Module):\n",
        "    def __init__(self, num_node_features, num_edge_features, num_output_features):\n",
        "        super().__init__()\n",
        "\n",
        "        conv_layer_size = 32\n",
        "        lstm_layer_size = 32\n",
        "\n",
        "        # graph convolutional layer to create graph representation\n",
        "        conv_lin = torch.nn.Linear(num_edge_features, num_node_features * conv_layer_size)\n",
        "        self.conv = geo.nn.NNConv(num_node_features, conv_layer_size, conv_lin)\n",
        "\n",
        "        # lstm to learn sequential patterns\n",
        "        self.lstm = torch.nn.LSTM(conv_layer_size, lstm_layer_size, dropout=0.5)\n",
        "\n",
        "        # initial trainable hidden state for lstm\n",
        "        self.lstm_h_s = torch.nn.Linear(num_output_features, lstm_layer_size)\n",
        "        self.lstm_c_s = torch.nn.Linear(num_output_features, lstm_layer_size)\n",
        "\n",
        "        # final linear layer to allow full expressivity for regression after tanh activation in lstm\n",
        "        self.final_linear = torch.nn.Linear(lstm_layer_size, num_output_features)\n",
        "\n",
        "    def forward(self, input):\n",
        "        initial, sequence = input.initial, input.sequence\n",
        "        \n",
        "        # create graph representation\n",
        "        graph_collection = []\n",
        "        for idx in range(len(sequence)):\n",
        "            x, edge_index, edge_attr = sequence[idx].x, sequence[idx].edge_index, sequence[idx].edge_attr\n",
        "            graph_step = torch.nn.functional.relu(self.conv(x, edge_index, edge_attr))\n",
        "            graph_collection.append(graph_step)\n",
        "        # provide graph representations as sequence to lstm\n",
        "        graph_series = torch.stack(graph_collection)\n",
        "\n",
        "        # recurrent stage\n",
        "        # initial state of lstm is representation of target prior to this sequence\n",
        "        lstm_output, _ = self.lstm(graph_series, (self.lstm_h_s(initial).unsqueeze(0), self.lstm_c_s(initial).unsqueeze(0)))\n",
        "\n",
        "        # final activation is relu as this is for regression and the metrics of this dataset are all positive\n",
        "        return self.final_linear(lstm_output)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yayAVCnUEoNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = RecurGraphNet(dataset[0].sequence[0].x.shape[1], dataset[0].sequence[0].edge_attr.shape[1], dataset[0].target.shape[2])\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-5)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF5CSIvxGZOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "    for batch in train_set:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch)\n",
        "        # set outputs with missing data to zero so that they don't affect backprop\n",
        "        # good loss for regression problems\n",
        "        loss = torch.nn.functional.smooth_l1_loss(out * batch.missing_mask, batch.target)\n",
        "        loss.backward()\n",
        "        loss_all += loss.item()\n",
        "        optimizer.step()\n",
        "    return loss_all / num_train\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(loader, state_dict=None):\n",
        "\n",
        "    if not state_dict is None:\n",
        "        model.load_state_dict(state_dict)\n",
        "\n",
        "    model.eval()\n",
        "    num_batches = 0\n",
        "    loss_all = 0\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        pred = model(batch)\n",
        "        # good loss for regression problems\n",
        "        loss = torch.nn.functional.smooth_l1_loss(pred * batch.missing_mask, batch.target)\n",
        "        loss_all += loss\n",
        "        num_batches += 1\n",
        "    return loss_all / num_batches"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WJ5lQMmKW-6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "outputId": "08549f9a-b39d-442d-cd62-fe45df823167"
      },
      "source": [
        "MAX_EPOCHS = 10000\n",
        "min_val_loss = float(\"inf\")\n",
        "epochs_since = 0\n",
        "NUM_NON_DECREASING = 50\n",
        "for epoch in range(MAX_EPOCHS):\n",
        "    train_loss = train()\n",
        "    val_loss = test(val_set)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        print('Epoch: {}, Train Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, train_loss, val_loss))\n",
        "\n",
        "    if val_loss < min_val_loss:\n",
        "        best_model = copy.deepcopy(model.state_dict())\n",
        "        min_val_loss = val_loss\n",
        "        epochs_since = 0\n",
        "\n",
        "    epochs_since += 1\n",
        "    if epochs_since > NUM_NON_DECREASING:\n",
        "        print(\"Early stopping engaged\")\n",
        "        break"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Train Loss: 0.0090, Validation Loss: 0.0797\n",
            "Epoch: 5, Train Loss: 0.0083, Validation Loss: 0.0831\n",
            "Epoch: 10, Train Loss: 0.0090, Validation Loss: 0.0786\n",
            "Epoch: 15, Train Loss: 0.0078, Validation Loss: 0.0758\n",
            "Epoch: 20, Train Loss: 0.0078, Validation Loss: 0.0772\n",
            "Epoch: 25, Train Loss: 0.0077, Validation Loss: 0.0768\n",
            "Epoch: 30, Train Loss: 0.0071, Validation Loss: 0.0792\n",
            "Epoch: 35, Train Loss: 0.0068, Validation Loss: 0.0706\n",
            "Epoch: 40, Train Loss: 0.0081, Validation Loss: 0.0767\n",
            "Epoch: 45, Train Loss: 0.0078, Validation Loss: 0.0823\n",
            "Epoch: 50, Train Loss: 0.0069, Validation Loss: 0.0737\n",
            "Epoch: 55, Train Loss: 0.0076, Validation Loss: 0.0759\n",
            "Epoch: 60, Train Loss: 0.0067, Validation Loss: 0.0779\n",
            "Epoch: 65, Train Loss: 0.0069, Validation Loss: 0.0735\n",
            "Epoch: 70, Train Loss: 0.0070, Validation Loss: 0.0752\n",
            "Epoch: 75, Train Loss: 0.0065, Validation Loss: 0.0677\n",
            "Epoch: 80, Train Loss: 0.0087, Validation Loss: 0.0805\n",
            "Epoch: 85, Train Loss: 0.0062, Validation Loss: 0.0726\n",
            "Epoch: 90, Train Loss: 0.0065, Validation Loss: 0.0769\n",
            "Epoch: 95, Train Loss: 0.0076, Validation Loss: 0.0809\n",
            "Epoch: 100, Train Loss: 0.0060, Validation Loss: 0.0782\n",
            "Epoch: 105, Train Loss: 0.0065, Validation Loss: 0.0750\n",
            "Epoch: 110, Train Loss: 0.0065, Validation Loss: 0.0752\n",
            "Epoch: 115, Train Loss: 0.0070, Validation Loss: 0.0774\n",
            "Epoch: 120, Train Loss: 0.0058, Validation Loss: 0.0814\n",
            "Epoch: 125, Train Loss: 0.0066, Validation Loss: 0.0710\n",
            "Early stopping engaged\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIvsSMuwKjoO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "189519bc-3c6c-470b-8702-e6287da390d0"
      },
      "source": [
        "test_loss = test(test_set, state_dict=best_model)\n",
        "print('Final Test Loss: {:.4f}'.format(test_loss))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Test Loss: 0.1037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZwrGTO1KouB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(best_model, os.path.join('/', 'content', 'drive', 'My Drive', 'projects', 'trade_democratization', 'best_model_recurrent.pkl'))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qIjCcKvIT81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}